{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_UE90AVlLzx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# A High-Resolution Framework for Analyzing Transatlantic Monetary Spillovers\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2509.13578-b31b1b.svg)](https://arxiv.org/abs/2509.13578)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-International%20Macroeconomics-blue)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-BVAR%20%7C%20Local%20Projection%20%7C%20HF%20Identification-orange)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Data Source](https://img.shields.io/badge/Data-High--Frequency%20Futures%20%26%20Macro-lightgrey)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/statsmodels-1A568C.svg?style=flat)](https://www.statsmodels.org/stable/index.html)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-4B5F6E.svg?style=flat)](https://pyyaml.org/)\n",
        "[![Joblib](https://img.shields.io/badge/joblib-2F72A4.svg?style=flat)](https://joblib.readthedocs.io/en/latest/)\n",
        "[![TQDM](https://img.shields.io/badge/tqdm-FFC107.svg?style=flat)](https://tqdm.github.io/)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Monetary%20Policy-brightgreen)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Framework](https://img.shields.io/badge/Framework-Bayesian-blueviolet)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Identification](https://img.shields.io/badge/Identification-Sign%20Restrictions-red)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Validation](https://img.shields.io/badge/Validation-Out--of--Sample-yellow)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![Robustness](https://img.shields.io/badge/Robustness-Sensitivity%20Analysis-lightgrey)](https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances)\n",
        "[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n",
        "\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"In-between Transatlantic (Monetary) Disturbances\"** by:\n",
        "\n",
        "*   Santiago Camara\n",
        "*   Jeanne Aublin\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for identifying source-dependent monetary policy shocks and analyzing their international spillovers. It delivers a modular, auditable, and extensible pipeline that replicates the paper's entire workflow: from rigorous high-frequency data processing and validation, through the sophisticated rotational decomposition for shock identification, to the estimation of BVAR and Local Projection models and a comprehensive suite of robustness checks.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: execute_full_study_pipeline](#key-callable-execute_full_study_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"In-between Transatlantic (Monetary) Disturbances.\" The core of this repository is the iPython Notebook `between_transatlantic_monetary_disturbances_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation and analysis of impulse response functions and robustness tests.\n",
        "\n",
        "The paper addresses a key question in international macroeconomics: How do monetary policy shocks from major economic blocs (the U.S. and the Euro Area) propagate to a smaller, open economy (Canada), and do the transmission channels differ? This codebase operationalizes the paper's advanced approach, allowing users to:\n",
        "-   Rigorously validate and cleanse high-frequency financial data and low-frequency macroeconomic data.\n",
        "-   Identify \"pure\" monetary policy shocks, purged of central bank information effects, using a high-frequency identification strategy with sign restrictions.\n",
        "-   Estimate the dynamic effects of these shocks using both Bayesian Vector Autoregressions (BVAR) and Local Projections (LP).\n",
        "-   Conduct a full suite of robustness checks to validate the stability of the findings across different identification schemes, sample periods, and model specifications.\n",
        "-   Systematically investigate specific transmission channels (e.g., trade, financial) by running augmented models.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in modern time-series econometrics and international finance.\n",
        "\n",
        "**1. High-Frequency Identification with Sign Restrictions:**\n",
        "Standard event studies can be confounded by the \"information effect,\" where a central bank's policy action reveals private information about the economic outlook. To solve this, the paper uses the methodology of Jarociński & Karadi (2020). Raw surprises in interest rates ($s^{rate}$) and equity prices ($s^{equity}$) are assumed to be linear combinations of two structural shocks: a pure monetary policy shock ($\\varepsilon^{MP}$) and an information shock ($\\varepsilon^{INFO}$).\n",
        "$$ \\begin{bmatrix} s^{rate}_t \\\\ s^{equity}_t \\end{bmatrix} = A \\begin{bmatrix} \\varepsilon^{MP}_t \\\\ \\varepsilon^{INFO}_t \\end{bmatrix} $$\n",
        "The identification of the mixing matrix `A` is achieved by finding all rotations of an initial Cholesky decomposition that satisfy a set of theoretical sign restrictions on the impulse responses (e.g., a contractionary MP shock must raise rates and lower equity prices).\n",
        "\n",
        "**2. Bayesian Vector Autoregression (BVAR):**\n",
        "The primary workhorse model is a VAR-X, where the identified shocks are treated as exogenous variables. The model for a vector of endogenous variables $Y_t$ is:\n",
        "$$ Y_t = c + \\sum_{i=1}^{p} B_i Y_{t-i} + \\Gamma_1 s_t^{ECB} + \\Gamma_2 s_t^{Fed} + D_t + e_t $$\n",
        "The model is estimated using Bayesian methods with a Normal-Wishart prior and a Minnesota-style specification for the prior hyperparameters. Inference is conducted by drawing from the posterior distribution using a Gibbs sampler.\n",
        "\n",
        "**3. Local Projections (LP):**\n",
        "As a robustness check, the impulse responses are also estimated using the Local Projection method of Jordà (2005). This involves running a separate regression for each forecast horizon `h`:\n",
        "$$ y_{k, t+h} = \\beta_h^{Shock} s_t^{Shock} + \\text{controls}_t + \\epsilon_{t+h} $$\n",
        "The sequence of estimated coefficients $\\{\\hat{\\beta}_h^{Shock}\\}_{h=0}^H$ forms the impulse response function. This method is robust to misspecification but less efficient than a VAR. Inference requires HAC (Newey-West) standard errors.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`between_transatlantic_monetary_disturbances_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Phase Architecture:** The entire pipeline is broken down into 17 distinct, modular tasks, from data validation to final robustness checks.\n",
        "-   **Configuration-Driven Design:** All methodological and computational parameters are managed in an external `config.yaml` file, allowing for easy customization without code changes.\n",
        "-   **Professional-Grade Data Pipeline:** A comprehensive validation, quality assessment, and cleansing suite for both high-frequency and low-frequency data, including robust handling of timezones and DST.\n",
        "-   **High-Fidelity Shock Identification:** A precise, vectorized implementation of the rotational-angle decomposition method.\n",
        "-   **Robust BVAR Estimation:** A complete Gibbs sampler for a BVAR with a Normal-Wishart prior, including intra-run convergence diagnostics.\n",
        "-   **Complete Local Projections Estimator:** A full implementation of the LP method with HAC-robust standard errors.\n",
        "-   **Advanced Robustness Toolkit:**\n",
        "    -   A framework for testing alternative identification schemes (Poor Man's Sign Restriction).\n",
        "    -   A parallelized framework for quantifying identification uncertainty by integrating over all admissible rotations.\n",
        "    -   A framework for testing sensitivity to alternative sample periods (pre-GFC, pre-COVID).\n",
        "    -   A framework for testing sensitivity to estimation choices (prior hyperparameters, lag length).\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Validation & Preprocessing (Tasks 1-3):** Ingests and rigorously validates all raw data and the `config.yaml` file, performs a deep data quality audit, and produces clean, analysis-ready data streams.\n",
        "2.  **Shock Identification (Tasks 4-6):** Defines event windows, extracts high-frequency prices, calculates raw surprises, and performs the rotational decomposition to identify structural shocks.\n",
        "3.  **Model Preparation (Tasks 7-8):** Aggregates the identified shocks to a monthly frequency and assembles the final, transformed dataset for econometric modeling.\n",
        "4.  **Estimation (Tasks 9-11):** Sets up and estimates the baseline BVAR via Gibbs sampling and the Local Projections model via OLS with HAC errors.\n",
        "5.  **Results & Validation (Tasks 12-14):** Calculates impulse response functions from the BVAR posterior and runs a full suite of in-sample and out-of-sample validation tests.\n",
        "6.  **Robustness Analysis (Tasks 16-17):** Orchestrates the entire suite of robustness checks on the identification and estimation methods.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `between_transatlantic_monetary_disturbances_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: execute_full_study_pipeline\n",
        "\n",
        "The central function in this project is `execute_full_study_pipeline`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study and all associated robustness checks.\n",
        "\n",
        "```python\n",
        "def execute_full_study_pipeline(\n",
        "    equity_tick_df: pd.DataFrame,\n",
        "    rate_tick_df: pd.DataFrame,\n",
        "    macro_df: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    target_market: str,\n",
        "    study_config: Dict[str, Any],\n",
        "    run_identification_robustness: bool = True,\n",
        "    run_estimation_robustness: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire research study, including the main analysis and all robustness checks.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `statsmodels`, `pyyaml`, `tqdm`, `joblib`, `pandas_market_calendars`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances.git\n",
        "    cd between_transatlantic_monetary_disturbances\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy statsmodels pyyaml tqdm joblib pandas_market_calendars\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires four `pandas.DataFrame`s and a configuration file as input. Mock data generation functions are provided in the main notebook to create valid examples for testing.\n",
        "1.  **`equity_tick_df` / `rate_tick_df`:** Must contain columns `['timestamp_micros_utc', 'price', 'volume', 'type']`.\n",
        "2.  **`macro_df`:** A long-format DataFrame with columns `['date', 'source_series_id', 'country', 'variable_name', 'value_raw']`.\n",
        "3.  **`announcement_df`:** Must contain columns `['event_id', 'central_bank', 'announcement_date_local', 'announcement_time_local', 'local_timezone']`.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `between_transatlantic_monetary_disturbances_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your four raw `pandas.DataFrame`s. Ensure the `config.yaml` file is present in the same directory.\n",
        "2.  **Execute Pipeline:** Call the grand orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs the entire project.\n",
        "    final_results = execute_full_study_pipeline(\n",
        "        equity_tick_df=my_equity_data,\n",
        "        rate_tick_df=my_rate_data,\n",
        "        macro_df=my_macro_data,\n",
        "        announcement_df=my_announcement_data,\n",
        "        target_market='CAN',\n",
        "        study_config=my_config_dict,\n",
        "        run_identification_robustness=False,  # Set to True for the full analysis\n",
        "        run_estimation_robustness=False\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** The returned `final_results` dictionary contains all generated artifacts, including intermediate data, final IRFs, and validation reports.\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_full_study_pipeline` function returns a single, comprehensive dictionary containing all generated artifacts, structured by analytical phase. Key outputs include:\n",
        "-   `benchmark_run`: The results of the main analysis.\n",
        "    -   `phase_2_identification['structural_shocks']`: The identified monthly shock series.\n",
        "    -   `phase_3_model_prep['analysis_ready_df']`: The final dataset used for estimation.\n",
        "    -   `phase_5_results['bvar_irfs']`: The final impulse response functions from the BVAR.\n",
        "    -   `phase_5_results['model_validation_reports']`: The full suite of diagnostic reports.\n",
        "-   `identification_robustness_suite`: (If run) Contains the results of the PMSR, rotational uncertainty, and sub-sample analyses.\n",
        "-   `estimation_robustness_suite`: (If run) Contains the results of the prior, lag, and specification sensitivity analyses.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "between_transatlantic_monetary_disturbances/\n",
        "│\n",
        "├── between_transatlantic_monetary_disturbances_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                               # Master configuration file\n",
        "├── requirements.txt                                          # Python package dependencies\n",
        "├── LICENSE                                                   # MIT license file\n",
        "└── README.md                                                 # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all methodological parameters, such as BVAR lags, prior hyperparameters, MCMC settings, and window definitions, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Visualization Module:** Creating a function that takes the final IRF results and automatically generates publication-quality plots that replicate the figures in the paper.\n",
        "-   **Automated Reporting:** Building a module that uses the generated results and validation reports to automatically create a full PDF or HTML summary report of the findings.\n",
        "-   **Alternative Priors:** Implementing other BVAR prior structures, such as the Independent Normal-Wishart prior or stochastic volatility.\n",
        "-   **Structural VAR Identification:** Adding modules for other SVAR identification schemes, such as Cholesky or long-run restrictions, for comparison.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{camara2025inbetween,\n",
        "  title={{In-between Transatlantic (Monetary) Disturbances}},\n",
        "  author={Camara, Santiago and Aublin, Jeanne},\n",
        "  journal={arXiv preprint arXiv:2509.13578},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A High-Resolution Framework for Analyzing Transatlantic Monetary Spillovers.\n",
        "GitHub repository: https://github.com/chirindaopensource/between_transatlantic_monetary_disturbances\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Santiago Camara and Jeanne Aublin** for their foundational research, which forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, Statsmodels, and Joblib**, whose work makes complex computational analysis accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `between_transatlantic_monetary_disturbances_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "laPydDkyEeKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*In-between Transatlantic (Monetary) Disturbances*\"\n",
        "\n",
        "Authors: Santiago Camara, Jeanne Aublin\n",
        "\n",
        "E-Journal Submission Date: 16 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2509.13578\n",
        "\n",
        "Abstract:\n",
        "\n",
        "This paper studies the spillovers of European Central Bank (ECB) interest rate shocks into the Canadian economy and compares them with those of the U.S. Federal Reserve (Fed). We combine a VAR model and local projection regressions with identification strategies that explicitly purge information effects around policy announcements. We find that an ECB rate hike leads to a depreciation of the Canadian dollar and a sharp contraction in economic activity. The main transmission channel is international trade: ECB shocks trigger a decline in oil prices and exports, while leaving domestic financial conditions largely unaffected. By contrast, Fed shocks tighten Canadian financial conditions significantly, with more limited effects on trade flows. These findings show that Canada is exposed to foreign monetary policy both directly and indirectly, through its integration in global financial and trade markets.\n",
        "\n",
        "\n",
        "Keywords: Monetary policy; Canadian Economy; Federal Reserve; European Central Bank; International Spillovers; Exchange Rates, International Spillovers; “Central\n",
        "Bank Information”. JEL Codes: F40, F41, E44, E51.\n"
      ],
      "metadata": {
        "id": "X7p8pL1hlZLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Here is a step-by-step summary of the paper \"In-between Transatlantic (Monetary) Disturbances\" by Aublin and Camara.\n",
        "\n",
        "**\n",
        "\n",
        "### **The Central Research Question and Motivation**\n",
        "\n",
        "The paper seeks to answer a fundamental question for small, open economies: How are they affected by monetary policy shocks from different major economic blocs?\n",
        "\n",
        "Specifically, it investigates the spillovers of monetary policy from the **European Central Bank (ECB)** to the Canadian economy and systematically compares them to the well-documented spillovers from the **U.S. Federal Reserve (Fed)**. The motivation is to challenge the conventional \"U.S.-centric\" view of Canada's external environment, which often models the rest of the world using U.S. variables alone. The authors hypothesize that this view is incomplete and that shocks from other major players like the ECB transmit through different, yet equally important, channels.\n",
        "\n",
        "### **The Core Methodological Engine**\n",
        "\n",
        "The authors' empirical strategy is sophisticated and rests on two key pillars, which are crucial for the credibility of their findings.\n",
        "\n",
        "**Pillar A: The Identification Strategy**\n",
        "This is the most critical piece of their methodology. They need to isolate *pure* monetary policy shocks—unexpected changes in the policy rate—from other news revealed during central bank announcements.\n",
        "\n",
        "1.  **The Problem:** A central bank rate hike could be a pure tightening action, or it could be a reaction to positive private information the bank has about future economic growth (an \"information effect\"). A standard high-frequency surprise, which just measures the change in asset prices around an announcement, conflates these two.\n",
        "2.  **The Solution:** They employ the high-frequency identification strategy pioneered by Jarociński and Karadi (2020). The logic is as follows:\n",
        "    *   A **pure contractionary monetary policy shock** should raise interest rates but lower equity prices (as future profits are discounted more heavily and economic activity is expected to slow). This implies a *negative* correlation between interest rate surprises and equity surprises.\n",
        "    *   An **information shock** (e.g., the central bank signals a stronger economy) should cause both interest rates and equity prices to rise. This implies a *positive* correlation.\n",
        "3.  **The Technique:** Using high-frequency data on interest rate and stock index futures around policy announcements, they use a \"rotational-angle decomposition\" with sign restrictions to separate the raw surprises into these two orthogonal shocks: a pure monetary policy shock and an information shock. Their benchmark results use the median rotation that satisfies these restrictions.\n",
        "\n",
        "**Pillar B: The Estimation Framework**\n",
        "Once the shocks are identified, the authors estimate their dynamic effects on the Canadian economy using two complementary econometric models to ensure robustness:\n",
        "\n",
        "1.  **Bayesian Vector Autoregression (BVAR):** A standard, efficient tool in macroeconomics for estimating impulse response functions (IRFs). It models the joint dynamics of a system of variables.\n",
        "2.  **Local Projections (LP):** A more flexible, single-equation approach that is less prone to misspecification bias than VARs, though often less precise.\n",
        "\n",
        "Using both methods allows them to check if their results are an artifact of a specific modeling choice.\n",
        "\n",
        "### **The Benchmark Empirical Findings**\n",
        "\n",
        "The paper's central results reveal a stark asymmetry in how ECB and Fed shocks affect Canada.\n",
        "\n",
        "*   **Response to an ECB Rate Hike (Contractionary Shock):**\n",
        "    *   **Exchange Rate:** The Canadian dollar (CAD) depreciates significantly against the euro.\n",
        "    *   **Domestic Policy:** The Bank of Canada *eases* policy, lowering its own interest rate to cushion the blow.\n",
        "    *   **Real Economy:** Canadian GDP contracts sharply and immediately.\n",
        "    *   **Financial Conditions:** Domestic financial conditions are largely unaffected or even ease slightly.\n",
        "\n",
        "*   **Response to a Fed Rate Hike (Contractionary Shock):**\n",
        "    *   **Exchange Rate:** The CAD depreciates, but much less than in the ECB case.\n",
        "    *   **Domestic Policy:** The Bank of Canada *tightens* policy, raising its interest rate in lockstep with the Fed.\n",
        "    *   **Real Economy:** Canadian GDP contracts more slowly, but the contraction is more persistent and lasts longer.\n",
        "    *   **Financial Conditions:** Canadian financial conditions tighten significantly. Equity markets fall sharply and remain depressed.\n",
        "\n",
        "In short, ECB shocks look like a foreign *real* shock to Canada, while Fed shocks act like a domestic *financial* shock.\n",
        "\n",
        "### **Decomposing the Transmission Channels**\n",
        "\n",
        "The authors then dig deeper to understand *why* these responses are so different.\n",
        "\n",
        "*   **The ECB Channel: Trade and Commodities**\n",
        "    *   An ECB tightening reduces demand in the large and open Euro Area, which lowers global commodity prices, particularly **oil**.\n",
        "    *   The fall in oil prices directly hits Canada's terms of trade, oil production, and oil exports, leading to the immediate contraction in GDP.\n",
        "    *   The effects are concentrated in commodity-related sectors. Non-oil exports are less affected.\n",
        "    *   Interestingly, the paper shows that this effect is largely indirect: the ECB shock slows the U.S. economy, which in turn reduces U.S. demand for Canadian exports.\n",
        "\n",
        "*   **The Fed Channel: Financial Linkages**\n",
        "    *   Due to the deep integration of North American financial markets, a Fed tightening directly spills over into Canadian financial conditions.\n",
        "    *   The Bank of Canada is compelled to follow the Fed to manage capital flows and exchange rate pressures.\n",
        "    *   This leads to higher government bond yields, higher mortgage rates, and falling house prices in Canada.\n",
        "    *   The economic contraction is therefore broad-based, affecting credit-sensitive sectors like construction and durable goods, consistent with a domestic financial tightening.\n",
        "\n",
        "### **Robustness and Validation**\n",
        "\n",
        "The paper conducts a battery of tests to confirm its findings are not spurious.\n",
        "\n",
        "1.  **Alternative Identification:** They show the results hold using a simpler \"Poor Man's Sign Restriction\" method.\n",
        "2.  **Bias from Ignoring Information:** They explicitly show that using the raw, conflated high-frequency surprise (as older studies did) produces biased and misleading results. For instance, it would wrongly suggest an ECB hike is *expansionary* for Canadian GDP on impact.\n",
        "3.  **Sample Period:** The results are robust to excluding the volatile COVID-19 period.\n",
        "4.  **Identification Uncertainty:** The findings hold even when considering the full range of possible shock decompositions (\"admissible rotations\"), not just the median one.\n",
        "\n",
        "### **Conclusion and Implications**\n",
        "\n",
        "The paper concludes with powerful implications for both researchers and policymakers:\n",
        "\n",
        "*   **For Researchers:** The standard U.S.-centric model of the global economy is insufficient, even for a country as closely tied to the U.S. as Canada. The source of a global shock matters immensely, as different central banks transmit their policy through distinct channels (real vs. financial).\n",
        "*   **For Canadian Policymakers:** The challenge of conducting monetary policy is multifaceted. Canada is caught \"in-between\" two types of transatlantic disturbances. It is exposed to **global demand and commodity shocks** originating from Europe and to **direct financial shocks** from the United States. The appropriate domestic policy response depends critically on diagnosing the origin of the external shock."
      ],
      "metadata": {
        "id": "KZG8fGZZmDda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Critique\n",
        "\n",
        "## Critical Assessment and Implementation Implications\n",
        "\n",
        "From a methodological standpoint, this work represents significant advancement in several dimensions:\n",
        "\n",
        "* **Identification Innovation**: The rotational-angle decomposition addresses a fundamental confound in monetary policy identification. The demonstration that standard high-frequency identification conflates policy and information effects has broad implications for the spillovers literature.\n",
        "\n",
        "* **Multi-source spillover framework**: Moving beyond US-centric analysis of Canadian external dependence represents an important theoretical and empirical contribution.\n",
        "\n",
        "* **Channel decomposition**: The systematic comparison of trade vs. financial transmission mechanisms provides actionable insights for both researchers and policymakers.\n",
        "\n",
        "However, several methodological considerations merit attention:\n",
        "\n",
        "* **Sample limitations**: Monthly frequency may mask higher-frequency adjustment dynamics, particularly in financial markets.\n",
        "\n",
        "* **Linear specification**: The VAR/LP framework assumes linear responses, potentially missing state-dependent transmission (e.g., during financial stress periods).\n",
        "\n",
        "* **Exchange rate endogeneity**: While the authors control for policy responses, exchange rate movements themselves may feedback to affect the transmission of foreign shocks."
      ],
      "metadata": {
        "id": "IqE4KkBpJShs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "pBpeoAj6Yj1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  High-Resolution Identification and Analysis of Transatlantic Monetary Spillovers\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"In-between Transatlantic (Monetary)\n",
        "#  Disturbances\" by Santiago Camara and Jeanne Aublin (2025). It delivers a\n",
        "#  computationally intensive, high-fidelity system for identifying source-\n",
        "#  dependent monetary policy shocks and tracing their international spillovers\n",
        "#  through distinct trade and financial channels.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • High-frequency identification of monetary policy and information shocks\n",
        "#  • Rotational-angle decomposition with sign restrictions for shock separation\n",
        "#  • Bayesian Vector Autoregression (BVAR) with Minnesota-style Normal-Wishart priors\n",
        "#  • Gibbs sampling for posterior inference of model parameters\n",
        "#  • Local Projections (Jordà, 2005) for robust impulse response estimation\n",
        "#  • Comprehensive robustness checks for identification, sample period, and priors\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Granular high-frequency data processing and cleansing pipeline\n",
        "#  • Robust time-series validation and anomaly detection\n",
        "#  • Explicit handling of timezone conversions and Daylight Saving Time\n",
        "#  • Vectorized, high-performance numerical linear algebra for identification\n",
        "#  • Modular, multi-phase orchestration for full pipeline reproducibility\n",
        "#  • Extensive logging, auditing, and structured results management\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Camara, S., & Aublin, J. (2025). In-between Transatlantic (Monetary)\n",
        "#  Disturbances. arXiv preprint arXiv:2509.13578.\n",
        "#  https://arxiv.org/abs/2509.13578\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# =============================================================================\n",
        "# STANDARD LIBRARIES\n",
        "# =============================================================================\n",
        "import copy\n",
        "import logging\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime, time\n",
        "\n",
        "# =============================================================================\n",
        "# THIRD-PARTY LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "# Core data manipulation and numerical computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Timezone and market calendar handling\n",
        "import pandas_market_calendars as mcal\n",
        "import pytz\n",
        "\n",
        "# Statistical modeling and econometrics\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "from scipy.stats import f, multivariate_normal, t\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.stats.sandwich_covariance import cov_hac\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Parallel processing and progress monitoring\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =============================================================================\n",
        "# TYPE HINTING\n",
        "# =============================================================================\n",
        "from typing import (Any, Dict, List, NoReturn, Optional, Set, Tuple, Union)\n"
      ],
      "metadata": {
        "id": "-VdrmsWuYpb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "PnQqUKYDYt8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Explication of Key Callables**\n",
        "\n",
        "#### **1. `run_phase1_task1_validation`**\n",
        "\n",
        "*   **Inputs:** Four raw `pd.DataFrame`s (`equity_tick_df`, `rate_tick_df`, `macro_df`, `announcement_df`), the `target_market` string, and the `study_config` dictionary.\n",
        "*   **Processes:** This function orchestrates the foundational integrity checks of the entire project.\n",
        "    1.  It calls `validate_study_configuration` to perform a deep validation of all critical parameters in the `study_config` dictionary against the paper's specified values (e.g., BVAR lags, prior hyperparameters).\n",
        "    2.  It calls `validate_dataframe_schema` for each of the four input DataFrames to ensure their columns, data types (including timezone awareness), and categorical levels exactly match the required schema for the analysis.\n",
        "    3.  It calls the remediated `validate_cross_dataset_alignment` to perform critical cross-dataset checks: ensuring the target market exists, that all announcement events are temporally covered by the high-frequency data (with a 24-hour buffer), and that the macroeconomic series for the target market do not have debilitating data gaps (> 3 months).\n",
        "*   **Outputs:** A dictionary of `pd.DataFrame` reports detailing the results of every validation check. The function's primary purpose is to halt execution via a specific exception (`ConfigurationValidationError`, `SchemaError`, `ValueError`) if any critical check fails.\n",
        "*   **Role in Research Pipeline:** This callable is the **Gatekeeper**. It ensures that the raw data and configuration are of sufficient quality and consistency to even begin the analysis. It programmatically enforces the basic data assumptions made on pages 6 (\"Dataset\") and 9 (\"Bayesian VARs\") of the paper, preventing the propagation of errors from corrupted or misconfigured inputs.\n",
        "\n",
        "#### **2. `run_phase1_task2_quality_assessment`**\n",
        "\n",
        "*   **Inputs:** The four raw `pd.DataFrame`s.\n",
        "*   **Processes:** This function orchestrates a deep, statistical quality assessment of the raw data.\n",
        "    1.  It calls `detect_tick_data_anomalies` to screen the high-frequency data for duplicates, price outliers (using the Modified Z-score), volume anomalies, and timestamp gaps.\n",
        "    2.  It calls `verify_announcement_metadata` to check the integrity of the event schedule, validating timezone strings, checking for events on non-business days, and ensuring chronological consistency.\n",
        "    3.  It calls the remediated `assess_macro_series_quality` to perform statistical checks on each macroeconomic time series, including a statistically correct Chow test for structural breaks and a Hampel filter for outliers.\n",
        "*   **Outputs:** A dictionary of `pd.DataFrame` reports containing detailed, observation-level anomaly flags and per-series quality metrics.\n",
        "*   **Role in Research Pipeline:** This callable is the **Microscope**. It moves beyond schema validation to inspect the statistical properties of the data itself. It is the practical implementation of the data scrutiny that is implicit in any high-quality empirical study, ensuring that the data used for the core analysis is clean and free of anomalies that could bias the results.\n",
        "\n",
        "#### **3. `run_phase1_task3_preprocessing`**\n",
        "\n",
        "*   **Inputs:** The four raw `pd.DataFrame`s, the anomaly flag reports from the previous task, and the `target_market` string.\n",
        "*   **Processes:** This orchestrator manages the initial cleaning and structuring of the data.\n",
        "    1.  It calls `cleanse_tick_data` to filter the raw tick data, removing duplicates, non-trade ticks, low-volume trades, and identified outliers.\n",
        "    2.  It calls `standardize_announcement_timestamps` to convert all local announcement times to robust, unambiguous UTC timestamps.\n",
        "    3.  It calls the amended `prepare_macroeconomic_data` to transform the raw long-format macro data into a clean, wide-format panel, indexed by date, with all series in their original levels. This includes limited interpolation and the exclusion of series with large gaps.\n",
        "*   **Outputs:** A dictionary containing the cleansed DataFrames and their associated audit trails. The key output is the `prepared_macro_df_levels`.\n",
        "*   **Role in Research Pipeline:** This callable is the **Refinery**. It takes the validated raw materials and transforms them into a set of clean, consistently structured, and aligned datasets that are the direct inputs for the core economic analysis. It separates the universal cleaning process from the model-specific transformations that come later.\n",
        "\n",
        "#### **4. `run_phase2_task4_data_extraction`**\n",
        "\n",
        "*   **Inputs:** The `clean_announcement_df`, `clean_equity_df`, `clean_rate_df`, and the `study_config`.\n",
        "*   **Processes:** This function orchestrates the core high-frequency event study data extraction.\n",
        "    1.  It calls `construct_event_windows` to define the precise `t_before` and `t_after` UTC timestamps for the analysis window around each announcement, correctly merging any overlapping windows.\n",
        "    2.  It calls `extract_window_prices` twice (once for equity, once for rates) to find the last traded price before the window and the first *stable* price after the window, using a robust multi-tiered fallback search and an iterative stabilization check.\n",
        "*   **Outputs:** A dictionary containing the event window definitions and the detailed price extraction results (including prices and metadata) for both equity and rate instruments.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Event Window Measurement**. It is the direct, high-fidelity implementation of the data extraction process required for the high-frequency identification strategy described on page 7 of the paper. It turns a list of event times into a set of pre- and post-event prices, which are the fundamental inputs for calculating surprises.\n",
        "\n",
        "#### **5. `run_phase2_task5_surprise_calculation`**\n",
        "\n",
        "*   **Inputs:** The price extraction results for equity and rates, and the `clean_announcement_df`.\n",
        "*   **Processes:** This orchestrator converts the extracted prices into reduced-form surprises.\n",
        "    1.  It calls the remediated `calculate_rate_surprises`, which uses an explicit configuration to apply the correct, instrument-specific formula (e.g., for `100 - R` contracts) to calculate the interest rate surprise in basis points.\n",
        "    2.  It calls `calculate_equity_surprises` to compute the equity surprise using the log-return formula: $s^{equity}_{t} = \\ln\\left(\\frac{P_{after}}{P_{before}}\\right) \\times 100$.\n",
        "    3.  It calls `construct_surprise_vectors` to assemble these into 2x1 vectors, $S_t = \\begin{bmatrix} s^{rate}_{t} \\\\ s^{equity}_{t} \\end{bmatrix}$, and applies a 5-sigma outlier filter on a per-bank basis.\n",
        "*   **Outputs:** A dictionary containing the final `(N, 2)` NumPy arrays of surprise vectors for each central bank, along with any identified outliers.\n",
        "*   **Role in Research Pipeline:** This callable is the **Surprise Generator**. It implements the first step of the identification strategy on page 7, converting raw price changes into the economically meaningful surprise vectors that form the basis for the rotational decomposition.\n",
        "\n",
        "#### **6. `run_phase2_task6_shock_identification`**\n",
        "\n",
        "*   **Inputs:** The dictionary of surprise vectors and the `study_config`.\n",
        "*   **Processes:** This is the core identification orchestrator. It calls `identify_structural_shocks`, which performs the full rotational-angle decomposition.\n",
        "    1.  It generates a grid of rotation matrices $Q(\\theta) = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}$.\n",
        "    2.  For each angle, it generates candidate structural shocks $\\epsilon(\\theta) = Q(\\theta) S^T$.\n",
        "    3.  It calls the remediated `_find_admissible_rotations` helper to check each angle against the sign restrictions (e.g., $\\rho(\\varepsilon^{MP}(\\theta), s^{rate}) > 0$ and $\\rho(\\varepsilon^{MP}(\\theta), s^{equity}) < 0$) to find the set of admissible angles.\n",
        "    4.  It calculates the median of the admissible angles (handling circularity) and uses this to generate the final, normalized benchmark structural shock series.\n",
        "*   **Outputs:** A dictionary containing the final, identified structural shock series for each central bank, along with metadata about the identification process (e.g., the set of admissible angles).\n",
        "*   **Role in Research Pipeline:** This callable is the **Shock Disentangler**. It is the direct implementation of the sophisticated identification strategy from Jarociński & Karadi (2020) described on page 7 of the paper. Its purpose is to solve the \"central bank information\" puzzle by separating the raw surprises into a pure monetary policy shock ($\\varepsilon^{MP}$) and an orthogonal information shock ($\\varepsilon^{INFO}$).\n",
        "\n",
        "#### **7. `run_phase3_task7_temporal_aggregation`**\n",
        "\n",
        "*   **Inputs:** The identified structural shocks and the `clean_announcement_df`.\n",
        "*   **Processes:** This orchestrator bridges the frequency gap between the high-frequency identification and the monthly models.\n",
        "    1.  It calls the remediated `map_events_to_months` to assign each event to a specific calendar month, correctly implementing the \"> 15th day\" rule and the December edge case.\n",
        "    2.  It calls the remediated `aggregate_shocks_to_monthly` to sum all shocks within each assigned month, $\\varepsilon^{MP}_{m} = \\sum_{t \\in \\text{month } m} \\varepsilon^{MP}_{t}$, and to create a dense time series where non-announcement months have a shock value of zero.\n",
        "    3.  It calls `validate_monthly_shock_series` to perform statistical checks (zero mean, no autocorrelation) on the final monthly series.\n",
        "*   **Outputs:** A dictionary containing the final monthly shock `pd.DataFrame` and a statistical validation report.\n",
        "*   **Role in Research Pipeline:** This callable is the **Frequency Bridge**. It converts the event-dated shock series into a monthly time series suitable for inclusion as an exogenous regressor in the VAR and LP models, as specified on page 9 (\"...include the identified ECB and Fed monetary policy shocks as exogenous regressors.\").\n",
        "\n",
        "#### **8. `run_phase3_task8_final_data_assembly`**\n",
        "\n",
        "*   **Inputs:** The `prepared_macro_df` (in levels), the `monthly_shocks_df`, and the `study_config`.\n",
        "*   **Processes:** This orchestrator was refactored. Its new, correct role is to perform the final model-specific transformations and assemble the complete dataset.\n",
        "    1.  It calls `transform_core_macro_variables` to apply log transformations ($y_t = 100 \\times \\ln(x_t)$) and calculate any necessary cross-rates.\n",
        "    2.  It calls `construct_control_variables` to generate the time trend and COVID-19 dummy.\n",
        "    3.  It calls the remediated `assemble_final_dataset` to merge the transformed macro data, the monthly shocks, and the control variables using the robust `outer join -> fill -> drop` procedure.\n",
        "*   **Outputs:** The final, complete, analysis-ready `pd.DataFrame`.\n",
        "*   **Role in Research Pipeline:** This callable is the **Model Assembler**. It creates the final, canonical dataset that is the direct input to all subsequent econometric models, ensuring all variables are correctly transformed and aligned.\n",
        "\n",
        "#### **9. `run_phase4_task9_bvar_setup`**\n",
        "\n",
        "*   **Inputs:** The `analysis_ready_df` and the `study_config`.\n",
        "*   **Processes:** This orchestrator prepares the inputs for the BVAR estimation.\n",
        "    1.  It calls `create_var_matrices` to transform the analysis DataFrame into the `Y` (endogenous) and `X` (regressors, including lags) NumPy matrices.\n",
        "    2.  It calls `construct_minnesota_prior` to translate the high-level hyperparameters from the config into the precise prior mean vector ($\\beta_0$) and covariance matrix ($V_0$) for the coefficients, and the parameters ($\\nu_0, S_0$) for the Inverse-Wishart prior on the error covariance.\n",
        "*   **Outputs:** A dictionary containing the `Y` and `X` matrices, the `priors` dictionary, and other model metadata.\n",
        "*   **Role in Research Pipeline:** This callable is the **BVAR Architect**. It implements the specific model and prior structure described on page 9 (\"Bayesian VARs\"), including the Normal-Wishart prior with Minnesota-style hyperparameters ($\\lambda_1=0.1$, etc.).\n",
        "\n",
        "#### **10. `run_bvar_gibbs_sampler`**\n",
        "\n",
        "*   **Inputs:** The `bvar_setup` dictionary and the `study_config`.\n",
        "*   **Processes:** This is a core computational engine, not an orchestrator. It performs the Gibbs sampling algorithm to draw from the posterior distribution of the BVAR parameters by iteratively drawing from the two conditional posteriors:\n",
        "    1.  Draw $\\Sigma | Y, X, B \\sim \\text{Inverse-Wishart}(\\nu_0+T, S_0 + U'U)$.\n",
        "    2.  Draw $\\text{vec}(B) | Y, X, \\Sigma \\sim \\mathcal{N}(\\bar{\\beta}, \\bar{V})$.\n",
        "    It runs for the specified number of draws, discards the burn-in, and includes the remediated intra-run Geweke convergence diagnostic.\n",
        "*   **Outputs:** A dictionary containing the posterior draws for the coefficient matrices (`B_draws`) and the error covariance matrices (`Sigma_draws`).\n",
        "*   **Role in Research Pipeline:** This callable is the **Bayesian Engine**. It is the direct implementation of the MCMC estimation procedure required to fit the BVAR model as described on page 9.\n",
        "\n",
        "#### **11. `run_phase4_task11_local_projections`**\n",
        "\n",
        "*   **Inputs:** The `analysis_ready_df` and the `study_config`.\n",
        "*   **Processes:** This orchestrator estimates the Local Projections model. It calls `estimate_local_projections`, which loops through each endogenous variable and each forecast horizon `h`. In each loop, it:\n",
        "    1.  Constructs the regression $y_{t+h} = \\beta_h \\text{shock}_t + \\text{controls}_t + \\epsilon_{t+h}$.\n",
        "    2.  Estimates the equation via OLS.\n",
        "    3.  Calculates Newey-West HAC standard errors to account for the serial correlation induced by the overlapping horizons.\n",
        "*   **Outputs:** A dictionary containing the impulse response functions (the sequence of $\\hat{\\beta}_h$) and their HAC-robust confidence intervals.\n",
        "*   **Role in Research Pipeline:** This callable is the **Robustness Estimator**. It implements the Local Projections method of Jordà (2005), which is used as the primary robustness check for the BVAR impulse responses, as described on page 10 (\"Local projections\").\n",
        "\n",
        "#### **12. `calculate_var_impulse_responses`**\n",
        "\n",
        "*   **Inputs:** The `posterior_draws` and `bvar_setup` dictionaries, and the `study_config`.\n",
        "*   **Processes:** This function translates the BVAR posterior draws into impulse responses. For each of the 15,000 posterior draws of the coefficients, it:\n",
        "    1.  Constructs the VAR's companion matrix `F`.\n",
        "    2.  Recursively calculates the impulse response functions up to the specified horizon using matrix powers: $\\Psi_h = J F^h J' B_0$.\n",
        "    3.  It then summarizes the resulting 15,000 IRF draws by calculating their posterior median (the point estimate) and percentiles (the credible interval).\n",
        "*   **Outputs:** A dictionary containing the summarized IRFs (median, lower/upper bounds) and a significance indicator.\n",
        "*   **Role in Research Pipeline:** This callable is the **Dynamic Interpreter**. It takes the static BVAR coefficient estimates and translates them into the key objects of interest for the paper's analysis: the dynamic impulse response functions that trace the effect of monetary policy shocks over time.\n",
        "\n",
        "#### **13. `run_transmission_channel_analysis`**\n",
        "\n",
        "*   **Inputs:** The `full_analysis_df` (containing all possible variables), the `study_config`, and a structured `channel_specifications` list.\n",
        "*   **Processes:** This is a high-level \"campaign manager\" orchestrator. It iterates through the provided specifications. For each, it creates an augmented set of endogenous variables (either by adding a group or by iterating one-by-one) and re-runs the entire BVAR pipeline (Tasks 9, 10, 12) on this new model specification.\n",
        "*   **Outputs:** A dictionary where keys are the channel names (e.g., `financial_channel`, `oil_exports_channel`) and values are the full IRF results for that specific augmented model.\n",
        "*   **Role in Research Pipeline:** This callable is the **Channel Investigator**. It directly implements the analysis described in Section 4 of the paper, where the baseline model is augmented with trade, oil, and financial variables to investigate the specific transmission channels of the monetary policy shocks.\n",
        "\n",
        "#### **14. `run_full_model_validation_suite`**\n",
        "\n",
        "*   **Inputs:** The `analysis_ready_df`, the estimated BVAR `bvar_setup` and `posterior_draws`, and the `study_config`.\n",
        "*   **Processes:** This orchestrator runs a suite of diagnostic and validation checks.\n",
        "    1.  It calls `assess_in_sample_fit` to compute RMSE and the Log Marginal Likelihood for the BVAR, and compares the BVAR's in-sample fit to the LP's using a Diebold-Mariano test.\n",
        "    2.  It calls the remediated `evaluate_forecasting_performance_full` to conduct a rigorous, computationally intensive, recursive out-of-sample forecasting horse race between the BVAR and LP models.\n",
        "    3.  It orchestrates the re-estimation of the BVAR for multiple lag lengths to compute information criteria (AIC, BIC, HQ) for lag selection.\n",
        "*   **Outputs:** A dictionary of `pd.DataFrame` reports summarizing all in-sample, out-of-sample, and diagnostic test results.\n",
        "*   **Role in Research Pipeline:** This callable is the **Skeptic**. Its purpose is to rigorously challenge the validity of the baseline model by assessing its fit, its forecasting performance, and the appropriateness of its specification choices, as is standard practice in any high-quality empirical paper.\n",
        "\n",
        "#### **15. `run_transatlantic_spillovers_analysis` (Master Orchestrator)**\n",
        "\n",
        "*   **Inputs:** The four raw `pd.DataFrame`s, the `target_market` string, and the `study_config`.\n",
        "*   **Processes:** This is the top-level master orchestrator. It executes the entire analytical workflow in the correct, logical sequence by calling the task-specific orchestrators from Task 1 through Task 14. Its refactored design ensures a clean separation of concerns between data cleaning (Phase 1) and model-specific data transformation (Phase 3).\n",
        "*   **Outputs:** A single, comprehensive, nested dictionary containing all inputs, intermediate data products, final results, and validation reports generated during the entire pipeline run.\n",
        "*   **Role in Research Pipeline:** This callable is the **Director**. It is the single entry point that executes the entire research paper from start to finish, ensuring perfect reproducibility.\n",
        "\n",
        "#### **16. `run_identification_robustness_suite`**\n",
        "\n",
        "*   **Inputs:** The results object from a benchmark run and the original raw data.\n",
        "*   **Processes:** This orchestrator manages the suite of robustness checks related to the shock identification.\n",
        "    1.  It calls `run_pmsr_robustness_analysis` to re-run the pipeline using the simpler PMSR identification.\n",
        "    2.  It calls `run_rotational_uncertainty_analysis` to re-run the pipeline 1000s of times in parallel for different valid rotation angles to quantify identification uncertainty.\n",
        "    3.  It calls `run_alternative_sample_analysis` to re-run the entire pipeline on truncated datasets (e.g., pre-COVID).\n",
        "*   **Outputs:** A dictionary containing the detailed results from each of the three identification robustness analyses.\n",
        "*   **Role in Research Pipeline:** This callable is the **Identification Validator**. It directly implements the robustness checks described in Section 5 of the paper, which are designed to ensure the main conclusions are not artifacts of the specific identification scheme or sample period.\n",
        "\n",
        "#### **17. `run_estimation_robustness_suite`**\n",
        "\n",
        "*   **Inputs:** The `analysis_ready_df` and the `study_config`.\n",
        "*   **Processes:** This orchestrator manages the suite of robustness checks related to the BVAR model's specification.\n",
        "    1.  It calls `run_prior_sensitivity_analysis` to re-estimate the model with different prior hyperparameter values.\n",
        "    2.  It calls `run_lag_length_robustness_analysis` to re-estimate the model with different lag lengths.\n",
        "    3.  It calls `run_specification_robustness_analysis` to re-estimate the model with different data transformations or deterministic components.\n",
        "*   **Outputs:** A dictionary containing the detailed results from each of the three estimation robustness analyses.\n",
        "*   **Role in Research Pipeline:** This callable is the **Model Validator**. It implements the final set of robustness checks, ensuring the results are not overly sensitive to specific modeling choices like the tightness of the prior or the number of lags included.\n",
        "\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "### **Example Usage of the End-to-End Pipeline**\n",
        "\n",
        "This document provides a complete, implementation-grade example of how to use the top-level orchestrator function, `execute_full_study_pipeline`. It covers the setup of all required inputs and the interpretation of the structured output.\n",
        "\n",
        "#### **Prerequisites**\n",
        "\n",
        "Before executing the pipeline, ensure the following conditions are met:\n",
        "\n",
        "1.  **Environment:** A Python environment with all necessary libraries installed is active. The consolidated import block provided previously contains the full list of dependencies (e.g., `pandas`, `numpy`, `pyyaml`, `statsmodels`, `joblib`, `tqdm`, `pandas_market_calendars`).\n",
        "2.  **Code Availability:** All the modular functions and orchestrators developed and remediated throughout Tasks 1-17 are available in the Python session, either in the same script or imported from their respective modules.\n",
        "3.  **Configuration File:** The `config.yaml` file, as previously generated, is present in the working directory.\n",
        "\n",
        "#### **Step 1: Load the Study Configuration from YAML**\n",
        "\n",
        "The first step in any run is to load the study parameters from the external configuration file. This is a critical best practice that separates the model's specification from its implementation, allowing for easy modification and experimentation without altering the source code.\n",
        "\n",
        "```python\n",
        "import yaml\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Define the path to the configuration file.\n",
        "config_path = \"config.yaml\"\n",
        "\n",
        "# Open and parse the YAML file into a Python dictionary.\n",
        "# The `yaml.safe_load` function is used to prevent arbitrary code execution.\n",
        "with open(config_path, 'r') as f:\n",
        "    study_config: Dict[str, Any] = yaml.safe_load(f)\n",
        "\n",
        "# At this point, `study_config` is a nested dictionary containing all\n",
        "# parameters needed to control the pipeline's execution.\n",
        "```\n",
        "\n",
        "#### **Step 2: Prepare Input Data Structures**\n",
        "\n",
        "The pipeline requires four raw `pandas` DataFrames as input. For this example, we will generate **illustrative** synthetic but structurally correct data for each. In a real-world scenario, this data would be loaded from a database, CSV files, or a data vendor API.\n",
        "\n",
        "**A. Announcement Metadata (`announcement_df`)**\n",
        "\n",
        "This DataFrame contains the schedule of monetary policy announcements.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample DataFrame for two announcements.\n",
        "announcement_data = {\n",
        "    'event_id': [101, 201],\n",
        "    'central_bank': ['FED', 'ECB'],\n",
        "    'announcement_date_local': pd.to_datetime(['2022-03-16', '2022-03-10']),\n",
        "    'announcement_time_local': ['14:00:00', '13:45:00'],\n",
        "    'local_timezone': ['America/New_York', 'Europe/Berlin']\n",
        "}\n",
        "announcement_df = pd.DataFrame(announcement_data)\n",
        "\n",
        "# Ensure categorical data types are correctly set.\n",
        "announcement_df['central_bank'] = announcement_df['central_bank'].astype('category')\n",
        "```\n",
        "\n",
        "**B. High-Frequency Tick Data (`equity_tick_df` and `rate_tick_df`)**\n",
        "\n",
        "This data represents the raw, tick-by-tick trades for the relevant financial instruments. We generate a small window of data around each announcement.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# --- Generate synthetic tick data for the FED announcement ---\n",
        "fed_ann_time_utc = pd.Timestamp('2022-03-16 18:00:00', tz='UTC')\n",
        "fed_time_range = pd.date_range(\n",
        "    start=fed_ann_time_utc - pd.Timedelta(hours=1),\n",
        "    end=fed_ann_time_utc + pd.Timedelta(hours=1),\n",
        "    freq='1S' # Generate one tick per second for simplicity\n",
        ")\n",
        "fed_ticks = pd.DataFrame({\n",
        "    'timestamp_micros_utc': fed_time_range,\n",
        "    'price': 100 + np.random.randn(len(fed_time_range)).cumsum() * 0.01,\n",
        "    'volume': np.random.randint(1, 100, size=len(fed_time_range)),\n",
        "    'type': 'TRADE'\n",
        "})\n",
        "\n",
        "# --- Generate synthetic tick data for the ECB announcement ---\n",
        "ecb_ann_time_utc = pd.Timestamp('2022-03-10 12:45:00', tz='UTC')\n",
        "ecb_time_range = pd.date_range(\n",
        "    start=ecb_ann_time_utc - pd.Timedelta(hours=1),\n",
        "    end=ecb_ann_time_utc + pd.Timedelta(hours=1),\n",
        "    freq='1S'\n",
        ")\n",
        "ecb_ticks = pd.DataFrame({\n",
        "    'timestamp_micros_utc': ecb_time_range,\n",
        "    'price': 98.5 + np.random.randn(len(ecb_time_range)).cumsum() * 0.005,\n",
        "    'volume': np.random.randint(1, 100, size=len(ecb_time_range)),\n",
        "    'type': 'TRADE'\n",
        "})\n",
        "\n",
        "# For this example, we use the same data for equity and rate futures.\n",
        "# In a real scenario, these would be two distinct datasets.\n",
        "equity_tick_df = pd.concat([fed_ticks, ecb_ticks]).reset_index(drop=True)\n",
        "rate_tick_df = equity_tick_df.copy() # Use a copy\n",
        "rate_tick_df['price'] = 100 - (rate_tick_df['price'] / 50) # Simulate a rate-like price\n",
        "\n",
        "# Ensure categorical data types are correctly set.\n",
        "equity_tick_df['type'] = equity_tick_df['type'].astype('category')\n",
        "rate_tick_df['type'] = rate_tick_df['type'].astype('category')\n",
        "```\n",
        "\n",
        "**C. Macroeconomic Time Series Data (`macro_df`)**\n",
        "\n",
        "This DataFrame contains the monthly macroeconomic data in a long (\"tidy\") format.\n",
        "\n",
        "```python\n",
        "# Create a monthly date range for the sample period.\n",
        "date_rng = pd.date_range(start='2020-01-01', end='2023-12-31', freq='MS')\n",
        "n_months = len(date_rng)\n",
        "\n",
        "# Generate synthetic data for Canada (the target market) and the US (a foreign country).\n",
        "macro_data_list = []\n",
        "for country in ['CAN', 'USA']:\n",
        "    for var in ['gdp', 'cpi', 'policy_rate', 'equity_index', 'exchange_rate']:\n",
        "        base_val = 1000 if var == 'gdp' else 100\n",
        "        data = {\n",
        "            'date': date_rng,\n",
        "            'source_series_id': f'{country}_{var.upper()}_M',\n",
        "            'country': country,\n",
        "            'variable_name': var,\n",
        "            'value_raw': base_val + np.random.randn(n_months).cumsum()\n",
        "        }\n",
        "        macro_data_list.append(pd.DataFrame(data))\n",
        "\n",
        "macro_df = pd.concat(macro_data_list, ignore_index=True)\n",
        "\n",
        "# Ensure categorical data types are correctly set.\n",
        "macro_df['country'] = macro_df['country'].astype('category')\n",
        "```\n",
        "\n",
        "#### **Step 3: Define Other Inputs**\n",
        "\n",
        "The remaining inputs are the `target_market` string and the boolean flags to control the execution of the computationally intensive robustness checks. For a demonstration, it is prudent to disable them.\n",
        "\n",
        "```python\n",
        "# Define the target market for the analysis.\n",
        "target_market = 'CAN'\n",
        "\n",
        "# Define flags to control the execution of the robustness suites.\n",
        "# Set to False for a faster initial run. Set to True for a full, rigorous analysis.\n",
        "run_identification_robustness = False\n",
        "run_estimation_robustness = False\n",
        "```\n",
        "\n",
        "#### **Step 4: Execute the Full Pipeline**\n",
        "\n",
        "With all inputs prepared, the entire research pipeline can be executed with a single function call.\n",
        "\n",
        "```python\n",
        "# This is the single entry point that runs the entire study.\n",
        "# It is assumed that `execute_full_study_pipeline` and all its dependencies\n",
        "# have been defined in the current session.\n",
        "\n",
        "master_results = execute_full_study_pipeline(\n",
        "    equity_tick_df=equity_tick_df,\n",
        "    rate_tick_df=rate_tick_df,\n",
        "    macro_df=macro_df,\n",
        "    announcement_df=announcement_df,\n",
        "    target_market=target_market,\n",
        "    study_config=study_config,\n",
        "    run_identification_robustness=run_identification_robustness,\n",
        "    run_estimation_robustness=run_estimation_robustness\n",
        ")\n",
        "```\n",
        "\n",
        "#### **Step 5: Inspect the Output**\n",
        "\n",
        "The `master_results` object is a deeply nested dictionary containing every artifact generated during the run. This structure allows for a complete and transparent audit of the entire analysis.\n",
        "\n",
        "```python\n",
        "# --- Check the overall status and metadata of the run ---\n",
        "print(\"--- Run Metadata ---\")\n",
        "print(f\"Final Status: {master_results['metadata']['final_status']}\")\n",
        "print(f\"Total Runtime: {master_results['metadata']['total_runtime_seconds']} seconds\")\n",
        "if master_results['metadata']['final_status'] == 'FAIL':\n",
        "    print(f\"Error Message: {master_results['metadata']['error_message']}\")\n",
        "\n",
        "# --- Inspect a key intermediate output: the identified shocks ---\n",
        "if master_results['metadata']['final_status'] == 'SUCCESS':\n",
        "    print(\"\\n--- Identified Structural Shocks (FED) ---\")\n",
        "    fed_shocks = master_results['phase_2_identification']['structural_shocks']['FED']\n",
        "    # In the remediated version, this is a DataFrame indexed by event_id\n",
        "    print(fed_shocks.head())\n",
        "\n",
        "    # --- Inspect a key final output: the BVAR impulse responses ---\n",
        "    print(\"\\n--- BVAR Impulse Response Functions (Median Estimates) ---\")\n",
        "    bvar_irfs_median = master_results['phase_5_results']['bvar_irfs']['irf_median']\n",
        "    # Shape is (num_endog_vars, num_shocks, horizon + 1)\n",
        "    print(f\"Shape of IRF tensor: {bvar_irfs_median.shape}\")\n",
        "\n",
        "    # Example: Get the response of the first endogenous variable to the first shock\n",
        "    endog_vars = study_config['bvar_specification']['endogenous_variables']\n",
        "    shock_vars = [v for v in study_config['bvar_specification']['exogenous_variables'] if 'shock' in v]\n",
        "    \n",
        "    print(f\"\\nResponse of '{endog_vars[0]}' to a '{shock_vars[0]}' shock:\")\n",
        "    print(bvar_irfs_median[0, 0, :])\n",
        "\n",
        "    # --- Inspect a validation report ---\n",
        "    print(\"\\n--- Lag Selection Report ---\")\n",
        "    # This report is only generated if the estimation robustness suite is run.\n",
        "    if 'estimation_robustness_suite' in master_results:\n",
        "        lag_report = master_results['estimation_robustness_suite']['lag_selection_report']\n",
        "        print(lag_report)\n",
        "    else:\n",
        "        print(\"Estimation robustness suite was not run.\")\n",
        "```"
      ],
      "metadata": {
        "id": "1nGab4I6Y1FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Parameter and Data Structure Validation\n",
        "\n",
        "# =============================================================================\n",
        "# CUSTOM EXCEPTION CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "class ConfigurationValidationError(ValueError):\n",
        "    \"\"\"\n",
        "    Custom exception raised for critical errors in the study configuration.\n",
        "\n",
        "    Purpose:\n",
        "        This exception is specifically designed to be raised when the main\n",
        "        `study_configuration` dictionary fails validation checks. It signals\n",
        "        that the foundational parameters of the analysis are incorrect,\n",
        "        preventing the pipeline from proceeding with potentially invalid\n",
        "        assumptions.\n",
        "\n",
        "    Attributes:\n",
        "        report (pd.DataFrame): A detailed DataFrame containing the results of\n",
        "                               every validation check performed on the\n",
        "                               configuration. This report includes timestamps,\n",
        "                               the parameter path, expected vs. actual values,\n",
        "                               and a pass/fail status for each check, providing\n",
        "                               a comprehensive audit trail for debugging.\n",
        "\n",
        "    Process:\n",
        "        The exception is initialized with an error message and the validation\n",
        "        report DataFrame. It extends Python's built-in `ValueError` and\n",
        "        overrides the constructor to store the report. The error message\n",
        "        presented to the user is automatically augmented with a string\n",
        "        representation of this report, ensuring that the context of the\n",
        "        failure is immediately clear.\n",
        "    \"\"\"\n",
        "    def __init__(self, message: str, report: pd.DataFrame) -> NoReturn:\n",
        "        # Store the detailed validation report as an attribute of the exception instance.\n",
        "        # This allows programmatic access to the validation results in an error handling block.\n",
        "        self.report: pd.DataFrame = report\n",
        "\n",
        "        # Construct a comprehensive error message that includes the high-level summary\n",
        "        # and the full, detailed report for immediate diagnosis.\n",
        "        full_message: str = f\"{message}\\nValidation Report:\\n{report.to_string()}\"\n",
        "\n",
        "        # Call the parent class (ValueError) constructor to properly initialize the exception.\n",
        "        super().__init__(full_message)\n",
        "\n",
        "\n",
        "class SchemaError(ValueError):\n",
        "    \"\"\"\n",
        "    Custom exception raised for DataFrame schema and data type violations.\n",
        "\n",
        "    Purpose:\n",
        "        This exception indicates that an input `pd.DataFrame` does not conform\n",
        "        to its required schema. This could involve missing columns, unexpected\n",
        "        extra columns, incorrect data types, or invalid categorical levels.\n",
        "        It is a critical data integrity check that prevents downstream\n",
        "        processing errors.\n",
        "\n",
        "    Process:\n",
        "        This exception is raised by schema validation functions when a\n",
        "        discrepancy is detected. The error message should be specific,\n",
        "        detailing the exact nature of the schema mismatch (e.g., listing\n",
        "        the missing columns) to facilitate rapid correction of the input data.\n",
        "        It extends `ValueError` without adding new attributes, serving as a\n",
        "        semantically distinct error type for targeted `try...except` blocks.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class EmptyDataError(ValueError):\n",
        "    \"\"\"\n",
        "    Custom exception raised when a required input DataFrame is empty or None.\n",
        "\n",
        "    Purpose:\n",
        "        This exception is raised as a preliminary check to ensure that essential\n",
        "        input `pd.DataFrame` objects (e.g., tick data, macro data) contain\n",
        "        data before any processing is attempted. Running the pipeline on empty\n",
        "        data would lead to cryptic errors or incorrect results.\n",
        "\n",
        "    Process:\n",
        "        Validation functions should check if a DataFrame is `None` or if its\n",
        "        `empty` attribute is `True` at the very beginning of their execution.\n",
        "        If so, this exception should be raised with a message identifying the\n",
        "        problematic DataFrame. It extends `ValueError` and provides a clear,\n",
        "        early-exit signal that a required data source is missing or invalid.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 1, STEP 1: CONFIGURATION DICTIONARY DEEP VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_config_parameter(\n",
        "    config: Dict[str, Any],\n",
        "    path: List[str],\n",
        "    expected_value: Any,\n",
        "    validation_type: str,\n",
        "    report_data: List[Dict[str, Any]]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Helper function to validate a single parameter within the configuration.\n",
        "\n",
        "    This function navigates a nested dictionary using a path, compares the\n",
        "    actual value to an expected value, and appends the result to a report list.\n",
        "    It handles missing keys and type errors gracefully.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "        path (List[str]): A list of keys representing the path to the parameter.\n",
        "        expected_value (Any): The expected value of the parameter.\n",
        "        validation_type (str): The type of validation ('exact' or 'isclose').\n",
        "        report_data (List[Dict[str, Any]]): A list to which the validation\n",
        "                                            result dictionary will be appended.\n",
        "    \"\"\"\n",
        "    # Initialize report entry with common fields\n",
        "    path_str = \".\".join(path)\n",
        "    result_entry = {\n",
        "        \"timestamp\": pd.Timestamp.now(tz='UTC'),\n",
        "        \"validation_type\": \"Configuration Parameter\",\n",
        "        \"parameter_path\": path_str,\n",
        "        \"expected_value\": str(expected_value),\n",
        "        \"actual_value\": \"N/A\",\n",
        "        \"pass_fail\": \"FAIL\",\n",
        "        \"error_message\": \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Traverse the nested dictionary to get the actual value\n",
        "        actual_value = config\n",
        "        for key in path:\n",
        "            actual_value = actual_value[key]\n",
        "        result_entry[\"actual_value\"] = str(actual_value)\n",
        "\n",
        "        # Perform validation based on the specified type\n",
        "        is_pass = False\n",
        "        if validation_type == 'isclose':\n",
        "            # Use numpy.isclose for robust floating-point comparison\n",
        "            if not isinstance(actual_value, (int, float)):\n",
        "                raise TypeError(\"Value is not a number.\")\n",
        "            is_pass = np.isclose(\n",
        "                actual_value,\n",
        "                expected_value,\n",
        "                rtol=1e-9,\n",
        "                atol=1e-12\n",
        "            )\n",
        "        elif validation_type == 'exact':\n",
        "            # Use exact string matching, stripping whitespace\n",
        "            if isinstance(actual_value, str):\n",
        "                is_pass = actual_value.strip() == expected_value\n",
        "            else:\n",
        "                is_pass = actual_value == expected_value\n",
        "\n",
        "        # Update the report entry based on the validation outcome\n",
        "        if is_pass:\n",
        "            result_entry[\"pass_fail\"] = \"PASS\"\n",
        "        else:\n",
        "            result_entry[\"error_message\"] = \"Actual value does not match expected value.\"\n",
        "\n",
        "    except KeyError:\n",
        "        # Handle cases where a key in the path does not exist\n",
        "        result_entry[\"error_message\"] = f\"Key not found in configuration: {path_str}\"\n",
        "    except TypeError as e:\n",
        "        # Handle type errors during comparison (e.g., comparing float with string)\n",
        "        result_entry[\"error_message\"] = f\"Type error during validation: {e}\"\n",
        "\n",
        "    # Append the detailed result to the main report list\n",
        "    report_data.append(result_entry)\n",
        "\n",
        "\n",
        "def validate_study_configuration(\n",
        "    study_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a deep validation of the study_configuration dictionary.\n",
        "\n",
        "    This function meticulously checks for the presence and correctness of all\n",
        "    critical parameters specified in the project plan. It verifies nested keys,\n",
        "    data types, and specific values required for the analysis to proceed\n",
        "    correctly, using numpy.isclose for floating-point comparisons.\n",
        "\n",
        "    Args:\n",
        "        study_config (Dict[str, Any]): The configuration dictionary for the\n",
        "                                       entire study.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A detailed report of the validation process, with each\n",
        "                      row representing a specific check.\n",
        "\n",
        "    Raises:\n",
        "        ConfigurationValidationError: If any critical parameter fails validation,\n",
        "                                      this exception is raised, containing the\n",
        "                                      full validation report.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store dictionaries, which is more efficient for appending\n",
        "    report_data: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Define the checklist of parameters to validate\n",
        "    # Each tuple contains: (path_list, expected_value, validation_type)\n",
        "    validation_checklist = [\n",
        "        # Top-level keys\n",
        "        ([\"identification_params\"], None, 'exists'),\n",
        "        ([\"bvar_specification\"], None, 'exists'),\n",
        "        ([\"bvar_priors\"], None, 'exists'),\n",
        "        ([\"local_projection_specification\"], None, 'exists'),\n",
        "        ([\"computation_settings\"], None, 'exists'),\n",
        "        # Nested parameters with specific values\n",
        "        ([\"identification_params\", \"method\"], \"rotational_angle_decomposition\", 'exact'),\n",
        "        ([\"identification_params\", \"benchmark_selection\"], \"median_rotation\", 'exact'),\n",
        "        ([\"bvar_specification\", \"lags\"], 3, 'exact'),\n",
        "        ([\"bvar_priors\", \"hyperparameters\", \"ar_coeff_prior\"], 0.8, 'isclose'),\n",
        "        ([\"bvar_priors\", \"hyperparameters\", \"lambda_1_overall_tightness\"], 0.1, 'isclose'),\n",
        "        ([\"bvar_priors\", \"hyperparameters\", \"lambda_3_lag_decay\"], 1.0, 'isclose'),\n",
        "        ([\"local_projection_specification\", \"lags_dependent_variable\"], 3, 'exact'),\n",
        "        ([\"computation_settings\", \"impulse_response_functions\", \"credible_interval_level\"], 0.90, 'isclose'),\n",
        "    ]\n",
        "\n",
        "    # Iterate through the checklist and validate each parameter\n",
        "    for path, expected, v_type in validation_checklist:\n",
        "        # The 'exists' type is a special case to check for key presence\n",
        "        if v_type == 'exists':\n",
        "            # Use dict.get() to safely check for the existence of top-level keys\n",
        "            if study_config.get(path[0]) is None:\n",
        "                report_data.append({\n",
        "                    \"timestamp\": pd.Timestamp.now(tz='UTC'),\n",
        "                    \"validation_type\": \"Configuration Key Presence\",\n",
        "                    \"parameter_path\": path[0],\n",
        "                    \"expected_value\": \"Exists\",\n",
        "                    \"actual_value\": \"Missing\",\n",
        "                    \"pass_fail\": \"FAIL\",\n",
        "                    \"error_message\": f\"Required top-level key '{path[0]}' is missing.\"\n",
        "                })\n",
        "        else:\n",
        "            # Use the helper function for standard value validation\n",
        "            _validate_config_parameter(study_config, path, expected, v_type, report_data)\n",
        "\n",
        "    # Convert the list of results into a pandas DataFrame\n",
        "    validation_report = pd.DataFrame(report_data)\n",
        "\n",
        "    # Check if any validation failed\n",
        "    if \"FAIL\" in validation_report[\"pass_fail\"].values:\n",
        "        # If any check fails, raise a custom exception with the full report\n",
        "        raise ConfigurationValidationError(\n",
        "            \"Critical error in study configuration.\",\n",
        "            report=validation_report\n",
        "        )\n",
        "\n",
        "    # If all checks pass, return the successful validation report\n",
        "    return validation_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 1, STEP 2: DATAFRAME SCHEMA AND DATA TYPE VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "def validate_dataframe_schema(\n",
        "    df: pd.DataFrame,\n",
        "    df_name: str,\n",
        "    schema: Dict[str, Union[type, Tuple[type, Set[str]]]],\n",
        "    expected_frequency: Optional[str] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates the schema of a single DataFrame against a detailed definition.\n",
        "\n",
        "    Purpose:\n",
        "        This function provides a rigorous and comprehensive validation of a\n",
        "        DataFrame's structure. It serves as a critical data integrity gateway,\n",
        "        ensuring that input data conforms to all expected column names, data\n",
        "        types (including timezone awareness), categorical levels, and,\n",
        "        optionally, time-series frequency.\n",
        "\n",
        "    Process:\n",
        "        1.  **Initial Checks**: Verifies that the input DataFrame is not None or empty.\n",
        "        2.  **Column Conformance**: Checks for an exact match between the\n",
        "            DataFrame's columns and the required columns defined in the schema.\n",
        "            It reports both missing and unexpected extra columns.\n",
        "        3.  **Column-wise Validation**: For each column specified in the schema:\n",
        "            - **Data Type**: Uses the precise `pandas.api.types` module to\n",
        "              verify the data type. For datetimes, it also validates timezone\n",
        "              awareness against the schema.\n",
        "            - **Categorical Levels**: For categorical columns, it verifies that\n",
        "              the set of categories is an exact match to the expected set.\n",
        "        4.  **Frequency Validation (Optional)**: If `expected_frequency` is\n",
        "            provided (e.g., 'MS' for monthly), it performs a check to ensure\n",
        "            the time series conforms to this frequency by analyzing the modal\n",
        "            difference between consecutive dates.\n",
        "        5.  **Reporting**: Compiles the results of all checks into a single,\n",
        "            detailed compliance report DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to validate.\n",
        "        df_name (str): The name of the DataFrame (for reporting purposes).\n",
        "        schema (Dict[str, Union[type, Tuple[type, Set[str]]]]):\n",
        "            A dictionary defining the required schema. Keys are column names.\n",
        "            Values are either a type (e.g., `np.float64`) or a tuple of\n",
        "            (type, set_of_valid_categories) for categorical columns.\n",
        "        expected_frequency (Optional[str]): If not None, specifies the expected\n",
        "            pandas frequency string (e.g., 'MS'). Triggers a time-series\n",
        "            frequency check.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A report detailing the compliance of each column and the\n",
        "                      overall frequency check.\n",
        "\n",
        "    Raises:\n",
        "        EmptyDataError: If the input DataFrame is empty or None.\n",
        "        SchemaError: If the set of columns does not exactly match the schema.\n",
        "    \"\"\"\n",
        "    # --- 1. Initial Checks ---\n",
        "    # A validation function must first ensure it has valid data to operate on.\n",
        "    if df is None or df.empty:\n",
        "        raise EmptyDataError(f\"Input DataFrame '{df_name}' is None or empty.\")\n",
        "\n",
        "    # --- 2. Column Conformance ---\n",
        "    # Check for an exact match between the required and actual column sets.\n",
        "    required_cols = set(schema.keys())\n",
        "    actual_cols = set(df.columns)\n",
        "\n",
        "    if required_cols != actual_cols:\n",
        "        # If they don't match, provide a detailed error message.\n",
        "        missing_cols = required_cols - actual_cols\n",
        "        extra_cols = actual_cols - required_cols\n",
        "        error_msg = f\"Schema mismatch for DataFrame '{df_name}'.\"\n",
        "        if missing_cols:\n",
        "            error_msg += f\" Missing columns: {sorted(list(missing_cols))}.\"\n",
        "        if extra_cols:\n",
        "            error_msg += f\" Extra columns found: {sorted(list(extra_cols))}.\"\n",
        "        raise SchemaError(error_msg)\n",
        "\n",
        "    # --- 3. Column-wise Validation ---\n",
        "    # Initialize a list to store compliance results for each column.\n",
        "    compliance_report_data: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Iterate through the defined schema to check each column individually.\n",
        "    for col, spec in schema.items():\n",
        "        status = \"PASS\"\n",
        "        message = \"\"\n",
        "        expected_type = spec[0] if isinstance(spec, tuple) else spec\n",
        "        actual_type = df[col].dtype\n",
        "\n",
        "        # Use the precise pandas.api.types module for robust dtype checking.\n",
        "        type_match = False\n",
        "        if pd.api.types.is_datetime64_ns_dtype(expected_type):\n",
        "            if pd.api.types.is_datetime64_ns_dtype(actual_type):\n",
        "                # For datetimes, timezone awareness is a critical part of the type.\n",
        "                if getattr(expected_type, 'tz', None) is not None:\n",
        "                    if df[col].dt.tz is None:\n",
        "                        status, message = \"FAIL\", \"Timestamp column is timezone-naive; expected aware.\"\n",
        "                    elif str(df[col].dt.tz) != str(expected_type.tz):\n",
        "                        status, message = \"FAIL\", f\"Incorrect timezone; expected {expected_type.tz}, got {df[col].dt.tz}.\"\n",
        "                    else:\n",
        "                        type_match = True\n",
        "                else: # Expected is timezone-naive\n",
        "                    if df[col].dt.tz is not None:\n",
        "                        status, message = \"FAIL\", \"Timestamp column is timezone-aware; expected naive.\"\n",
        "                    else:\n",
        "                        type_match = True\n",
        "            else:\n",
        "                status, message = \"FAIL\", \"Expected datetime64[ns] dtype.\"\n",
        "        # Check other common dtypes.\n",
        "        elif pd.api.types.is_categorical_dtype(expected_type):\n",
        "            type_match = pd.api.types.is_categorical_dtype(actual_type)\n",
        "        elif pd.api.types.is_float_dtype(expected_type):\n",
        "            type_match = pd.api.types.is_float_dtype(actual_type)\n",
        "        elif pd.api.types.is_integer_dtype(expected_type):\n",
        "            type_match = pd.api.types.is_integer_dtype(actual_type)\n",
        "        elif pd.api.types.is_object_dtype(expected_type):\n",
        "            type_match = pd.api.types.is_object_dtype(actual_type)\n",
        "\n",
        "        if not type_match and status == \"PASS\":\n",
        "            status, message = \"FAIL\", f\"Type mismatch. Expected: {expected_type}, Actual: {actual_type}.\"\n",
        "\n",
        "        # If the column is categorical, validate its defined levels.\n",
        "        if status == \"PASS\" and isinstance(spec, tuple):\n",
        "            expected_categories = spec[1]\n",
        "            actual_categories = set(df[col].cat.categories)\n",
        "            if actual_categories != expected_categories:\n",
        "                status, message = \"FAIL\", f\"Category mismatch. Expected: {expected_categories}, Actual: {actual_categories}.\"\n",
        "\n",
        "        # Append the detailed result for the current column to the report list.\n",
        "        compliance_report_data.append({\n",
        "            \"dataframe_name\": df_name, \"column_name\": col, \"expected_spec\": str(spec),\n",
        "            \"actual_dtype\": str(actual_type), \"compliance_status\": status, \"message\": message\n",
        "        })\n",
        "\n",
        "    # --- 4. Frequency Validation (Optional) ---\n",
        "    # This new block implements the previously omitted frequency check.\n",
        "    if expected_frequency:\n",
        "        freq_status = \"PASS\"\n",
        "        freq_message = \"\"\n",
        "        # The check requires a 'date' column of a datetime type.\n",
        "        if 'date' not in df.columns or not pd.api.types.is_datetime64_ns_dtype(df['date'].dtype):\n",
        "            freq_status, freq_message = \"FAIL\", \"Frequency check requires a 'date' column of datetime dtype.\"\n",
        "        else:\n",
        "            # Sort by date to ensure differences are chronological.\n",
        "            day_diffs = df['date'].sort_values().diff().dt.days.dropna()\n",
        "\n",
        "            if day_diffs.empty:\n",
        "                freq_status, freq_message = \"WARN\", \"Not enough data points to check frequency.\"\n",
        "            else:\n",
        "                modal_diff = day_diffs.mode()\n",
        "                # Handle the edge case of an empty mode result.\n",
        "                if modal_diff.empty:\n",
        "                    freq_status, freq_message = \"FAIL\", \"Could not determine a modal frequency.\"\n",
        "                elif expected_frequency == 'MS': # Monthly check\n",
        "                    # Check if the most common difference is within the range for a month.\n",
        "                    if not (28 <= modal_diff[0] <= 31):\n",
        "                        freq_status, freq_message = \"FAIL\", f\"Expected monthly frequency, but modal day difference is {modal_diff[0]}.\"\n",
        "\n",
        "        # Append the frequency check result to the report.\n",
        "        compliance_report_data.append({\n",
        "            \"dataframe_name\": df_name, \"column_name\": \"_FREQUENCY_CHECK_\", \"expected_spec\": expected_frequency,\n",
        "            \"actual_dtype\": \"N/A\", \"compliance_status\": freq_status, \"message\": freq_message\n",
        "        })\n",
        "\n",
        "    # --- 5. Reporting ---\n",
        "    # Convert the list of results into the final pandas DataFrame report.\n",
        "    return pd.DataFrame(compliance_report_data)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 1, STEP 3: CROSS-DATASET ALIGNMENT VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "def validate_cross_dataset_alignment(\n",
        "    equity_tick_df: pd.DataFrame,\n",
        "    rate_tick_df: pd.DataFrame,\n",
        "    macro_df: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    target_market: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs comprehensive alignment checks across all input datasets.\n",
        "\n",
        "    Purpose:\n",
        "        This function serves as a critical data integrity gateway. It verifies\n",
        "        that all disparate datasets (high-frequency, low-frequency, event metadata)\n",
        "        are logically consistent and temporally aligned before any analysis is\n",
        "        attempted. It checks for content (variable coverage), time (event\n",
        "        coverage), and data quality (macroeconomic series gaps).\n",
        "\n",
        "    Process:\n",
        "        1.  **Content Validation**:\n",
        "            - Verifies that the specified `target_market` exists in the macro data.\n",
        "            - Ensures that all required macroeconomic variables for that market\n",
        "              are present in the dataset.\n",
        "        2.  **Temporal Alignment**:\n",
        "            - **UTC Conversion**: Calls the robust `standardize_announcement_timestamps`\n",
        "              function to accurately convert all local announcement times to UTC,\n",
        "              correctly handling timezones and Daylight Saving Time.\n",
        "            - **Range Check**: Determines the full temporal range of the high-\n",
        "              frequency tick data.\n",
        "            - **Buffer Implementation**: Adds a 24-hour buffer to each end of\n",
        "              the tick data range.\n",
        "            - **Verification**: Checks if all UTC announcement timestamps fall\n",
        "              within this buffered range. Any events outside this range are\n",
        "              flagged as critical errors.\n",
        "        3.  **Macroeconomic Gap Analysis**:\n",
        "            - **Pivoting**: Transforms the long-format macro data for the target\n",
        "              market into a wide-format DataFrame.\n",
        "            - **Densification**: Reindexes the data to a complete, uninterrupted\n",
        "              monthly time grid.\n",
        "            - **Gap Detection**: Identifies any gaps in the series and flags\n",
        "              series with gaps exceeding 3 consecutive months as critical failures.\n",
        "\n",
        "    Args:\n",
        "        equity_tick_df (pd.DataFrame): High-frequency equity data.\n",
        "        rate_tick_df (pd.DataFrame): High-frequency interest rate data.\n",
        "        macro_df (pd.DataFrame): Monthly macroeconomic time series data.\n",
        "        announcement_df (pd.DataFrame): Central bank announcement metadata.\n",
        "        target_market (str): The market to study (e.g., 'CAN').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing a summary status and detailed\n",
        "                        report DataFrames for each alignment check.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a critical alignment check fails (e.g., target market\n",
        "                    not found, announcements outside tick data range, or macro\n",
        "                    gaps > 3 months).\n",
        "    \"\"\"\n",
        "    # --- Initialization of the results and reporting structure ---\n",
        "    # This dictionary will hold the final status and any generated reports.\n",
        "    results: Dict[str, Any] = {\n",
        "        \"overall_status\": \"PASS\",\n",
        "        \"reports\": {}\n",
        "    }\n",
        "\n",
        "    # --- 1. Content Validation: Target Market and Variable Coverage ---\n",
        "    # Check if the specified market exists in the 'country' column.\n",
        "    if target_market not in macro_df['country'].unique():\n",
        "        # If not, this is a fatal configuration error.\n",
        "        raise ValueError(f\"Target market '{target_market}' not found in macroeconomic data's 'country' column.\")\n",
        "\n",
        "    # Filter the macro data to only the target market.\n",
        "    target_macro_df = macro_df[macro_df['country'] == target_market]\n",
        "\n",
        "    # Define the set of essential variables for the core VAR model.\n",
        "    required_vars = {'exchange_rate', 'policy_rate', 'cpi', 'equity_index', 'gdp'}\n",
        "\n",
        "    # Check if all required variables are present for the target market.\n",
        "    present_vars = set(target_macro_df['variable_name'].unique())\n",
        "    if not required_vars.issubset(present_vars):\n",
        "        # If any are missing, this is a fatal data error.\n",
        "        missing_vars = required_vars - present_vars\n",
        "        raise ValueError(f\"Target market '{target_market}' is missing required variables: {sorted(list(missing_vars))}\")\n",
        "\n",
        "    # --- 2. Temporal Alignment Validation ---\n",
        "    # Determine the absolute start and end times of the available tick data.\n",
        "    min_tick_time: pd.Timestamp = min(equity_tick_df['timestamp_micros_utc'].min(), rate_tick_df['timestamp_micros_utc'].min())\n",
        "    max_tick_time: pd.Timestamp = max(equity_tick_df['timestamp_micros_utc'].max(), rate_tick_df['timestamp_micros_utc'].max())\n",
        "\n",
        "    # Define the specified 24-hour buffer.\n",
        "    buffer: pd.Timedelta = pd.Timedelta(hours=24)\n",
        "\n",
        "    # Calculate the valid temporal range, inclusive of the buffer.\n",
        "    valid_start_time: pd.Timestamp = min_tick_time - buffer\n",
        "    valid_end_time: pd.Timestamp = max_tick_time + buffer\n",
        "\n",
        "    try:\n",
        "        # **CRITICAL STEP**: Use the robust, DST-aware function from Task 3 to get accurate UTC timestamps.\n",
        "        # This replaces the previous naive and incorrect conversion logic.\n",
        "        ann_df_with_utc, _ = standardize_announcement_timestamps(announcement_df)\n",
        "        ann_utc_timestamps: pd.Series = ann_df_with_utc['announcement_timestamp_utc']\n",
        "    except Exception as e:\n",
        "        # If the robust conversion itself fails, it's a critical data quality issue.\n",
        "        raise ValueError(f\"Failed to standardize announcement timestamps for alignment check. Error: {e}\")\n",
        "\n",
        "    # Create a boolean mask to identify any announcements that fall outside the valid range.\n",
        "    uncovered_mask: pd.Series = (ann_utc_timestamps < valid_start_time) | (ann_utc_timestamps > valid_end_time)\n",
        "\n",
        "    # Check if any announcements were flagged.\n",
        "    if uncovered_mask.any():\n",
        "        # If so, this is a critical failure.\n",
        "        results['overall_status'] = \"FAIL\"\n",
        "        # Isolate the specific events that are uncovered for clear reporting.\n",
        "        uncovered_announcements: pd.DataFrame = ann_df_with_utc[uncovered_mask]\n",
        "        results['reports']['temporal_alignment_failures'] = uncovered_announcements\n",
        "        # Raise an error with a detailed message and context.\n",
        "        raise ValueError(\n",
        "            f\"{uncovered_mask.sum()} announcements are outside the buffered high-frequency data time range \"\n",
        "            f\"({valid_start_time} to {valid_end_time}). See report for details.\"\n",
        "        )\n",
        "\n",
        "    # --- 3. Macroeconomic Data Gap Analysis ---\n",
        "    # Pivot the data to wide format to make missing values explicit.\n",
        "    macro_wide: pd.DataFrame = target_macro_df.pivot_table(\n",
        "        index='date', columns='variable_name', values='value_raw'\n",
        "    )\n",
        "\n",
        "    # Create a complete monthly date range for the sample period of the macro data.\n",
        "    full_date_range: pd.DatetimeIndex = pd.date_range(\n",
        "        start=macro_wide.index.min(), end=macro_wide.index.max(), freq='MS'\n",
        "    )\n",
        "    # Reindex to the full grid to make any implicit gaps explicit NaNs.\n",
        "    macro_wide = macro_wide.reindex(full_date_range)\n",
        "\n",
        "    # Identify gaps and check for consecutive missing months exceeding the threshold.\n",
        "    gap_report_data: List[Dict[str, Any]] = []\n",
        "    for var in required_vars:\n",
        "        # Isolate the series for the current variable.\n",
        "        series: pd.Series = macro_wide[var]\n",
        "        # Create a boolean series indicating where data is missing.\n",
        "        is_na: pd.Series = series.isna()\n",
        "\n",
        "        if is_na.any():\n",
        "            # Identify contiguous blocks of NaNs.\n",
        "            blocks: pd.Series = (is_na.diff() != 0).cumsum()\n",
        "            nan_blocks = series[is_na].groupby(blocks[is_na])\n",
        "\n",
        "            for _, block_series in nan_blocks:\n",
        "                # For each block of missing data, calculate its length.\n",
        "                gap_length = len(block_series)\n",
        "                # If the gap is longer than 3 months, it's a critical failure.\n",
        "                status = \"FAIL\" if gap_length > 3 else \"WARN\"\n",
        "                gap_report_data.append({\n",
        "                    \"variable\": var,\n",
        "                    \"gap_start\": block_series.index.min(),\n",
        "                    \"gap_end\": block_series.index.max(),\n",
        "                    \"gap_months\": gap_length,\n",
        "                    \"status\": status\n",
        "                })\n",
        "\n",
        "    # Check if any gaps were detected.\n",
        "    if gap_report_data:\n",
        "        # Create a report DataFrame from the collected gap information.\n",
        "        gap_report = pd.DataFrame(gap_report_data)\n",
        "        results['reports']['macro_gap_report'] = gap_report\n",
        "        # If any of the gaps were marked as a failure, this is a critical issue.\n",
        "        if \"FAIL\" in gap_report['status'].values:\n",
        "            results['overall_status'] = \"FAIL\"\n",
        "            raise ValueError(\n",
        "                \"Critical Error: Macroeconomic data contains gaps exceeding 3 consecutive months. \"\n",
        "                \"See 'macro_gap_report' in the returned dictionary for details.\"\n",
        "            )\n",
        "\n",
        "    # If all checks passed, return the success status and any warning reports.\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 1\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase1_task1_validation(\n",
        "    equity_tick_df: pd.DataFrame,\n",
        "    rate_tick_df: pd.DataFrame,\n",
        "    macro_df: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    target_market: str,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all validation steps in Task 1.\n",
        "\n",
        "    This master function sequentially calls the validation functions for the\n",
        "    configuration dictionary, DataFrame schemas, and cross-dataset alignment.\n",
        "    It aggregates the results into a final dictionary of reports.\n",
        "\n",
        "    Args:\n",
        "        equity_tick_df (pd.DataFrame): Raw high-frequency equity data.\n",
        "        rate_tick_df (pd.DataFrame): Raw high-frequency rate data.\n",
        "        macro_df (pd.DataFrame): Raw macroeconomic time series data.\n",
        "        announcement_df (pd.DataFrame): Raw announcement metadata.\n",
        "        target_market (str): The market to study (e.g., 'CAN').\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the detailed validation\n",
        "                                 reports from each step.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Configuration Dictionary Validation ---\n",
        "    config_report = validate_study_configuration(study_config)\n",
        "\n",
        "    # --- Step 2: DataFrame Schema Validation ---\n",
        "    # Define the required schemas for each DataFrame\n",
        "    tick_schema = {\n",
        "        \"timestamp_micros_utc\": pd.DatetimeTZDtype(unit='ns', tz=pytz.UTC),\n",
        "        \"price\": np.float64,\n",
        "        \"volume\": np.int64,\n",
        "        \"type\": (pd.CategoricalDtype, {'TRADE', 'BID', 'ASK'})\n",
        "    }\n",
        "    macro_schema = {\n",
        "        \"date\": pd.to_datetime(\"2000-01-01\").__class__, # A way to get datetime64[ns] type\n",
        "        \"source_series_id\": np.object_,\n",
        "        \"country\": pd.CategoricalDtype,\n",
        "        \"variable_name\": np.object_,\n",
        "        \"value_raw\": np.float64\n",
        "    }\n",
        "    announcement_schema = {\n",
        "        \"event_id\": np.int64,\n",
        "        \"central_bank\": (pd.CategoricalDtype, {'ECB', 'FED'}),\n",
        "        \"announcement_date_local\": pd.to_datetime(\"2000-01-01\").__class__,\n",
        "        \"announcement_time_local\": np.object_,\n",
        "        \"local_timezone\": np.object_\n",
        "    }\n",
        "\n",
        "    # Validate each DataFrame and collect reports\n",
        "    equity_schema_report = validate_dataframe_schema(equity_tick_df, \"equity_tick_df\", tick_schema)\n",
        "    rate_schema_report = validate_dataframe_schema(rate_tick_df, \"rate_tick_df\", tick_schema)\n",
        "    macro_schema_report = validate_dataframe_schema(macro_df, \"macro_df\", macro_schema)\n",
        "    announcement_schema_report = validate_dataframe_schema(announcement_df, \"announcement_df\", announcement_schema)\n",
        "\n",
        "    # Combine schema reports into a single DataFrame\n",
        "    full_schema_report = pd.concat([\n",
        "        equity_schema_report,\n",
        "        rate_schema_report,\n",
        "        macro_schema_report,\n",
        "        announcement_schema_report\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    if \"FAIL\" in full_schema_report[\"compliance_status\"].values:\n",
        "        raise SchemaError(f\"One or more DataFrames failed schema validation.\\n{full_schema_report.to_string()}\")\n",
        "\n",
        "    # --- Step 3: Cross-Dataset Alignment Validation ---\n",
        "    alignment_results = validate_cross_dataset_alignment(\n",
        "        equity_tick_df, rate_tick_df, macro_df, announcement_df, target_market\n",
        "    )\n",
        "\n",
        "    # --- Aggregate and return all reports ---\n",
        "    final_reports = {\n",
        "        \"config_validation_report\": config_report,\n",
        "        \"schema_validation_report\": full_schema_report,\n",
        "        \"alignment_validation_results\": alignment_results\n",
        "    }\n",
        "\n",
        "    return final_reports\n"
      ],
      "metadata": {
        "id": "uzgbkrMXY2fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Quality Assessment and Anomaly Detection\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 2, STEP 1: HIGH-FREQUENCY TICK DATA ANOMALY DETECTION\n",
        "# =============================================================================\n",
        "\n",
        "def detect_tick_data_anomalies(\n",
        "    tick_df: pd.DataFrame,\n",
        "    market_hours: Dict[str, time],\n",
        "    local_timezone_str: str\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive quality assessment on high-frequency tick data.\n",
        "\n",
        "    Purpose:\n",
        "        This function identifies common anomalies in tick-level financial data\n",
        "        to ensure its integrity before it is used for surprise calculation. It\n",
        "        flags duplicates, price outliers, volume irregularities, timestamp gaps,\n",
        "        and other issues.\n",
        "\n",
        "    Process:\n",
        "        1.  **Duplicate Timestamps**: Identifies ticks with identical timestamps.\n",
        "        2.  **Price Outliers**: Uses the robust Modified Z-score method to flag\n",
        "            prices that deviate significantly from the local median. The formula is:\n",
        "            Z_i = (0.6745 * (price_i - median_price)) / MAD\n",
        "            where MAD is the Median Absolute Deviation. A threshold of 3.5 is used.\n",
        "        3.  **Volume Anomalies**: Flags trades with volume less than 1 or\n",
        "            exceeding the 99.9th percentile of the sample.\n",
        "        4.  **Timestamp Gaps**: Detects time gaps greater than 5 minutes that\n",
        "            occur during specified market hours.\n",
        "        5.  **Invalid Prices**: Flags any ticks with a price less than or equal to zero.\n",
        "        6.  **Monotonicity**: Checks if the entire timestamp series is strictly\n",
        "            increasing.\n",
        "\n",
        "    Args:\n",
        "        tick_df (pd.DataFrame): DataFrame containing tick data with columns\n",
        "                                ['timestamp_micros_utc', 'price', 'volume'].\n",
        "        market_hours (Dict[str, time]): A dictionary with 'start' and 'end'\n",
        "                                        keys defining market hours.\n",
        "        local_timezone_str (str): The IANA timezone string for the market\n",
        "                                  (e.g., 'America/New_York').\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        - A DataFrame of boolean flags with the same index as `tick_df`, where\n",
        "          each column corresponds to a specific anomaly type.\n",
        "        - A summary DataFrame quantifying the number and percentage of each\n",
        "          detected anomaly.\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not isinstance(tick_df, pd.DataFrame) or tick_df.empty:\n",
        "        raise ValueError(\"tick_df must be a non-empty pandas DataFrame.\")\n",
        "    required_cols = {'timestamp_micros_utc', 'price', 'volume'}\n",
        "    if not required_cols.issubset(tick_df.columns):\n",
        "        raise ValueError(f\"tick_df missing required columns: {required_cols - set(tick_df.columns)}\")\n",
        "\n",
        "    # Initialize a DataFrame to store anomaly flags, sharing the same index\n",
        "    n_ticks = len(tick_df)\n",
        "    anomalies = pd.DataFrame(index=tick_df.index)\n",
        "\n",
        "    # --- Anomaly 1: Duplicate Timestamps ---\n",
        "    # Find all occurrences of timestamps that are not unique.\n",
        "    anomalies['duplicate_timestamp'] = tick_df['timestamp_micros_utc'].duplicated(keep=False)\n",
        "\n",
        "    # --- Anomaly 2: Price Outliers (Modified Z-Score) ---\n",
        "    # Calculate the median price, which is robust to outliers.\n",
        "    median_price = tick_df['price'].median()\n",
        "    # Calculate the Median Absolute Deviation (MAD) from the median.\n",
        "    mad = np.median(np.abs(tick_df['price'] - median_price))\n",
        "    # Define a minimum MAD to avoid division by zero if all prices are identical.\n",
        "    mad_safe = mad if mad > 1e-9 else 1.0\n",
        "    # Calculate the Modified Z-Score for each price point.\n",
        "    # The constant 0.6745 is norm.ppf(0.75), scaling MAD to be a consistent\n",
        "    # estimator of the standard deviation.\n",
        "    modified_z_score = (norm.ppf(0.75) * (tick_df['price'] - median_price)) / mad_safe\n",
        "    # Flag prices where the absolute Modified Z-Score exceeds the threshold of 3.5.\n",
        "    anomalies['price_outlier'] = np.abs(modified_z_score) > 3.5\n",
        "\n",
        "    # --- Anomaly 3: Volume Anomalies ---\n",
        "    # Calculate the 99.9th percentile for volume to identify extreme trades.\n",
        "    volume_q999 = tick_df['volume'].quantile(0.999)\n",
        "    # Flag trades with zero or negative volume.\n",
        "    anomalies['low_volume'] = tick_df['volume'] < 1\n",
        "    # Flag trades with volume exceeding the high threshold.\n",
        "    anomalies['high_volume'] = tick_df['volume'] > volume_q999\n",
        "\n",
        "    # --- Anomaly 4: Timestamp Gaps during Market Hours ---\n",
        "    # Calculate the time difference between consecutive ticks in seconds.\n",
        "    time_diff_seconds = tick_df['timestamp_micros_utc'].diff().dt.total_seconds()\n",
        "    # Get the local timezone object.\n",
        "    local_tz = pytz.timezone(local_timezone_str)\n",
        "    # Convert UTC timestamps to the market's local time.\n",
        "    local_time = tick_df['timestamp_micros_utc'].dt.tz_convert(local_tz).dt.time\n",
        "    # Create a boolean mask for ticks occurring within market hours.\n",
        "    in_market_hours = (local_time >= market_hours['start']) & (local_time <= market_hours['end'])\n",
        "    # Flag gaps greater than 5 minutes (300 seconds) that occur within market hours.\n",
        "    anomalies['timestamp_gap'] = (time_diff_seconds > 300) & in_market_hours\n",
        "\n",
        "    # --- Anomaly 5: Invalid Prices ---\n",
        "    # Flag any ticks with a price that is not strictly positive.\n",
        "    anomalies['invalid_price'] = tick_df['price'] <= 0\n",
        "\n",
        "    # --- Anomaly 6: Timestamp Monotonicity ---\n",
        "    # Check if the entire timestamp series is monotonically increasing.\n",
        "    is_monotonic = tick_df['timestamp_micros_utc'].is_monotonic_increasing\n",
        "    # If not monotonic, this is a serious data quality issue.\n",
        "    # This flag will be False for all rows if the check fails globally.\n",
        "    anomalies['non_monotonic'] = not is_monotonic\n",
        "\n",
        "    # --- Generate Summary Report ---\n",
        "    summary_data = []\n",
        "    for col in anomalies.columns:\n",
        "        count = anomalies[col].sum()\n",
        "        percentage = (count / n_ticks) * 100 if n_ticks > 0 else 0\n",
        "        summary_data.append({\n",
        "            \"anomaly_type\": col,\n",
        "            \"count\": count,\n",
        "            \"percentage\": percentage\n",
        "        })\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "    return anomalies, summary_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 2, STEP 2: ANNOUNCEMENT METADATA INTEGRITY VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "def verify_announcement_metadata(\n",
        "    announcement_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a rigorous integrity check on the announcement metadata.\n",
        "\n",
        "    Purpose:\n",
        "        This function validates the announcement schedule data to ensure it is\n",
        "        reliable. It checks for unique identifiers, valid timezones, parseable\n",
        "        times, and flags announcements on non-business days.\n",
        "\n",
        "    Process:\n",
        "        1.  **Unique Event IDs**: Verifies that every `event_id` is unique.\n",
        "        2.  **Timezone Validity**: Checks `local_timezone` strings against the\n",
        "            IANA database using `pytz`.\n",
        "        3.  **Time Format**: Attempts to parse `announcement_time_local`.\n",
        "        4.  **Weekend/Holiday Announcements**: Flags events scheduled on weekends\n",
        "            or on known market holidays for the relevant exchanges (CME for FED,\n",
        "            EUREX for ECB).\n",
        "        5.  **Chronological Order**: Ensures announcements for each central bank\n",
        "            are sorted chronologically.\n",
        "        6.  **Duplicate Announcements**: Detects logical duplicates based on\n",
        "            central bank, date, and time.\n",
        "\n",
        "    Args:\n",
        "        announcement_df (pd.DataFrame): DataFrame with announcement metadata.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A detailed report DataFrame, indexed by `event_id`,\n",
        "                      containing the status and messages for each validation check.\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not isinstance(announcement_df, pd.DataFrame) or announcement_df.empty:\n",
        "        raise ValueError(\"announcement_df must be a non-empty pandas DataFrame.\")\n",
        "\n",
        "    # Create a report DataFrame indexed by the event_id for clear mapping\n",
        "    report = pd.DataFrame(index=announcement_df['event_id'])\n",
        "    df = announcement_df.set_index('event_id')\n",
        "\n",
        "    # --- Check 1: Unique Event IDs ---\n",
        "    report['is_unique_id'] = ~df.index.duplicated(keep=False)\n",
        "\n",
        "    # --- Check 2: Timezone Validity ---\n",
        "    def is_valid_timezone(tz_str):\n",
        "        try:\n",
        "            pytz.timezone(tz_str)\n",
        "            return True\n",
        "        except pytz.UnknownTimeZoneError:\n",
        "            return False\n",
        "    report['is_valid_timezone'] = df['local_timezone'].apply(is_valid_timezone)\n",
        "\n",
        "    # --- Check 3: Time Format Consistency ---\n",
        "    # Attempt to parse the time string; unparseable formats become NaT.\n",
        "    parsed_time = pd.to_datetime(df['announcement_time_local'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "    report['is_valid_time_format'] = parsed_time.notna()\n",
        "\n",
        "    # --- Check 4: Weekend/Holiday Announcements ---\n",
        "    # Check for weekends (Saturday=5, Sunday=6).\n",
        "    report['is_weekday'] = df['announcement_date_local'].dt.dayofweek < 5\n",
        "    # Check for market holidays using pandas_market_calendars.\n",
        "    cme_cal = mcal.get_calendar('CME')\n",
        "    eurex_cal = mcal.get_calendar('EUREX')\n",
        "    cme_holidays = cme_cal.holidays().holidays\n",
        "    eurex_holidays = eurex_cal.holidays().holidays\n",
        "\n",
        "    def is_not_holiday(row):\n",
        "        if row['central_bank'] == 'FED':\n",
        "            return row['announcement_date_local'].date() not in cme_holidays\n",
        "        elif row['central_bank'] == 'ECB':\n",
        "            return row['announcement_date_local'].date() not in eurex_holidays\n",
        "        return True\n",
        "    report['is_not_holiday'] = df.apply(is_not_holiday, axis=1)\n",
        "\n",
        "    # --- Check 5: Chronological Ordering ---\n",
        "    # Check if dates are sorted within each central bank group.\n",
        "    is_sorted = df.groupby('central_bank')['announcement_date_local'].is_monotonic_increasing.all()\n",
        "    report['is_chronological'] = is_sorted # Broadcasts the global result to all rows\n",
        "\n",
        "    # --- Check 6: Duplicate Announcements ---\n",
        "    # Detect duplicates based on a composite key.\n",
        "    key_cols = ['central_bank', 'announcement_date_local', 'announcement_time_local']\n",
        "    report['is_not_duplicate_event'] = ~df.duplicated(subset=key_cols, keep=False)\n",
        "\n",
        "    return report.reset_index()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 2, STEP 3: MACROECONOMIC TIME SERIES STATISTICAL QUALITY CONTROL\n",
        "# =============================================================================\n",
        "\n",
        "def _chow_test(\n",
        "    series: pd.Series,\n",
        "    breakpoint_date: pd.Timestamp\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Performs a Chow test for a structural break at a specific date.\n",
        "\n",
        "    This helper function tests the null hypothesis that the coefficients of a\n",
        "    linear regression model are stable across a breakpoint.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The time series data to test.\n",
        "        breakpoint_date (pd.Timestamp): The date of the potential structural break.\n",
        "\n",
        "    Returns:\n",
        "        float: The p-value of the Chow test. Returns np.nan if the test cannot be performed.\n",
        "    \"\"\"\n",
        "    # Define the regression model: y_t = beta_0 + beta_1 * t + e_t\n",
        "    y = series\n",
        "    X = sm.add_constant(np.arange(len(series)))\n",
        "    X.index = y.index\n",
        "    k = X.shape[1]  # Number of regressors (2: constant and trend)\n",
        "    N = len(y)\n",
        "\n",
        "    # Check if the breakpoint is valid and allows for estimation in both sub-samples.\n",
        "    if breakpoint_date not in y.index or y.index.get_loc(breakpoint_date) < k:\n",
        "        return np.nan\n",
        "\n",
        "    # Split the data into two sub-samples at the breakpoint.\n",
        "    X1, y1 = X[X.index < breakpoint_date], y[y.index < breakpoint_date]\n",
        "    X2, y2 = X[X.index >= breakpoint_date], y[y.index >= breakpoint_date]\n",
        "\n",
        "    # Check if each sub-sample is large enough to estimate the model.\n",
        "    if len(y1) < k or len(y2) < k or (N - 2 * k) <= 0:\n",
        "        return np.nan\n",
        "\n",
        "    # --- Perform Regressions ---\n",
        "    # 1. Restricted model (pooled data)\n",
        "    rss_restricted = sm.OLS(y, X).fit().ssr\n",
        "\n",
        "    # 2. Unrestricted models (separate sub-samples)\n",
        "    rss_unrestricted_1 = sm.OLS(y1, X1).fit().ssr\n",
        "    rss_unrestricted_2 = sm.OLS(y2, X2).fit().ssr\n",
        "    rss_unrestricted_total = rss_unrestricted_1 + rss_unrestricted_2\n",
        "\n",
        "    # --- Calculate F-statistic ---\n",
        "    # Equation: F = [(RSS_R - RSS_UR) / k] / [RSS_UR / (N - 2k)]\n",
        "    f_statistic = ((rss_restricted - rss_unrestricted_total) / k) / (rss_unrestricted_total / (N - 2 * k))\n",
        "\n",
        "    # --- Calculate p-value ---\n",
        "    # The p-value is derived from the F-distribution's survival function (1 - CDF).\n",
        "    # Numerator degrees of freedom: dfn = k\n",
        "    # Denominator degrees of freedom: dfd = N - 2k\n",
        "    p_value = f.sf(f_statistic, dfn=k, dfd=N - 2 * k)\n",
        "\n",
        "    return p_value\n",
        "\n",
        "\n",
        "def _assess_single_series(\n",
        "    series_group: pd.DataFrame\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Worker function to perform all quality checks on a single time series.\n",
        "\n",
        "    This function is designed to be used with `groupby().apply()`.\n",
        "\n",
        "    Args:\n",
        "        series_group (pd.DataFrame): A DataFrame for a single source_series_id,\n",
        "                                     with 'date' as index and 'value_raw' as the column.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series containing all calculated quality metrics for the input series.\n",
        "    \"\"\"\n",
        "    # Extract the series and its original size for missing value calculation.\n",
        "    original_size = len(series_group)\n",
        "    series = series_group['value_raw'].dropna()\n",
        "\n",
        "    # If the series has too few observations after dropping NaNs, return a partial report.\n",
        "    if len(series) < 24:  # Require at least 2 years of data for meaningful tests.\n",
        "        return pd.Series({\n",
        "            'missing_pct': (1 - len(series) / original_size) * 100 if original_size > 0 else 100,\n",
        "            'chow_p_val_gfc': np.nan,\n",
        "            'chow_p_val_covid': np.nan,\n",
        "            'outlier_pct': np.nan,\n",
        "            'is_monthly': False,\n",
        "            'is_strictly_positive': False,\n",
        "            'notes': 'Series too short for full assessment.'\n",
        "        })\n",
        "\n",
        "    # --- 1. Missing Value Percentage ---\n",
        "    missing_pct = (1 - len(series) / original_size) * 100\n",
        "\n",
        "    # --- 2. Structural Break (Chow Test) at two key dates ---\n",
        "    chow_p_gfc = _chow_test(series, pd.Timestamp('2008-09-15'))\n",
        "    chow_p_covid = _chow_test(series, pd.Timestamp('2020-03-11'))\n",
        "\n",
        "    # --- 3. Outliers (Hampel Filter) ---\n",
        "    # Use a 7-month centered window (k=3) to calculate local median and MAD.\n",
        "    rolling_median = series.rolling(window=7, center=True, min_periods=4).median()\n",
        "    # Calculate Median Absolute Deviation from the rolling median.\n",
        "    rolling_mad = (series - rolling_median).abs().rolling(window=7, center=True, min_periods=4).median()\n",
        "    # An observation is an outlier if it's more than 3 MADs from the local median.\n",
        "    is_outlier = (series - rolling_median).abs() > (3 * rolling_mad)\n",
        "    outlier_pct = is_outlier.sum() / len(series) * 100\n",
        "\n",
        "    # --- 4. Temporal Consistency (Monthly Frequency Check) ---\n",
        "    # Calculate the difference in days between consecutive observations.\n",
        "    day_diffs = series.index.to_series().diff().dt.days\n",
        "    # The mode of these differences should be between 28 and 31 for a monthly series.\n",
        "    modal_diff = day_diffs.mode()\n",
        "    is_monthly = (modal_diff >= 28).all() and (modal_diff <= 31).all() if not modal_diff.empty else False\n",
        "\n",
        "    # --- 5. Value Range Check (Strictly Positive) ---\n",
        "    # A common requirement for variables like GDP, CPI, etc.\n",
        "    is_positive = (series > 0).all()\n",
        "\n",
        "    # --- Assemble and return the results Series ---\n",
        "    return pd.Series({\n",
        "        'missing_pct': missing_pct,\n",
        "        'chow_p_val_gfc': chow_p_gfc,\n",
        "        'chow_p_val_covid': chow_p_covid,\n",
        "        'outlier_pct': outlier_pct,\n",
        "        'is_monthly': is_monthly,\n",
        "        'is_strictly_positive': is_positive,\n",
        "        'notes': ''\n",
        "    })\n",
        "\n",
        "\n",
        "def assess_macro_series_quality(\n",
        "    macro_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs statistical quality control on macroeconomic time series.\n",
        "\n",
        "    Purpose:\n",
        "        This function assesses each time series in the provided long-format\n",
        "        DataFrame for common issues like missing data, structural breaks,\n",
        "        outliers, and inconsistent frequency. It produces a comprehensive\n",
        "        report to guide data cleaning and model specification.\n",
        "\n",
        "    Process:\n",
        "        1.  **Group Data**: Groups the input DataFrame by `source_series_id`.\n",
        "        2.  **Apply Checks**: Uses the `_assess_single_series` worker function\n",
        "            on each group to perform the full suite of tests in a vectorized manner.\n",
        "            - **Missing Values**: Calculates the percentage of missing observations.\n",
        "            - **Structural Breaks**: Performs a statistically correct Chow test\n",
        "              for breaks at the GFC (2008-09-15) and COVID-19 (2020-03-11) dates.\n",
        "            - **Outliers**: Applies a Hampel filter with a 3-month rolling window.\n",
        "            - **Temporal Consistency**: Verifies a consistent monthly frequency.\n",
        "            - **Value Ranges**: Checks if values are strictly positive.\n",
        "        3.  **Compile Report**: Assembles the results from all series into a\n",
        "            single, clean DataFrame, indexed by `source_series_id`.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): Long-format DataFrame of macro series with\n",
        "                                 columns ['source_series_id', 'date', 'value_raw'].\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary report DataFrame, indexed by `source_series_id`,\n",
        "                      detailing the quality metrics for each time series.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(macro_df, pd.DataFrame) or macro_df.empty:\n",
        "        raise ValueError(\"macro_df must be a non-empty pandas DataFrame.\")\n",
        "    required_cols = {'source_series_id', 'date', 'value_raw'}\n",
        "    if not required_cols.issubset(macro_df.columns):\n",
        "        raise ValueError(f\"macro_df missing required columns: {required_cols - set(macro_df.columns)}\")\n",
        "\n",
        "    # --- Group by series and apply the assessment function ---\n",
        "    # Set 'date' as the index to enable time-series operations within the helper.\n",
        "    quality_report = macro_df.set_index('date').groupby('source_series_id').apply(_assess_single_series)\n",
        "\n",
        "    # Return the final report.\n",
        "    return quality_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 2\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase1_task2_quality_assessment(\n",
        "    equity_tick_df: pd.DataFrame,\n",
        "    rate_tick_df: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    macro_df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all data quality assessment steps in Task 2.\n",
        "\n",
        "    This master function calls the specialized assessment functions for each\n",
        "    type of input data (tick, announcement, macro) and aggregates the\n",
        "    resulting quality reports into a single dictionary.\n",
        "\n",
        "    Args:\n",
        "        equity_tick_df (pd.DataFrame): Raw high-frequency equity data.\n",
        "        rate_tick_df (pd.DataFrame): Raw high-frequency rate data.\n",
        "        announcement_df (pd.DataFrame): Raw announcement metadata.\n",
        "        macro_df (pd.DataFrame): Raw macroeconomic time series data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the detailed data\n",
        "                                 quality and anomaly reports from each step.\n",
        "    \"\"\"\n",
        "    # Define market hours for anomaly detection\n",
        "    fed_market_hours = {'start': time(9, 30), 'end': time(16, 0)}\n",
        "    ecb_market_hours = {'start': time(9, 0), 'end': time(17, 30)}\n",
        "\n",
        "    # --- Step 1: High-Frequency Data Quality Checks ---\n",
        "    # Note: Assuming equity and rate futures trade on corresponding exchanges\n",
        "    equity_anomalies, equity_summary = detect_tick_data_anomalies(\n",
        "        equity_tick_df, fed_market_hours, 'America/New_York'\n",
        "    )\n",
        "    rate_anomalies, rate_summary = detect_tick_data_anomalies(\n",
        "        rate_tick_df, ecb_market_hours, 'Europe/Berlin' # Example for ECB\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Announcement Metadata Quality Assessment ---\n",
        "    announcement_report = verify_announcement_metadata(announcement_df)\n",
        "\n",
        "    # --- Step 3: Macroeconomic Time Series Quality Control ---\n",
        "    macro_quality_report = assess_macro_series_quality(macro_df)\n",
        "\n",
        "    # --- Aggregate and return all reports ---\n",
        "    final_reports = {\n",
        "        \"equity_anomaly_flags\": equity_anomalies,\n",
        "        \"equity_anomaly_summary\": equity_summary,\n",
        "        \"rate_anomaly_flags\": rate_anomalies,\n",
        "        \"rate_anomaly_summary\": rate_summary,\n",
        "        \"announcement_integrity_report\": announcement_report,\n",
        "        \"macro_series_quality_report\": macro_quality_report\n",
        "    }\n",
        "\n",
        "    return final_reports\n"
      ],
      "metadata": {
        "id": "xPXspEpkcFF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Data Cleansing and Preprocessing\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3, STEP 1: HIGH-FREQUENCY TICK DATA CLEANSING PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "def cleanse_tick_data(\n",
        "    raw_tick_df: pd.DataFrame,\n",
        "    anomaly_flags: pd.DataFrame,\n",
        "    min_volume_threshold: int = 10\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Applies a rigorous cleansing pipeline to raw high-frequency tick data.\n",
        "\n",
        "    Purpose:\n",
        "        This function transforms raw tick data into a clean, analysis-ready\n",
        "        dataset by systematically removing invalid, anomalous, or irrelevant\n",
        "        observations. It generates a detailed audit trail of the entire process.\n",
        "\n",
        "    Process:\n",
        "        1.  **Input Validation**: Ensures the raw data and anomaly flags are valid.\n",
        "        2.  **Duplicate Removal**: Removes records with identical timestamp and price.\n",
        "        3.  **Type Filtering**: Retains only 'TRADE' type ticks, as these\n",
        "            represent actual transactions.\n",
        "        4.  **Volume Filtering**: Excludes trades with volume below a specified\n",
        "            liquidity threshold (default 10).\n",
        "        5.  **Outlier Removal**: Removes ticks previously flagged as price\n",
        "            outliers by the anomaly detection process.\n",
        "        6.  **Gap Filling**: This step is omitted here as per the instructions,\n",
        "            which specify forward-filling for small gaps. This is better handled\n",
        "            during the price extraction phase (Task 4) where the context of an\n",
        "            event window is known, rather than on the entire tick history.\n",
        "            Applying a global forward-fill on tick data is generally incorrect.\n",
        "\n",
        "    Args:\n",
        "        raw_tick_df (pd.DataFrame): The raw tick data, including 'type' column.\n",
        "        anomaly_flags (pd.DataFrame): Boolean flags for outliers from Task 2.\n",
        "        min_volume_threshold (int): The minimum trade volume to be considered valid.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "        - A new DataFrame containing the cleansed tick data.\n",
        "        - A dictionary serving as an audit trail, detailing the number of\n",
        "          records at the start and end, and records removed at each step.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if raw_tick_df.empty:\n",
        "        raise ValueError(\"Input 'raw_tick_df' cannot be empty.\")\n",
        "    if anomaly_flags.empty or not raw_tick_df.index.equals(anomaly_flags.index):\n",
        "        raise ValueError(\"Anomaly flags must be a non-empty DataFrame with an index matching the raw tick data.\")\n",
        "\n",
        "    # --- Initialization of Audit Trail ---\n",
        "    initial_count = len(raw_tick_df)\n",
        "    audit_trail = {\n",
        "        \"initial_record_count\": initial_count,\n",
        "        \"processing_timestamp_utc\": pd.Timestamp.now(tz='UTC'),\n",
        "        \"steps\": []\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Remove Duplicates ---\n",
        "    # A duplicate is defined as a record with the same timestamp and price.\n",
        "    df = raw_tick_df.drop_duplicates(subset=['timestamp_micros_utc', 'price'], keep='first')\n",
        "    count_after_dedup = len(df)\n",
        "    audit_trail[\"steps\"].append({\n",
        "        \"step_name\": \"Remove Duplicates\",\n",
        "        \"records_removed\": initial_count - count_after_dedup,\n",
        "        \"records_remaining\": count_after_dedup\n",
        "    })\n",
        "\n",
        "    # --- Step 2: Filter for 'TRADE' type ticks ---\n",
        "    # Only actual trades are relevant for calculating price changes.\n",
        "    trade_mask = df['type'] == 'TRADE'\n",
        "    df = df[trade_mask]\n",
        "    count_after_trade_filter = len(df)\n",
        "    audit_trail[\"steps\"].append({\n",
        "        \"step_name\": \"Filter for TRADE type\",\n",
        "        \"records_removed\": count_after_dedup - count_after_trade_filter,\n",
        "        \"records_remaining\": count_after_trade_filter\n",
        "    })\n",
        "\n",
        "    # --- Step 3: Apply Minimum Volume Filter ---\n",
        "    # Exclude trades with insufficient volume, which may not be representative.\n",
        "    volume_mask = df['volume'] >= min_volume_threshold\n",
        "    df = df[volume_mask]\n",
        "    count_after_volume_filter = len(df)\n",
        "    audit_trail[\"steps\"].append({\n",
        "        \"step_name\": f\"Filter for Volume >= {min_volume_threshold}\",\n",
        "        \"records_removed\": count_after_trade_filter - count_after_volume_filter,\n",
        "        \"records_remaining\": count_after_volume_filter\n",
        "    })\n",
        "\n",
        "    # --- Step 4: Remove Price Outliers ---\n",
        "    # Use the pre-computed outlier flags from Task 2 for consistency.\n",
        "    # Align the outlier flags with the current state of the DataFrame.\n",
        "    outlier_mask = anomaly_flags.loc[df.index, 'price_outlier']\n",
        "    df = df[~outlier_mask]\n",
        "    count_after_outlier_removal = len(df)\n",
        "    audit_trail[\"steps\"].append({\n",
        "        \"step_name\": \"Remove Price Outliers\",\n",
        "        \"records_removed\": count_after_volume_filter - count_after_outlier_removal,\n",
        "        \"records_remaining\": count_after_outlier_removal\n",
        "    })\n",
        "\n",
        "    # --- Finalization ---\n",
        "    audit_trail[\"final_record_count\"] = count_after_outlier_removal\n",
        "\n",
        "    # Return the cleansed DataFrame and the detailed audit trail.\n",
        "    return df.copy(), audit_trail\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3, STEP 2: ANNOUNCEMENT TIMESTAMP UTC STANDARDIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def standardize_announcement_timestamps(\n",
        "    announcement_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Converts local announcement times to standardized UTC timestamps.\n",
        "\n",
        "    Purpose:\n",
        "        This function creates a single, unambiguous UTC timestamp for each\n",
        "        announcement, which is essential for accurately aligning events with\n",
        "        the high-frequency tick data.\n",
        "\n",
        "    Process:\n",
        "        1.  **Combine Date and Time**: Merges the local date and time columns.\n",
        "        2.  **Localize Timestamp**: Converts the naive datetime to a timezone-\n",
        "            aware datetime using the specified `local_timezone`.\n",
        "        3.  **Handle DST Transitions**: Critically, it uses `is_dst=None` to\n",
        "            force `pytz` to raise errors on ambiguous or non-existent times\n",
        "            during DST changes. It then catches these specific errors and\n",
        "            applies documented fallback logic.\n",
        "        4.  **Convert to UTC**: Converts the final localized timestamp to UTC.\n",
        "        5.  **Validate**: Checks if the resulting UTC timestamps fall within\n",
        "            a reasonable window (08:00-20:00 UTC).\n",
        "        6.  **Audit**: Produces a detailed diagnostic report of the conversion.\n",
        "\n",
        "    Args:\n",
        "        announcement_df (pd.DataFrame): The announcement metadata DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        - The input DataFrame with a new 'announcement_timestamp_utc' column.\n",
        "        - A diagnostic DataFrame auditing the conversion for each event.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if announcement_df.empty:\n",
        "        raise ValueError(\"Input 'announcement_df' cannot be empty.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    diagnostics = []\n",
        "    utc_timestamps = []\n",
        "\n",
        "    # --- Iterate through each announcement for precise DST handling ---\n",
        "    for _, row in announcement_df.iterrows():\n",
        "        event_id = row['event_id']\n",
        "        tz_str = row['local_timezone']\n",
        "        status = \"OK\"\n",
        "        notes = \"\"\n",
        "\n",
        "        try:\n",
        "            # Combine date and time strings into a naive datetime object.\n",
        "            naive_dt = pd.to_datetime(\n",
        "                f\"{row['announcement_date_local'].date()} {row['announcement_time_local']}\"\n",
        "            )\n",
        "\n",
        "            # Get the pytz timezone object.\n",
        "            local_tz = pytz.timezone(tz_str)\n",
        "\n",
        "            # Localize the naive datetime. is_dst=None is CRITICAL.\n",
        "            # It forces an error on ambiguous/non-existent times instead of guessing.\n",
        "            local_dt = local_tz.localize(naive_dt, is_dst=None)\n",
        "\n",
        "        except pytz.AmbiguousTimeError:\n",
        "            # Occurs during \"fall back\" DST transition (e.g., 1:30 AM happens twice).\n",
        "            # The standard resolution is to choose the first occurrence (daylight time).\n",
        "            local_dt = local_tz.localize(naive_dt, is_dst=True)\n",
        "            status = \"Warning\"\n",
        "            notes = \"AmbiguousTimeError: Resolved by choosing first occurrence (DST).\"\n",
        "\n",
        "        except pytz.NonExistentTimeError:\n",
        "            # Occurs during \"spring forward\" DST transition (e.g., 2:30 AM does not exist).\n",
        "            # The standard resolution is to shift forward by the DST offset (typically 1 hour).\n",
        "            local_dt = local_tz.localize(naive_dt + pd.Timedelta(hours=1), is_dst=None)\n",
        "            status = \"Warning\"\n",
        "            notes = \"NonExistentTimeError: Resolved by shifting time forward by 1 hour.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any other parsing or timezone errors.\n",
        "            local_dt = pd.NaT\n",
        "            status = \"FAIL\"\n",
        "            notes = f\"Unhandled exception: {str(e)}\"\n",
        "\n",
        "        # Convert the final localized datetime to UTC.\n",
        "        utc_dt = pd.to_datetime(local_dt).tz_convert('UTC') if pd.notna(local_dt) else pd.NaT\n",
        "        utc_timestamps.append(utc_dt)\n",
        "\n",
        "        # Append detailed diagnostics for this event.\n",
        "        diagnostics.append({\n",
        "            \"event_id\": event_id,\n",
        "            \"original_local_datetime\": naive_dt,\n",
        "            \"local_timezone\": tz_str,\n",
        "            \"final_utc_timestamp\": utc_dt,\n",
        "            \"conversion_status\": status,\n",
        "            \"notes\": notes\n",
        "        })\n",
        "\n",
        "    # --- Finalize and Validate ---\n",
        "    output_df = announcement_df.copy()\n",
        "    output_df['announcement_timestamp_utc'] = utc_timestamps\n",
        "\n",
        "    # Validate that timestamps are not NaT.\n",
        "    if output_df['announcement_timestamp_utc'].isna().any():\n",
        "        raise ValueError(\"Failed to convert one or more timestamps to UTC.\")\n",
        "\n",
        "    # Create and return the diagnostic report.\n",
        "    diagnostic_report = pd.DataFrame(diagnostics).set_index(\"event_id\")\n",
        "\n",
        "    return output_df, diagnostic_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 3, STEP 3: MACROECONOMIC SERIES TRANSFORMATION PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "def prepare_macroeconomic_data(\n",
        "    macro_df: pd.DataFrame,\n",
        "    target_market: str\n",
        ") -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"\n",
        "    Cleans and prepares raw macroeconomic data into a wide-format panel.\n",
        "\n",
        "    Purpose:\n",
        "        This function serves as the primary cleaning and structuring tool for\n",
        "        raw, long-format macroeconomic data. Its sole responsibility is to\n",
        "        produce a clean, validated, wide-format DataFrame where each series is\n",
        "        in its original, untransformed level. It explicitly does NOT perform\n",
        "        model-specific transformations like logarithms.\n",
        "\n",
        "    Process:\n",
        "        1.  **Filter**: Selects data for the specified `target_market`.\n",
        "        2.  **Pivot**: Converts the data from long to wide format, with dates\n",
        "            as the index and variables as columns.\n",
        "        3.  **Interpolate**: Fills small gaps (<= 2 consecutive months) using\n",
        "            linear interpolation to handle minor, sporadic missing data.\n",
        "        4.  **Validate and Exclude**: Identifies any series that still contain\n",
        "            gaps after interpolation. It issues a warning, excludes these\n",
        "            incomplete series, and proceeds with the remaining valid data.\n",
        "        5.  **Reindex**: Ensures the final DataFrame has a complete, uninterrupted\n",
        "            monthly frequency from start to end.\n",
        "        6.  **Standardize Names**: Renames columns to a consistent format,\n",
        "            e.g., `CAN_gdp`, ensuring clarity for downstream tasks.\n",
        "\n",
        "    Args:\n",
        "        macro_df (pd.DataFrame): The raw, long-format macroeconomic data.\n",
        "        target_market (str): The country/market code to process (e.g., 'CAN').\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, List[str]]:\n",
        "        - An analysis-ready, wide-format DataFrame containing only the valid\n",
        "          series in their original levels.\n",
        "        - A list of the names of any series that were dropped due to large gaps.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation and Filtering ---\n",
        "    # Ensure the input DataFrame is not empty.\n",
        "    if macro_df.empty:\n",
        "        raise ValueError(\"Input 'macro_df' cannot be empty.\")\n",
        "\n",
        "    # Filter the dataset for the specified target market.\n",
        "    df_market = macro_df[macro_df['country'] == target_market].copy()\n",
        "    if df_market.empty:\n",
        "        raise ValueError(f\"No data found for target market '{target_market}'.\")\n",
        "\n",
        "    # --- 2. Pivot from Long to Wide Format ---\n",
        "    # This transformation makes each time series a column.\n",
        "    df_wide = df_market.pivot_table(\n",
        "        index='date', columns='variable_name', values='value_raw'\n",
        "    )\n",
        "    # Ensure the index is a proper DatetimeIndex.\n",
        "    df_wide.index = pd.to_datetime(df_wide.index)\n",
        "\n",
        "    # --- 3. Interpolate Limited Gaps ---\n",
        "    # Use linear interpolation to fill at most 2 consecutive missing months.\n",
        "    df_interpolated = df_wide.interpolate(method='linear', limit=2, axis=0)\n",
        "\n",
        "    # --- 4. Validate and Exclude Incomplete Series ---\n",
        "    # Identify any series that still have missing values after the limited fill.\n",
        "    incomplete_series: List[str] = df_interpolated.columns[df_interpolated.isna().any()].tolist()\n",
        "\n",
        "    # If any series are found to be incomplete:\n",
        "    if incomplete_series:\n",
        "        # Issue a clear, actionable warning to the user.\n",
        "        warnings.warn(\n",
        "            f\"The following series for market '{target_market}' contain gaps > 2 months and will be excluded \"\n",
        "            f\"from the final dataset: {incomplete_series}\",\n",
        "            UserWarning\n",
        "        )\n",
        "        # Exclude the problematic series from the dataset.\n",
        "        df_interpolated.drop(columns=incomplete_series, inplace=True)\n",
        "\n",
        "    # Check if any columns remain after potentially dropping some.\n",
        "    if df_interpolated.empty:\n",
        "        raise ValueError(f\"No complete time series remained for market '{target_market}' after validation.\")\n",
        "\n",
        "    # --- 5. Reindex to a Complete Monthly Date Range ---\n",
        "    # This step ensures the time series has a consistent frequency with no missing rows.\n",
        "    full_date_index = pd.date_range(\n",
        "        start=df_interpolated.index.min(),\n",
        "        end=df_interpolated.index.max(),\n",
        "        freq='MS' # 'MS' for Month Start frequency is the standard convention.\n",
        "    )\n",
        "    df_reindexed = df_interpolated.reindex(full_date_index)\n",
        "\n",
        "    # A final check for any NaNs that might have been introduced by reindexing.\n",
        "    if df_reindexed.isna().sum().sum() > 0:\n",
        "        raise ValueError(\"Data is not on a consistent monthly frequency; gaps detected after reindexing.\")\n",
        "\n",
        "    # --- 6. Standardize Column Naming ---\n",
        "    # Create clean, predictable column names for the output DataFrame.\n",
        "    # Example: 'gdp' becomes 'CAN_gdp'.\n",
        "    df_reindexed.columns = [f\"{target_market}_{col}\" for col in df_reindexed.columns]\n",
        "\n",
        "    # Return the final prepared DataFrame (in levels) and the list of any dropped series.\n",
        "    return df_reindexed, incomplete_series\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPLEMENTATION OF `run_phase1_task3_preprocessing` (Task 3 Orchestrator)\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase1_task3_preprocessing(\n",
        "    raw_equity_df: pd.DataFrame,\n",
        "    raw_rate_df: pd.DataFrame,\n",
        "    raw_announcement_df: pd.DataFrame,\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    equity_anomaly_flags: pd.DataFrame,\n",
        "    rate_anomaly_flags: pd.DataFrame,\n",
        "    target_market: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the data cleansing and preparation pipeline for Phase 1.\n",
        "\n",
        "    Purpose:\n",
        "        This master function for Phase 1 executes the complete data cleansing\n",
        "        and preprocessing workflow. Its sole responsibility is to transform raw,\n",
        "        validated input data into clean, consistently structured datasets ready\n",
        "        for the subsequent analytical phases. It strictly separates this cleaning\n",
        "        process from any model-specific numerical transformations.\n",
        "\n",
        "    Process:\n",
        "        1.  **Cleanse Tick Data**: Calls the `cleanse_tick_data` function for\n",
        "            both the equity and rate futures data streams. This involves\n",
        "            removing duplicates, filtering by trade type and volume, and removing\n",
        "            identified outliers.\n",
        "        2.  **Standardize Timestamps**: Calls the `standardize_announcement_timestamps`\n",
        "            function to convert all local announcement times to robust, timezone-\n",
        "            aware UTC timestamps, correctly handling all DST-related edge cases.\n",
        "        3.  **Prepare Macro Data**: Calls the `prepare_macroeconomic_data` function\n",
        "            to convert the long-format raw macro data into a clean, wide-format,\n",
        "            interpolated DataFrame. This version of the function prepares the\n",
        "            data in its original levels, without applying any log transformations.\n",
        "        4.  **Compile Outputs**: Gathers all the cleansed DataFrames and their\n",
        "            associated audit trail metadata into a single, well-structured\n",
        "            dictionary for handoff to the next phase of the pipeline.\n",
        "\n",
        "    Args:\n",
        "        raw_equity_df (pd.DataFrame): Raw high-frequency equity data.\n",
        "        raw_rate_df (pd.DataFrame): Raw high-frequency rate data.\n",
        "        raw_announcement_df (pd.DataFrame): Raw announcement metadata.\n",
        "        raw_macro_df (pd.DataFrame): Raw macroeconomic time series data.\n",
        "        equity_anomaly_flags (pd.DataFrame): Anomaly flags for equity data from Task 2.\n",
        "        rate_anomaly_flags (pd.DataFrame): Anomaly flags for rate data from Task 2.\n",
        "        target_market (str): The market to study (e.g., 'CAN').\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all the cleansed and prepared\n",
        "                        DataFrames and their associated audit trails. The macro\n",
        "                        data is explicitly marked as being in 'levels'.\n",
        "    \"\"\"\n",
        "    # --- 1. Cleanse High-Frequency Tick Data ---\n",
        "    # Execute the cleansing pipeline for the equity tick data.\n",
        "    clean_equity_df, equity_audit = cleanse_tick_data(\n",
        "        raw_tick_df=raw_equity_df,\n",
        "        anomaly_flags=equity_anomaly_flags\n",
        "    )\n",
        "\n",
        "    # Execute the cleansing pipeline for the interest rate tick data.\n",
        "    clean_rate_df, rate_audit = cleanse_tick_data(\n",
        "        raw_tick_df=raw_rate_df,\n",
        "        anomaly_flags=rate_anomaly_flags\n",
        "    )\n",
        "\n",
        "    # --- 2. Standardize Announcement Timestamps ---\n",
        "    # Convert all local announcement times to unambiguous UTC timestamps.\n",
        "    clean_announcement_df, timestamp_audit = standardize_announcement_timestamps(\n",
        "        announcement_df=raw_announcement_df\n",
        "    )\n",
        "\n",
        "    # --- 3. Prepare Macroeconomic Data (to Levels) ---\n",
        "    # This call now correctly omits the transformation map. The function's\n",
        "    # responsibility is only to clean, pivot, interpolate, and validate the macro data.\n",
        "    prepared_macro_df_levels, dropped_macro_series = prepare_macroeconomic_data(\n",
        "        macro_df=raw_macro_df,\n",
        "        target_market=target_market\n",
        "    )\n",
        "\n",
        "    # --- 4. Compile and Return All Results ---\n",
        "    # The output dictionary is structured to clearly distinguish between the\n",
        "    # cleansed data and the audit trails generated during the process.\n",
        "    results = {\n",
        "        \"clean_equity_df\": clean_equity_df,\n",
        "        \"equity_cleaning_audit\": equity_audit,\n",
        "        \"clean_rate_df\": clean_rate_df,\n",
        "        \"rate_cleaning_audit\": rate_audit,\n",
        "        \"clean_announcement_df\": clean_announcement_df,\n",
        "        \"timestamp_conversion_audit\": timestamp_audit,\n",
        "        \"prepared_macro_df_levels\": prepared_macro_df_levels,\n",
        "        \"dropped_macro_series\": dropped_macro_series\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ESeZ2riKdiOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Announcement Window Definition and Data Extraction\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 4, STEP 1: PRECISE EVENT WINDOW TEMPORAL BOUNDARY CONSTRUCTION\n",
        "# =============================================================================\n",
        "\n",
        "def construct_event_windows(\n",
        "    clean_announcement_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs and merges temporal boundaries for monetary policy announcements.\n",
        "\n",
        "    Purpose:\n",
        "        This function defines the precise start (`t_before`) and end (`t_after`)\n",
        "        of the high-frequency analysis window around each event. It is a\n",
        "        critical step for isolating the market reaction. This implementation\n",
        "        includes a robust mechanism to handle rare cases where event windows\n",
        "        for different announcements overlap, merging them into a single,\n",
        "        continuous window to avoid analytical ambiguity.\n",
        "\n",
        "    Process:\n",
        "        1.  **Parameter Extraction**: Retrieves `window_minutes_before` and\n",
        "            `window_minutes_after` from the configuration dictionary.\n",
        "        2.  **Initial Boundary Calculation**: For each event's UTC timestamp, it\n",
        "            vectorially calculates the initial `t_before` and `t_after` boundaries.\n",
        "        3.  **Overlap Group Identification**:\n",
        "            - The events are sorted chronologically.\n",
        "            - It identifies the start of each new, non-overlapping block of events.\n",
        "              A new block begins if an event's start time (`t_before`) is after\n",
        "              the previous event's end time (`t_after`).\n",
        "            - A cumulative sum over this \"new block\" indicator creates a unique\n",
        "              group ID for each set of contiguous, overlapping windows.\n",
        "        4.  **Window Union**:\n",
        "            - It groups the events by this new overlap group ID.\n",
        "            - Within each group, it calculates the union of the windows by\n",
        "              finding the minimum of all `t_before` times and the maximum of\n",
        "              all `t_after` times.\n",
        "            - `groupby().transform()` is used to broadcast these unionized\n",
        "              boundaries back to every event within the overlapping group.\n",
        "        5.  **Validation and Finalization**: The final DataFrame is returned,\n",
        "            indexed by `event_id`, with consistent and non-overlapping windows.\n",
        "\n",
        "    Args:\n",
        "        clean_announcement_df (pd.DataFrame): The processed announcement data\n",
        "                                              containing the essential\n",
        "                                              'announcement_timestamp_utc' and\n",
        "                                              'event_id' columns.\n",
        "        config (Dict[str, Any]): The study's main configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame indexed by 'event_id' containing the\n",
        "                      announcement timestamp and its final, potentially merged,\n",
        "                      `t_before` and `t_after` boundaries.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation and Parameter Extraction ---\n",
        "    # Ensure the required column is present.\n",
        "    if 'announcement_timestamp_utc' not in clean_announcement_df.columns:\n",
        "        raise KeyError(\"The 'announcement_timestamp_utc' column is missing from the announcement DataFrame.\")\n",
        "\n",
        "    # Retrieve window parameters from the configuration dictionary.\n",
        "    params: Dict[str, Any] = config.get(\"identification_params\", {})\n",
        "    minutes_before: int = params.get(\"window_minutes_before\", 10)\n",
        "    minutes_after: int = params.get(\"window_minutes_after\", 20)\n",
        "\n",
        "    # --- 2. Initial Boundary Calculation ---\n",
        "    # Create a working copy with the necessary columns.\n",
        "    windows_df = clean_announcement_df[['event_id', 'announcement_timestamp_utc']].copy()\n",
        "\n",
        "    # Calculate the 'before' timestamp boundary for the window.\n",
        "    windows_df['t_before'] = windows_df['announcement_timestamp_utc'] - pd.Timedelta(minutes=minutes_before)\n",
        "\n",
        "    # Calculate the 'after' timestamp boundary for the window.\n",
        "    windows_df['t_after'] = windows_df['announcement_timestamp_utc'] + pd.Timedelta(minutes=minutes_after)\n",
        "\n",
        "    # --- 3. Overlap Group Identification ---\n",
        "    # Sort the DataFrame chronologically to compare adjacent events.\n",
        "    windows_df.sort_values('announcement_timestamp_utc', inplace=True)\n",
        "\n",
        "    # An overlap occurs if the start of the current window is before the end of the previous one.\n",
        "    # We identify the START of a NEW, non-overlapping block.\n",
        "    is_new_block: pd.Series = windows_df['t_before'] > windows_df['t_after'].shift(1)\n",
        "\n",
        "    # The very first event always starts a new block.\n",
        "    is_new_block.iloc[0] = True\n",
        "\n",
        "    # The cumulative sum of this boolean series creates a unique ID for each contiguous block of overlapping events.\n",
        "    overlap_group_id: pd.Series = is_new_block.cumsum()\n",
        "\n",
        "    # --- 4. Window Union ---\n",
        "    # If there are any overlaps, the number of unique groups will be less than the number of events.\n",
        "    if overlap_group_id.nunique() < len(windows_df):\n",
        "        # For each group, find the earliest start time and latest end time.\n",
        "        # `groupby().transform()` broadcasts the result of the aggregation (min/max)\n",
        "        # back to every row within the group, effectively applying the union.\n",
        "        union_t_before = windows_df.groupby(overlap_group_id)['t_before'].transform('min')\n",
        "        union_t_after = windows_df.groupby(overlap_group_id)['t_after'].transform('max')\n",
        "\n",
        "        # Overwrite the initial boundaries with the new, merged boundaries.\n",
        "        windows_df['t_before'] = union_t_before\n",
        "        windows_df['t_after'] = union_t_after\n",
        "\n",
        "    # --- 5. Validation and Finalization ---\n",
        "    # Final check for logical consistency.\n",
        "    if not (windows_df['t_before'] < windows_df['announcement_timestamp_utc']).all() or \\\n",
        "       not (windows_df['announcement_timestamp_utc'] < windows_df['t_after']).all():\n",
        "        raise ValueError(\"Window boundary inconsistency detected after processing overlaps.\")\n",
        "\n",
        "    # Set event_id as the index for efficient lookups and return.\n",
        "    # The DataFrame is re-indexed to its original order if sorting changed it.\n",
        "    return windows_df.set_index('event_id').reindex(clean_announcement_df['event_id'])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 4, STEPS 2 & 3: PRE/POST-EVENT PRICE EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "def _extract_price_with_fallback(\n",
        "    event_timestamps: pd.Series,\n",
        "    tick_df: pd.DataFrame,\n",
        "    direction: str,\n",
        "    primary_tolerance: pd.Timedelta,\n",
        "    secondary_tolerance: pd.Timedelta\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts prices using a two-tiered fallback search with `pd.merge_asof`.\n",
        "\n",
        "    Purpose:\n",
        "        This is a high-performance helper function designed to find the nearest\n",
        "        tick to a series of event timestamps. It is the core engine for the\n",
        "        pre- and post-event price extraction. Its key feature is a robust,\n",
        "        two-tiered search strategy to handle cases of low liquidity around an\n",
        "        event.\n",
        "\n",
        "    Process:\n",
        "        1.  **Validation**: It first performs a critical check to ensure the\n",
        "            input `tick_df` is sorted by timestamp, as this is a prerequisite\n",
        "            for `pd.merge_asof` to function correctly and efficiently.\n",
        "        2.  **Tier 1 (Primary Search)**: It executes a `pd.merge_asof` to find\n",
        "            the nearest tick for each event timestamp within the narrow\n",
        "            `primary_tolerance`. This is the ideal case, finding a price very\n",
        "            close to the window boundary.\n",
        "        3.  **Identify Failures**: It identifies all events for which the\n",
        "            primary search failed to find a tick (resulting in a `NaN` price).\n",
        "        4.  **Tier 2 (Secondary Search)**: If any failures occurred, it re-runs\n",
        "            `pd.merge_asof` *only on the failed events* using the much wider\n",
        "            `secondary_tolerance`. This is the fallback mechanism.\n",
        "        5.  **Combine Results**: The results from the successful secondary\n",
        "            search are used to update the records that failed the primary search.\n",
        "        6.  **Audit**: A 'search_method' column is populated to explicitly\n",
        "            document which search tier was successful ('primary' or 'secondary')\n",
        "            or if both failed ('failed') for each event.\n",
        "\n",
        "    Args:\n",
        "        event_timestamps (pd.Series): A Series of UTC timestamps representing\n",
        "                                      the search points (e.g., all `t_before`\n",
        "                                      or `t_after` values).\n",
        "        tick_df (pd.DataFrame): The cleansed tick data, containing at least\n",
        "                                'timestamp_micros_utc' and 'price' columns.\n",
        "                                MUST be pre-sorted by timestamp.\n",
        "        direction (str): The direction for the as-of search. Must be one of\n",
        "                         'backward' (for pre-event prices), 'forward' (for\n",
        "                         post-event), or 'nearest'.\n",
        "        primary_tolerance (pd.Timedelta): The maximum time gap allowed for the\n",
        "                                          initial, high-precision search.\n",
        "        secondary_tolerance (pd.Timedelta): The wider time gap allowed for the\n",
        "                                            fallback search if the primary\n",
        "                                            search fails.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the same index as `event_timestamps`,\n",
        "                      containing the extracted price, the timestamp of that\n",
        "                      price, and metadata about the search method used.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input `tick_df` is not sorted by timestamp.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # `pd.merge_asof` relies on sorted data for its binary search algorithm.\n",
        "    # This check prevents incorrect results and is a critical safeguard.\n",
        "    if not tick_df['timestamp_micros_utc'].is_monotonic_increasing:\n",
        "        # Raise an error if the precondition is not met.\n",
        "        raise ValueError(\"Tick data must be sorted by timestamp for extraction.\")\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "    # Convert the input Series of event timestamps into a DataFrame, which is the\n",
        "    # required format for the 'left' table in `pd.merge_asof`.\n",
        "    events: pd.DataFrame = event_timestamps.to_frame(name='timestamp_micros_utc')\n",
        "\n",
        "    # --- Tier 1: Primary Search Window ---\n",
        "    # Execute the primary as-of merge. This is a highly optimized operation.\n",
        "    # 'direction' controls whether we look for the last tick before ('backward')\n",
        "    # or the first tick after ('forward') the event timestamp.\n",
        "    # 'tolerance' defines the maximum distance in time to look.\n",
        "    merged_primary: pd.DataFrame = pd.merge_asof(\n",
        "        left=events,\n",
        "        right=tick_df,\n",
        "        on='timestamp_micros_utc',\n",
        "        direction=direction,\n",
        "        tolerance=primary_tolerance\n",
        "    )\n",
        "    # Annotate the results with the search method used for auditing.\n",
        "    merged_primary['search_method'] = 'primary'\n",
        "\n",
        "    # --- Tier 2: Secondary (Fallback) Search Window ---\n",
        "    # Identify events where the primary search yielded no result (price is NaN).\n",
        "    failed_primary_mask: pd.Series = merged_primary['price'].isna()\n",
        "\n",
        "    # Proceed with the fallback only if there were failures to avoid unnecessary work.\n",
        "    if failed_primary_mask.any():\n",
        "        # Isolate the events that need a second search attempt.\n",
        "        failed_events: pd.DataFrame = merged_primary.loc[failed_primary_mask, ['timestamp_micros_utc']]\n",
        "\n",
        "        # Rerun `pd.merge_asof` on the subset of failed events, but with the wider\n",
        "        # secondary tolerance, increasing the chance of finding a match.\n",
        "        merged_secondary: pd.DataFrame = pd.merge_asof(\n",
        "            left=failed_events,\n",
        "            right=tick_df,\n",
        "            on='timestamp_micros_utc',\n",
        "            direction=direction,\n",
        "            tolerance=secondary_tolerance\n",
        "        )\n",
        "        # Annotate these fallback results as 'secondary'.\n",
        "        merged_secondary['search_method'] = 'secondary'\n",
        "\n",
        "        # Use the index of the failed events to align and update the original\n",
        "        # `merged_primary` DataFrame with the new results from the secondary search.\n",
        "        # This efficiently patches the missing values without a slow loop.\n",
        "        merged_primary.update(merged_secondary)\n",
        "\n",
        "    # --- Final Auditing ---\n",
        "    # For any events that still have a NaN price after both attempts, mark their\n",
        "    # search method as 'failed' for a complete and clear audit trail.\n",
        "    merged_primary['search_method'] = merged_primary['search_method'].where(\n",
        "        merged_primary['price'].notna(), 'failed'\n",
        "    )\n",
        "\n",
        "    # --- Rename Columns for Clarity ---\n",
        "    # Rename the merged columns to be specific about their origin (pre- or post-event)\n",
        "    # to prevent name collisions when this function's output is joined later.\n",
        "    # The original event timestamp is now redundant and is implicitly the index.\n",
        "    # The timestamp from the tick data is renamed to `timestamp_{direction}`.\n",
        "    return merged_primary.rename(\n",
        "        columns={\n",
        "            'price': f'price_{direction}',\n",
        "            'timestamp_micros_utc_y': f'timestamp_{direction}'\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def extract_window_prices(\n",
        "    event_windows_df: pd.DataFrame,\n",
        "    clean_tick_df: pd.DataFrame,\n",
        "    stabilization_ticks: int = 4,\n",
        "    stabilization_threshold: float = 0.01,\n",
        "    max_stabilization_attempts: int = 100\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts pre- and post-announcement prices from high-frequency data.\n",
        "\n",
        "    Purpose:\n",
        "        For each event, this function finds the last traded price just before\n",
        "        the analysis window (`price_before`) and the first stable price just\n",
        "        after the window (`price_after`). This is the core data extraction step\n",
        "        for calculating high-frequency surprises.\n",
        "\n",
        "    Process:\n",
        "        1.  **Pre-Event Price**:\n",
        "            - Uses `pd.merge_asof` for a highly efficient \"as-of\" join to find\n",
        "              the last trade at or before `t_before`.\n",
        "            - Implements a two-tiered fallback: first searches within the\n",
        "              standard window, then extends the search to 2 hours before if\n",
        "              no trade is found.\n",
        "        2.  **Post-Event Price**:\n",
        "            - Finds the first trade at or after `t_after`.\n",
        "            - **Stabilization Check**: After finding an initial price, it\n",
        "              inspects the next N-1 trades (default 3). If the price range\n",
        "              (max/min - 1) of these N trades exceeds a threshold (default 1%),\n",
        "              the initial price is discarded, and the check is repeated starting\n",
        "              from the next trade. This continues until a stable price is found\n",
        "              or a limit is reached.\n",
        "        3.  **Metadata**: Returns a rich DataFrame with extracted prices and\n",
        "            detailed metadata on the search method, stabilization status, and\n",
        "            timestamps for full auditability.\n",
        "\n",
        "    Args:\n",
        "        event_windows_df (pd.DataFrame): DataFrame of event windows from\n",
        "                                         `construct_event_windows`.\n",
        "        clean_tick_df (pd.DataFrame): The cleansed high-frequency tick data.\n",
        "        stabilization_ticks (int): Number of ticks (N) to check for stability.\n",
        "        stabilization_threshold (float): Max allowed price range for stability.\n",
        "        max_stabilization_attempts (int): Max iterations for the stabilization search.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by `event_id` with columns for pre-\n",
        "                      and post-event prices and extensive metadata.\n",
        "    \"\"\"\n",
        "    # --- Pre-computation and Sorting ---\n",
        "    # Sorting is critical for the performance of merge_asof and searchsorted.\n",
        "    ticks = clean_tick_df.sort_values('timestamp_micros_utc').reset_index(drop=True)\n",
        "\n",
        "    # --- Pre-Event Price Extraction ---\n",
        "    pre_event_prices = _extract_price_with_fallback(\n",
        "        event_timestamps=event_windows_df['t_before'],\n",
        "        tick_df=ticks[['timestamp_micros_utc', 'price']],\n",
        "        direction='backward',\n",
        "        primary_tolerance=pd.Timedelta(minutes=10), # Standard window\n",
        "        secondary_tolerance=pd.Timedelta(hours=2)   # Extended fallback\n",
        "    )\n",
        "    pre_event_prices = pre_event_prices.set_index(event_windows_df.index)\n",
        "\n",
        "    # --- Post-Event Price Extraction with Stabilization ---\n",
        "    post_event_results = []\n",
        "    # Find the index of the first tick at or after each t_after.\n",
        "    # searchsorted is a highly optimized binary search.\n",
        "    start_indices = ticks['timestamp_micros_utc'].searchsorted(event_windows_df['t_after'], side='left')\n",
        "\n",
        "    for event_id, start_idx in zip(event_windows_df.index, start_indices):\n",
        "        attempts = 0\n",
        "        stable_price_found = False\n",
        "\n",
        "        current_idx = start_idx\n",
        "\n",
        "        # Iteratively search for a stable price.\n",
        "        while not stable_price_found and attempts < max_stabilization_attempts and (current_idx + stabilization_ticks) <= len(ticks):\n",
        "            # Define the slice of ticks to check for stability.\n",
        "            window_slice = ticks.iloc[current_idx : current_idx + stabilization_ticks]\n",
        "\n",
        "            # Calculate the price range within the stabilization window.\n",
        "            min_p, max_p = window_slice['price'].min(), window_slice['price'].max()\n",
        "            price_range = (max_p / min_p - 1) if min_p > 0 else float('inf')\n",
        "\n",
        "            # Check if the price is stable.\n",
        "            if price_range < stabilization_threshold:\n",
        "                stable_price_found = True\n",
        "                post_event_results.append({\n",
        "                    'event_id': event_id,\n",
        "                    'price_forward': window_slice['price'].iloc[0],\n",
        "                    'timestamp_forward': window_slice['timestamp_micros_utc'].iloc[0],\n",
        "                    'stabilization_status': 'stable',\n",
        "                    'stabilization_attempts': attempts + 1\n",
        "                })\n",
        "            else:\n",
        "                # If not stable, advance to the next tick and try again.\n",
        "                current_idx += 1\n",
        "                attempts += 1\n",
        "\n",
        "        # If the loop finishes without finding a stable price, record the failure.\n",
        "        if not stable_price_found:\n",
        "            price = ticks['price'].iloc[start_idx] if start_idx < len(ticks) else np.nan\n",
        "            timestamp = ticks['timestamp_micros_utc'].iloc[start_idx] if start_idx < len(ticks) else pd.NaT\n",
        "            post_event_results.append({\n",
        "                'event_id': event_id,\n",
        "                'price_forward': price,\n",
        "                'timestamp_forward': timestamp,\n",
        "                'stabilization_status': 'unstable' if start_idx < len(ticks) else 'no_trade_found',\n",
        "                'stabilization_attempts': attempts\n",
        "            })\n",
        "\n",
        "    post_event_prices = pd.DataFrame(post_event_results).set_index('event_id')\n",
        "\n",
        "    # --- Combine Results ---\n",
        "    # Join the pre- and post-event results back to the main event window frame.\n",
        "    final_results = event_windows_df.join(pre_event_prices, how='left').join(post_event_prices, how='left')\n",
        "\n",
        "    return final_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 4\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase2_task4_data_extraction(\n",
        "    clean_announcement_df: pd.DataFrame,\n",
        "    clean_equity_df: pd.DataFrame,\n",
        "    clean_rate_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full data extraction pipeline for Task 4.\n",
        "\n",
        "    This master function first defines the event windows and then extracts the\n",
        "    pre- and post-event prices for both the equity and interest rate series,\n",
        "    returning a dictionary of the results.\n",
        "\n",
        "    Args:\n",
        "        clean_announcement_df (pd.DataFrame): Cleansed announcement metadata.\n",
        "        clean_equity_df (pd.DataFrame): Cleansed high-frequency equity data.\n",
        "        clean_rate_df (pd.DataFrame): Cleansed high-frequency rate data.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the event windows\n",
        "                                 and the price extraction results for both\n",
        "                                 equity and rate instruments.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Construct Event Windows ---\n",
        "    # This is done once and serves as the basis for all extractions.\n",
        "    event_windows = construct_event_windows(clean_announcement_df, config)\n",
        "\n",
        "    # --- Step 2 & 3: Extract Prices for Equity Series ---\n",
        "    equity_prices = extract_window_prices(\n",
        "        event_windows_df=event_windows,\n",
        "        clean_tick_df=clean_equity_df,\n",
        "        # Parameters can be exposed or read from config if needed\n",
        "        stabilization_ticks=4,\n",
        "        stabilization_threshold=0.01,\n",
        "        max_stabilization_attempts=100\n",
        "    )\n",
        "\n",
        "    # --- Step 2 & 3: Extract Prices for Rate Series ---\n",
        "    rate_prices = extract_window_prices(\n",
        "        event_windows_df=event_windows,\n",
        "        clean_tick_df=clean_rate_df,\n",
        "        stabilization_ticks=4,\n",
        "        stabilization_threshold=0.01, # Threshold might differ for rates\n",
        "        max_stabilization_attempts=100\n",
        "    )\n",
        "\n",
        "    # --- Aggregate and return all results ---\n",
        "    results = {\n",
        "        \"event_windows\": event_windows,\n",
        "        \"equity_price_extraction_results\": equity_prices,\n",
        "        \"rate_price_extraction_results\": rate_prices\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "9_CFdPZJefmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Raw Surprise Calculation\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 5, STEP 1: INTEREST RATE SURPRISE COMPUTATION\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_rate_surprises(\n",
        "    price_extraction_results: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    contract_specifications: Dict[str, Dict[str, Any]]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes interest rate surprises using explicit, configuration-driven logic.\n",
        "\n",
        "    Purpose:\n",
        "        This function translates the pre- and post-event prices of interest\n",
        "        rate futures into standardized monetary policy surprises, measured in\n",
        "        basis points. It is designed to be robust and extensible by decoupling\n",
        "        the calculation logic from the specific quoting conventions of financial\n",
        "        instruments, which are provided via a configuration dictionary.\n",
        "\n",
        "    Process:\n",
        "        1.  **Input Validation**: Checks that the contract specification dictionary\n",
        "            contains entries for all central banks present in the data.\n",
        "        2.  **Merge Data**: Joins price data with announcement metadata to access\n",
        "            the `central_bank` for each event.\n",
        "        3.  **Map Specifications**: Maps the `central_bank` of each event to its\n",
        "            corresponding `convention` and `multiplier` from the provided\n",
        "            `contract_specifications` dictionary.\n",
        "        4.  **Conditional Calculation**: Uses vectorized operations (`np.select`)\n",
        "            to apply the correct formula based on the mapped 'convention':\n",
        "            - **'100_minus_rate'**: For contracts like Fed Funds Futures, where\n",
        "              Price = 100 - Rate. The surprise is calculated as:\n",
        "              `s_rate = -multiplier * (P_after - P_before)`\n",
        "            - **'yield_based'**: For contracts priced as a yield or index. The\n",
        "              surprise is calculated as a basis point change:\n",
        "              `s_rate = multiplier * (P_after - P_before) / P_before`\n",
        "        5.  **Safety Checks**: Ensures division-by-zero is avoided for the\n",
        "            'yield_based' convention.\n",
        "\n",
        "    Args:\n",
        "        price_extraction_results (pd.DataFrame): DataFrame from Task 4 with\n",
        "                                                  'price_backward' and\n",
        "                                                  'price_forward' columns, indexed\n",
        "                                                  by 'event_id'.\n",
        "        announcement_df (pd.DataFrame): The clean announcement metadata, which\n",
        "                                        contains the 'central_bank' and 'event_id'\n",
        "                                        columns.\n",
        "        contract_specifications (Dict[str, Dict[str, Any]]): A dictionary\n",
        "            mapping each central bank to its instrument's properties.\n",
        "            Example: {'FED': {'convention': '100_minus_rate', 'multiplier': 100}}\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series of calculated interest rate surprises in basis\n",
        "                   points, indexed by 'event_id'.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a central bank in the data is not found in the\n",
        "                  `contract_specifications` dictionary.\n",
        "        ValueError: If input DataFrames are empty.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if price_extraction_results.empty or announcement_df.empty:\n",
        "        raise ValueError(\"Input DataFrames cannot be empty.\")\n",
        "\n",
        "    # Check that all central banks in the data have a defined specification.\n",
        "    banks_in_data = set(announcement_df['central_bank'].unique())\n",
        "    banks_in_spec = set(contract_specifications.keys())\n",
        "    if not banks_in_data.issubset(banks_in_spec):\n",
        "        missing_banks = banks_in_data - banks_in_spec\n",
        "        raise KeyError(f\"Contract specifications are missing for the following central banks: {sorted(list(missing_banks))}\")\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "    # Merge central bank information onto the price data using the event_id index.\n",
        "    data = price_extraction_results.join(\n",
        "        announcement_df.set_index('event_id')[['central_bank']],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # --- Map Specifications to each event ---\n",
        "    # Create new columns in the DataFrame for convention and multiplier.\n",
        "    data['convention'] = data['central_bank'].map(lambda x: contract_specifications[x]['convention'])\n",
        "    data['multiplier'] = data['central_bank'].map(lambda x: contract_specifications[x]['multiplier'])\n",
        "\n",
        "    # --- Conditional Surprise Calculation ---\n",
        "    # Define the conditions based on the mapped 'convention'.\n",
        "    conditions = [\n",
        "        data['convention'] == '100_minus_rate',\n",
        "        data['convention'] == 'yield_based'\n",
        "    ]\n",
        "\n",
        "    # Define the calculation corresponding to each condition.\n",
        "    # Calculation for '100_minus_rate' convention.\n",
        "    calc_100_minus_rate = -data['multiplier'] * (data['price_forward'] - data['price_backward'])\n",
        "\n",
        "    # Calculation for 'yield_based' convention, with a safety check for division by zero.\n",
        "    safe_price_backward = data['price_backward'].where(data['price_backward'] > 1e-9, np.nan)\n",
        "    calc_yield_based = data['multiplier'] * (data['price_forward'] - safe_price_backward) / safe_price_backward\n",
        "\n",
        "    # Create the list of calculation vectors.\n",
        "    calculations = [\n",
        "        calc_100_minus_rate,\n",
        "        calc_yield_based\n",
        "    ]\n",
        "\n",
        "    # Use np.select for efficient, vectorized conditional computation.\n",
        "    # If an event's convention matches none of the conditions, the result will be NaN.\n",
        "    surprises = np.select(conditions, calculations, default=np.nan)\n",
        "\n",
        "    # Return the final result as a pandas Series with the correct name and index.\n",
        "    return pd.Series(surprises, index=data.index, name='rate_surprise_bps')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 5, STEP 2: EQUITY PRICE SURPRISE LOG RETURN CALCULATION\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_equity_surprises(\n",
        "    price_extraction_results: pd.DataFrame,\n",
        "    outlier_threshold_pct: float = 20.0\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Computes equity price surprises using log returns.\n",
        "\n",
        "    Purpose:\n",
        "        This function calculates the continuously compounded return on an\n",
        "        equity index future across the event window. This serves as the measure\n",
        "        of the equity market's reaction to the announcement.\n",
        "\n",
        "    Process:\n",
        "        1.  **Log Return Formula**: Implements the standard formula for log returns:\n",
        "            `s_equity = 100 * log(price_after / price_before)`\n",
        "        2.  **Safety Checks**: The calculation is wrapped in a `np.where` clause\n",
        "            to ensure both pre- and post-event prices are strictly positive,\n",
        "            preventing mathematical errors with the logarithm.\n",
        "        3.  **Outlier Flagging**: It identifies and flags any calculated surprises\n",
        "            whose absolute value exceeds a large, pre-defined threshold,\n",
        "            indicating a potential data error or an extremely volatile event\n",
        "            that may warrant manual review.\n",
        "\n",
        "    Args:\n",
        "        price_extraction_results (pd.DataFrame): DataFrame from Task 4 with\n",
        "                                                  'price_backward' and\n",
        "                                                  'price_forward' columns.\n",
        "        outlier_threshold_pct (float): The absolute percentage change above\n",
        "                                       which a surprise is flagged as an outlier.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]:\n",
        "        - A Series of calculated equity surprises in percentage points,\n",
        "          indexed by 'event_id'.\n",
        "        - A boolean Series of the same index, where `True` indicates a\n",
        "          surprise that exceeded the outlier threshold.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if price_extraction_results.empty:\n",
        "        raise ValueError(\"Input DataFrame cannot be empty.\")\n",
        "\n",
        "    # --- Log Return Calculation ---\n",
        "    # Alias prices for clarity.\n",
        "    price_before = price_extraction_results['price_backward']\n",
        "    price_after = price_extraction_results['price_forward']\n",
        "\n",
        "    # Create a mask to identify valid prices for the log operation.\n",
        "    valid_prices_mask = (price_before > 0) & (price_after > 0) & price_before.notna() & price_after.notna()\n",
        "\n",
        "    # Initialize surprise series with NaNs.\n",
        "    surprises = pd.Series(np.nan, index=price_extraction_results.index)\n",
        "\n",
        "    # Equation: s_equity = 100 * ln(P_after / P_before)\n",
        "    # Apply the log return formula only where prices are valid.\n",
        "    surprises.loc[valid_prices_mask] = 100 * np.log(\n",
        "        price_after[valid_prices_mask] / price_before[valid_prices_mask]\n",
        "    )\n",
        "\n",
        "    # --- Outlier Flagging ---\n",
        "    # Identify surprises with an absolute magnitude greater than the threshold.\n",
        "    outlier_flags = surprises.abs() > outlier_threshold_pct\n",
        "\n",
        "    return surprises.rename('equity_surprise_pct'), outlier_flags.rename('is_outlier')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 5, STEP 3: SURPRISE VECTOR CONSTRUCTION AND QUALITY CONTROL\n",
        "# =============================================================================\n",
        "\n",
        "def construct_surprise_vectors(\n",
        "    rate_surprises: pd.Series,\n",
        "    equity_surprises: pd.Series,\n",
        "    announcement_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, np.ndarray], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Assembles surprise vectors and performs a final quality control filter.\n",
        "\n",
        "    Purpose:\n",
        "        This function combines the individual rate and equity surprises into\n",
        "        2x1 vectors for each event. It then applies a statistical outlier\n",
        "        filter and structures the final, clean data into NumPy arrays ready\n",
        "        for the rotational decomposition analysis.\n",
        "\n",
        "    Process:\n",
        "        1.  **Combine Surprises**: Merges the rate and equity surprise Series\n",
        "            into a single DataFrame, aligned by `event_id`.\n",
        "        2.  **Handle Missing Data**: Drops any events for which either surprise\n",
        "            could not be calculated.\n",
        "        3.  **5-Sigma Filter**: For each central bank's set of announcements\n",
        "            separately, it calculates the standard deviation of each surprise\n",
        "            type. It then removes any event where *either* the rate or equity\n",
        "            surprise exceeds 5 times its respective standard deviation.\n",
        "        4.  **Structure Output**: Groups the cleaned surprises by central bank\n",
        "            and converts them into `(N, 2)` NumPy arrays, stored in a dictionary.\n",
        "        5.  **Audit**: Returns a DataFrame containing all the outliers that\n",
        "            were removed during the filtering process for full transparency.\n",
        "\n",
        "    Args:\n",
        "        rate_surprises (pd.Series): Series of rate surprises.\n",
        "        equity_surprises (pd.Series): Series of equity surprises.\n",
        "        announcement_df (pd.DataFrame): Clean announcement metadata to get\n",
        "                                        `central_bank` info.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[str, np.ndarray], pd.DataFrame]:\n",
        "        - A dictionary where keys are central bank names ('FED', 'ECB') and\n",
        "          values are (N, 2) NumPy arrays of their surprise vectors.\n",
        "        - A DataFrame containing the surprise data for all events that were\n",
        "          filtered out as outliers.\n",
        "    \"\"\"\n",
        "    # --- Combine Surprises into a single DataFrame ---\n",
        "    df = pd.concat([rate_surprises, equity_surprises], axis=1)\n",
        "    df = df.join(announcement_df.set_index('event_id')[['central_bank']], how='left')\n",
        "\n",
        "    # --- Handle Missing Data ---\n",
        "    # Drop events where either surprise is missing before statistical calculations.\n",
        "    clean_df = df.dropna()\n",
        "    if clean_df.empty:\n",
        "        raise ValueError(\"No complete surprise observations remaining after dropping NaNs.\")\n",
        "\n",
        "    # --- 5-Sigma Outlier Filter (per Central Bank) ---\n",
        "    # Calculate the standard deviation for each surprise type, grouped by central bank.\n",
        "    # .transform() broadcasts the result back to the original shape for easy comparison.\n",
        "    std_devs = clean_df.groupby('central_bank')[['rate_surprise_bps', 'equity_surprise_pct']].transform('std')\n",
        "\n",
        "    # Define the outlier condition: absolute surprise > 5 * standard deviation.\n",
        "    is_outlier = (clean_df[['rate_surprise_bps', 'equity_surprise_pct']].abs() > 5 * std_devs).any(axis=1)\n",
        "\n",
        "    # --- Separate clean data from outliers ---\n",
        "    outliers_df = clean_df[is_outlier].copy()\n",
        "    final_df = clean_df[~is_outlier].copy()\n",
        "\n",
        "    # --- Structure Output ---\n",
        "    surprise_vectors = {}\n",
        "    # Group the final clean DataFrame by central bank.\n",
        "    for bank, group in final_df.groupby('central_bank'):\n",
        "        # For each bank, extract the two surprise columns and convert to a NumPy array.\n",
        "        # This creates the S_t = [s_rate, s_equity]' vectors.\n",
        "        surprise_vectors[bank] = group[['rate_surprise_bps', 'equity_surprise_pct']].values\n",
        "\n",
        "    return surprise_vectors, outliers_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 5\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase2_task5_surprise_calculation(\n",
        "    equity_price_results: pd.DataFrame,\n",
        "    rate_price_results: pd.DataFrame,\n",
        "    clean_announcement_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full surprise calculation pipeline for Task 5.\n",
        "\n",
        "    This master function executes the calculation and quality control steps\n",
        "    to produce the final, analysis-ready surprise vectors.\n",
        "\n",
        "    Args:\n",
        "        equity_price_results (pd.DataFrame): Price extraction results for equity.\n",
        "        rate_price_results (pd.DataFrame): Price extraction results for rates.\n",
        "        clean_announcement_df (pd.DataFrame): Cleansed announcement metadata.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the final surprise vectors,\n",
        "                        the outliers removed, and intermediate results for\n",
        "                        auditing.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Calculate Interest Rate Surprises ---\n",
        "    rate_surprises = calculate_rate_surprises(\n",
        "        price_extraction_results=rate_price_results,\n",
        "        announcement_df=clean_announcement_df\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Calculate Equity Surprises ---\n",
        "    equity_surprises, equity_outlier_flags = calculate_equity_surprises(\n",
        "        price_extraction_results=equity_price_results\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Construct Final Surprise Vectors ---\n",
        "    surprise_vectors, statistical_outliers = construct_surprise_vectors(\n",
        "        rate_surprises=rate_surprises,\n",
        "        equity_surprises=equity_surprises,\n",
        "        announcement_df=clean_announcement_df\n",
        "    )\n",
        "\n",
        "    # --- Aggregate and return all results ---\n",
        "    results = {\n",
        "        \"surprise_vectors\": surprise_vectors,\n",
        "        \"statistical_outliers_removed\": statistical_outliers,\n",
        "        \"intermediate_results\": {\n",
        "            \"rate_surprises_raw\": rate_surprises,\n",
        "            \"equity_surprises_raw\": equity_surprises,\n",
        "            \"equity_extreme_value_flags\": equity_outlier_flags\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "SVP_LmeufbCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Rotational-Angle Decomposition Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# CUSTOM EXCEPTION FOR THIS TASK\n",
        "# =============================================================================\n",
        "\n",
        "class IdentificationError(Exception):\n",
        "    \"\"\"\n",
        "    Custom exception raised for failures in the structural shock identification.\n",
        "\n",
        "    Purpose:\n",
        "        This exception is raised when the rotational-angle decomposition\n",
        "        method fails to identify a set of structural shocks that satisfy the\n",
        "        theoretical sign restrictions. This is a critical failure in the\n",
        "        econometric methodology, indicating that the data and the theoretical\n",
        "        assumptions are inconsistent.\n",
        "\n",
        "    Process:\n",
        "        This exception should be raised by the `identify_structural_shocks`\n",
        "        function if, after searching through all possible rotation angles, no\n",
        "        angle produces a candidate monetary policy shock that is simultaneously\n",
        "        positively correlated with interest rate surprises and negatively\n",
        "        correlated with equity surprises. The error message should clearly\n",
        "        state which central bank's data failed the identification procedure.\n",
        "        It extends Python's base `Exception` class and serves as a specific,\n",
        "        catchable error that signals a fundamental problem with the analysis,\n",
        "        requiring researcher intervention.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 6, STEPS 1-3: ROTATIONAL-ANGLE DECOMPOSITION\n",
        "# =============================================================================\n",
        "\n",
        "def _generate_rotation_matrices(\n",
        "    num_angles: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates a grid of angles and their corresponding 2D rotation matrices.\n",
        "\n",
        "    This helper creates a 3D array where each slice along the first axis is a\n",
        "    2x2 orthogonal rotation matrix for an angle in the grid.\n",
        "\n",
        "    Args:\n",
        "        num_angles (int): The number of angles to generate on the grid\n",
        "                          (e.g., 999).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]:\n",
        "        - A 1D array of angles `theta` of shape (num_angles,).\n",
        "        - A 3D array of rotation matrices `Q` of shape (num_angles, 2, 2).\n",
        "    \"\"\"\n",
        "    # Create a grid of `num_angles` equally spaced angles from 0 to 2*pi.\n",
        "    theta_grid = np.linspace(0, 2 * np.pi, num_angles)\n",
        "\n",
        "    # Compute cosines and sines for all angles in a single vectorized operation.\n",
        "    cos_theta = np.cos(theta_grid)\n",
        "    sin_theta = np.sin(theta_grid)\n",
        "\n",
        "    # Pre-allocate the 3D array for all rotation matrices for efficiency.\n",
        "    rotation_matrices = np.zeros((num_angles, 2, 2))\n",
        "\n",
        "    # Assemble the rotation matrices using the pre-computed trig values.\n",
        "    # Equation: Q(theta) = [[cos(theta), -sin(theta)], [sin(theta), cos(theta)]]\n",
        "    rotation_matrices[:, 0, 0] = cos_theta\n",
        "    rotation_matrices[:, 0, 1] = -sin_theta\n",
        "    rotation_matrices[:, 1, 0] = sin_theta\n",
        "    rotation_matrices[:, 1, 1] = cos_theta\n",
        "\n",
        "    return theta_grid, rotation_matrices\n",
        "\n",
        "\n",
        "def _find_admissible_rotations(\n",
        "    surprise_vectors: np.ndarray,\n",
        "    rotation_matrices: np.ndarray,\n",
        "    method: str = 'correlation',\n",
        "    pass_rate_threshold: float = 0.60\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Identifies admissible rotation angles based on specified sign restrictions.\n",
        "\n",
        "    Purpose:\n",
        "        This is the core identification function. It tests each possible rotation\n",
        "        of the reduced-form surprises against a set of theoretical sign\n",
        "        restrictions to find the subset of rotations that are consistent with a\n",
        "        structural monetary policy shock.\n",
        "\n",
        "    Process:\n",
        "        1.  **Rotate Surprises**: It first performs a batched matrix multiplication\n",
        "            to apply every rotation matrix to the surprise vectors, generating a\n",
        "            full set of candidate structural shocks.\n",
        "            Equation: `epsilon(theta) = Q(theta) * S'`\n",
        "        2.  **Select Method**: Based on the `method` parameter, it proceeds with\n",
        "            one of two validation algorithms:\n",
        "            a. **'correlation' (Default)**:\n",
        "               - It calculates the Pearson correlation between the candidate\n",
        "                 monetary policy shock and each of the original surprise series\n",
        "                 in a fully vectorized and exact manner.\n",
        "               - An angle is admissible if:\n",
        "                 `rho(epsilon_mp, s_rate) > 0` AND `rho(epsilon_mp, s_equity) < 0`.\n",
        "            b. **'observation_count'**:\n",
        "               - For each observation, it checks if the co-movement between the\n",
        "                 candidate shock and the surprises has the correct sign.\n",
        "               - It calculates the percentage of observations for which both\n",
        "                 sign restrictions hold simultaneously.\n",
        "               - An angle is admissible if this \"pass rate\" is greater than or\n",
        "                 equal to `pass_rate_threshold` (e.g., 60%).\n",
        "        3.  **Return Mask**: It returns a boolean array indicating which angles\n",
        "            in the original grid are admissible.\n",
        "\n",
        "    Args:\n",
        "        surprise_vectors (np.ndarray): An (N, 2) array of [rate, equity] surprises.\n",
        "        rotation_matrices (np.ndarray): A (G, 2, 2) array of rotation matrices.\n",
        "        method (str): The identification method to use. Must be either\n",
        "                      'correlation' or 'observation_count'.\n",
        "        pass_rate_threshold (float): The minimum pass rate for the\n",
        "                                     'observation_count' method.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D boolean array of length G, where `True` indicates an\n",
        "                    admissible rotation angle.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported `method` is provided.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    if method not in ['correlation', 'observation_count']:\n",
        "        raise ValueError(f\"Unsupported method '{method}'. Must be 'correlation' or 'observation_count'.\")\n",
        "\n",
        "    # --- 2. Vectorized Rotation of Surprises ---\n",
        "    # Equation: epsilon(theta) = Q(theta) * S'\n",
        "    # Use np.einsum for efficient batched matrix multiplication.\n",
        "    # 'gij,jk->gik' means: for each grid point g, multiply the (i,j) matrix Q\n",
        "    # with the (j,k) matrix S.T, resulting in a (g,i,k) matrix.\n",
        "    # Result shape: (num_angles, 2, num_events)\n",
        "    rotated_shocks: np.ndarray = np.einsum('gij,jk->gik', rotation_matrices, surprise_vectors.T)\n",
        "\n",
        "    # The candidate monetary policy shock is the first row of each rotated matrix.\n",
        "    mp_shocks: np.ndarray = rotated_shocks[:, 0, :]  # Shape: (num_angles, num_events)\n",
        "\n",
        "    # --- 3. Apply Identification Method ---\n",
        "    if method == 'correlation':\n",
        "        # --- 3a. Exact Vectorized Correlation Method ---\n",
        "        # De-mean all series. N = number of events.\n",
        "        N = surprise_vectors.shape[0]\n",
        "        mp_shocks_demeaned = mp_shocks - mp_shocks.mean(axis=1, keepdims=True)\n",
        "        surprises_demeaned = surprise_vectors - surprise_vectors.mean(axis=0)\n",
        "\n",
        "        # Calculate the covariance term (numerator of correlation).\n",
        "        # einsum 'gn,nj->gj' computes the dot product of each of the G candidate shocks\n",
        "        # with the 2 original surprise series, resulting in a (G, 2) covariance matrix.\n",
        "        covariance = np.einsum('gn,nj->gj', mp_shocks_demeaned, surprises_demeaned) / (N - 1)\n",
        "\n",
        "        # Calculate the standard deviations (for the denominator).\n",
        "        mp_shocks_std = mp_shocks_demeaned.std(axis=1)\n",
        "        surprises_std = surprises_demeaned.std(axis=0)\n",
        "\n",
        "        # Calculate the correlation matrix by dividing covariance by the product of stds.\n",
        "        # Add a small epsilon to prevent division by zero.\n",
        "        correlations = covariance / (mp_shocks_std[:, np.newaxis] * surprises_std + 1e-12)\n",
        "\n",
        "        # Apply sign restrictions on the correlations.\n",
        "        # Restriction 1: rho(epsilon_mp, s_rate) > 0\n",
        "        corr_rate_ok = correlations[:, 0] > 0\n",
        "        # Restriction 2: rho(epsilon_mp, s_equity) < 0\n",
        "        corr_equity_ok = correlations[:, 1] < 0\n",
        "\n",
        "        # An angle is admissible if both correlation restrictions hold.\n",
        "        is_admissible = corr_rate_ok & corr_equity_ok\n",
        "\n",
        "    elif method == 'observation_count':\n",
        "        # --- 3b. Observation Count Method ---\n",
        "        # De-mean the series to analyze co-movements around the mean.\n",
        "        mp_shocks_demeaned = mp_shocks - mp_shocks.mean(axis=1, keepdims=True)\n",
        "        surprises_demeaned = surprise_vectors - surprise_vectors.mean(axis=0)\n",
        "\n",
        "        # The sign of the product of demeaned series gives the sign of the co-movement for each observation.\n",
        "        # Shape: (num_angles, num_events, 2)\n",
        "        co_movements = mp_shocks_demeaned[:, :, np.newaxis] * surprises_demeaned\n",
        "\n",
        "        # Check if the sign restrictions hold for each individual observation.\n",
        "        # Restriction 1: Co-movement with rate surprise must be positive.\n",
        "        rate_co_movement_ok = co_movements[:, :, 0] > 0\n",
        "        # Restriction 2: Co-movement with equity surprise must be negative.\n",
        "        equity_co_movement_ok = co_movements[:, :, 1] < 0\n",
        "\n",
        "        # A successful observation is one where both restrictions hold.\n",
        "        both_restrictions_ok = rate_co_movement_ok & equity_co_movement_ok\n",
        "\n",
        "        # The pass rate for each angle is the mean of the boolean mask along the event axis.\n",
        "        pass_rate = np.mean(both_restrictions_ok, axis=1)\n",
        "\n",
        "        # An angle is admissible if its pass rate meets the threshold.\n",
        "        is_admissible = pass_rate >= pass_rate_threshold\n",
        "\n",
        "    # --- 4. Return the final boolean mask ---\n",
        "    return is_admissible\n",
        "\n",
        "\n",
        "def identify_structural_shocks(\n",
        "    surprise_vectors: Dict[str, np.ndarray],\n",
        "    num_angles: int = 999\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Identifies structural shocks from reduced-form surprises via rotational decomposition.\n",
        "\n",
        "    Purpose:\n",
        "        This is the main function for Task 6. It implements the Jarociński &\n",
        "        Karadi (2020) methodology to disentangle pure monetary policy shocks\n",
        "        from central bank information shocks.\n",
        "\n",
        "    Process:\n",
        "        For each central bank's surprise vectors:\n",
        "        1.  **Setup**: Generates a grid of rotation angles and corresponding\n",
        "            rotation matrices.\n",
        "        2.  **Identification**: Applies each rotation and identifies the subset of\n",
        "            \"admissible\" angles that satisfy the theoretical sign restrictions\n",
        "            on the correlations between shocks and surprises.\n",
        "        3.  **Benchmark Selection**: Calculates the median of the admissible\n",
        "            angles, carefully handling cases where the set of angles \"wraps\n",
        "            around\" the 2-pi circle.\n",
        "        4.  **Shock Construction**: Applies the median rotation to the raw\n",
        "            surprises to generate the benchmark structural shock series.\n",
        "        5.  **Normalization**: Normalizes the structural shocks to have unit\n",
        "            standard deviation and ensures the monetary policy shock is signed\n",
        "            as contractionary (i.e., positively correlated with interest rates).\n",
        "\n",
        "    Args:\n",
        "        surprise_vectors (Dict[str, np.ndarray]): A dictionary where keys are\n",
        "            central bank names and values are (N, 2) NumPy arrays of their\n",
        "            [rate, equity] surprise vectors.\n",
        "        num_angles (int): The number of angles to use in the grid search for\n",
        "                          the rotation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A nested dictionary where top-level keys are\n",
        "        central bank names. Each inner dictionary contains:\n",
        "        - 'structural_shocks': The (N, 2) array of normalized shocks.\n",
        "        - 'median_angle': The benchmark rotation angle (in radians).\n",
        "        - 'admissible_angles': The array of all angles satisfying the restrictions.\n",
        "        - 'admissibility_mask': The boolean mask for the angle grid.\n",
        "\n",
        "    Raises:\n",
        "        IdentificationError: If no admissible rotation angles can be found for\n",
        "                             a given central bank.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Generate the angle grid and rotation matrices once.\n",
        "    theta_grid, rotation_matrices = _generate_rotation_matrices(num_angles)\n",
        "\n",
        "    # Prepare the dictionary to store results for each central bank.\n",
        "    results = {}\n",
        "\n",
        "    # --- Process each central bank's surprises independently ---\n",
        "    for bank, surprises in surprise_vectors.items():\n",
        "        # --- Step 2: Find Admissible Rotations ---\n",
        "        is_admissible = _find_admissible_rotations(surprises, rotation_matrices, theta_grid)\n",
        "\n",
        "        # Extract the angles that satisfy the restrictions.\n",
        "        admissible_angles = theta_grid[is_admissible]\n",
        "\n",
        "        # --- Error Handling: Check if identification was successful ---\n",
        "        if len(admissible_angles) == 0:\n",
        "            raise IdentificationError(f\"Identification failed for {bank}: No admissible rotation angles found.\")\n",
        "\n",
        "        # --- Step 3: Calculate Benchmark Median Rotation (handling circularity) ---\n",
        "        # Sort angles to check for \"wrap-around\" issues on the 0-2pi circle.\n",
        "        sorted_angles = np.sort(admissible_angles)\n",
        "        # A large jump in sorted angles suggests the set wraps around 2pi.\n",
        "        if (np.diff(sorted_angles) > np.pi).any():\n",
        "            # Shift angles < pi by adding 2pi to \"unwrap\" the set.\n",
        "            adjusted_angles = np.where(sorted_angles < np.pi, sorted_angles + 2 * np.pi, sorted_angles)\n",
        "            # Calculate the median on the adjusted set.\n",
        "            median_angle = np.median(adjusted_angles)\n",
        "            # Map the median back to the [0, 2pi] range if it was shifted.\n",
        "            median_angle = median_angle % (2 * np.pi)\n",
        "        else:\n",
        "            # If no wrap-around, the standard median is correct.\n",
        "            median_angle = np.median(admissible_angles)\n",
        "\n",
        "        # --- Step 4: Construct Benchmark Shocks ---\n",
        "        # Create the single benchmark rotation matrix from the median angle.\n",
        "        q_median = np.array([\n",
        "            [np.cos(median_angle), -np.sin(median_angle)],\n",
        "            [np.sin(median_angle), np.cos(median_angle)]\n",
        "        ])\n",
        "\n",
        "        # Apply the rotation to get the raw structural shocks.\n",
        "        raw_structural_shocks = (q_median @ surprises.T).T\n",
        "\n",
        "        # --- Step 5: Normalization and Sign Convention ---\n",
        "        # Ensure the monetary policy shock (column 0) is signed as contractionary.\n",
        "        # It must be positively correlated with the interest rate surprise (column 0).\n",
        "        if np.corrcoef(raw_structural_shocks[:, 0], surprises[:, 0])[0, 1] < 0:\n",
        "            # If the correlation is negative, flip the sign of BOTH structural shocks\n",
        "            # to maintain their orthogonality and relative interpretation.\n",
        "            raw_structural_shocks *= -1\n",
        "\n",
        "        # Normalize both shock series to have a standard deviation of 1.\n",
        "        std_devs = raw_structural_shocks.std(axis=0)\n",
        "        # Avoid division by zero if a shock series has zero variance.\n",
        "        safe_std_devs = np.where(std_devs > 1e-9, std_devs, 1.0)\n",
        "        normalized_shocks = raw_structural_shocks / safe_std_devs\n",
        "\n",
        "        # --- Store Results ---\n",
        "        results[bank] = {\n",
        "            'structural_shocks': normalized_shocks,\n",
        "            'median_angle': median_angle,\n",
        "            'admissible_angles': admissible_angles,\n",
        "            'admissibility_mask': is_admissible\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 6\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase2_task6_shock_identification(\n",
        "    surprise_vectors: Dict[str, np.ndarray],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full rotational decomposition pipeline for Task 6.\n",
        "\n",
        "    This is a simple wrapper around the main `identify_structural_shocks`\n",
        "    function, intended to fit into the broader project pipeline structure. It\n",
        "    retrieves any necessary parameters from the configuration and calls the\n",
        "    core implementation.\n",
        "\n",
        "    Args:\n",
        "        surprise_vectors (Dict[str, np.ndarray]): The dictionary of raw surprise\n",
        "                                                  vectors from Task 5.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: The nested dictionary containing the\n",
        "                                   identified structural shocks and metadata\n",
        "                                   for each central bank.\n",
        "    \"\"\"\n",
        "    # Extract computational settings from the configuration.\n",
        "    num_angles = config.get(\"computation_settings\", {}).get(\"robustness_checks\", {}).get(\"identification_uncertainty_draws\", 999)\n",
        "\n",
        "    # Call the main identification function with the specified parameters.\n",
        "    structural_shock_results = identify_structural_shocks(\n",
        "        surprise_vectors=surprise_vectors,\n",
        "        num_angles=num_angles\n",
        "    )\n",
        "\n",
        "    return structural_shock_results\n"
      ],
      "metadata": {
        "id": "D0jfJ8-Ugr1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Monthly Shock Aggregation\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 7, STEP 1: EVENT-TO-MONTH MAPPING\n",
        "# =============================================================================\n",
        "\n",
        "def map_events_to_months(\n",
        "    announcement_df: pd.DataFrame,\n",
        "    day_of_month_threshold: int = 15\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Maps high-frequency events to their corresponding modeling month.\n",
        "\n",
        "    Purpose:\n",
        "        This function translates the precise timestamp of each announcement into\n",
        "        a standardized monthly bucket for aggregation. It implements a specific\n",
        "        rule to account for information lags, assigning events that occur late\n",
        "        in a month to the subsequent month, with a special exception for December.\n",
        "\n",
        "    Process:\n",
        "        1.  **Input Validation**: Ensures the input DataFrame contains the required\n",
        "            `event_id` and `announcement_timestamp_utc` columns.\n",
        "        2.  **Base Month Calculation**: For each event, it determines the\n",
        "            calendar month in which it occurred and standardizes this to the\n",
        "            first day of that month (e.g., '2022-01-17' -> '2022-01-01').\n",
        "        3.  **Apply Threshold Rule with Edge Case**: It creates a boolean mask to\n",
        "            identify events that should be shifted to the next month. An event\n",
        "            is shifted only if both of the following conditions are true:\n",
        "            a. Its day is after the `day_of_month_threshold` (e.g., > 15).\n",
        "            b. Its month is NOT December.\n",
        "        4.  **Assign Final Month**: Using the mask, it adds a one-month offset\n",
        "            to all identified events. Events not matching the criteria (i.e.,\n",
        "            early-in-month events or any December event) retain their original\n",
        "            base month.\n",
        "\n",
        "    Args:\n",
        "        announcement_df (pd.DataFrame): The clean announcement metadata with an\n",
        "                                        'event_id' index or column and a valid\n",
        "                                        'announcement_timestamp_utc' column.\n",
        "        day_of_month_threshold (int): The day of the month used as the cutoff.\n",
        "                                      Events after this day are pushed to the\n",
        "                                      next month (unless in December).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with 'event_id' as the index and a single\n",
        "                      'assigned_month' column containing the standardized\n",
        "                      monthly timestamp for each event.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Ensure the required timestamp column exists.\n",
        "    if 'announcement_timestamp_utc' not in announcement_df.columns:\n",
        "        raise KeyError(\"Input DataFrame must contain the 'announcement_timestamp_utc' column.\")\n",
        "    # Ensure the event identifier is present.\n",
        "    if 'event_id' not in announcement_df.columns:\n",
        "        raise KeyError(\"Input DataFrame must contain the 'event_id' column.\")\n",
        "\n",
        "    # --- 2. Data Preparation ---\n",
        "    # Work on a copy to avoid modifying the original DataFrame.\n",
        "    df = announcement_df[['event_id', 'announcement_timestamp_utc']].copy()\n",
        "\n",
        "    # Set event_id as the index for clean joins and a well-structured output.\n",
        "    df.set_index('event_id', inplace=True)\n",
        "\n",
        "    # Get the timestamp series for easier access.\n",
        "    timestamps = df['announcement_timestamp_utc']\n",
        "\n",
        "    # Determine the base calendar month for all events, standardized to the first day.\n",
        "    # Using .dt.to_period('M').dt.to_timestamp() is a robust way to achieve this.\n",
        "    base_month = timestamps.dt.to_period('M').dt.to_timestamp()\n",
        "\n",
        "    # --- 3. Apply Threshold Rule with December Edge Case ---\n",
        "    # Condition A: The event occurs after the specified day-of-month threshold.\n",
        "    is_late_in_month: pd.Series = timestamps.dt.day > day_of_month_threshold\n",
        "\n",
        "    # Condition B: The event's month is not December (month == 12).\n",
        "    is_not_december: pd.Series = timestamps.dt.month != 12\n",
        "\n",
        "    # Combine the conditions: an event is shifted only if it is both late in the month AND not in December.\n",
        "    mask_for_shift: pd.Series = is_late_in_month & is_not_december\n",
        "\n",
        "    # --- 4. Assign Final Month ---\n",
        "    # Initialize the assigned_month series with the base month values.\n",
        "    assigned_month = base_month.copy()\n",
        "\n",
        "    # For the events identified by the mask, add a one-month offset.\n",
        "    # pd.DateOffset is the correct tool for robust date arithmetic that handles year-ends correctly.\n",
        "    assigned_month.loc[mask_for_shift] = base_month.loc[mask_for_shift] + pd.DateOffset(months=1)\n",
        "\n",
        "    # Return the final mapping as a DataFrame.\n",
        "    return assigned_month.to_frame(name='assigned_month')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 7, STEP 2: WITHIN-MONTH SHOCK AGGREGATION\n",
        "# =============================================================================\n",
        "\n",
        "def aggregate_shocks_to_monthly(\n",
        "    structural_shocks: Dict[str, pd.DataFrame],\n",
        "    event_to_month_map: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates event-level structural shocks into a monthly time series.\n",
        "\n",
        "    Purpose:\n",
        "        This function robustly converts the high-frequency, event-level structural\n",
        "        shocks into the monthly frequency required for macroeconomic models like\n",
        "        BVARs. It ensures perfect alignment between shocks and their assigned\n",
        "        aggregation month by using an explicit `event_id` index.\n",
        "\n",
        "    Process:\n",
        "        1.  **Input Validation**: Verifies that the input `structural_shocks` is a\n",
        "            dictionary of DataFrames, each indexed by `event_id`.\n",
        "        2.  **Combine Shocks**: Concatenates the shock DataFrames for all central\n",
        "            banks into a single DataFrame, preserving the `event_id` index. This\n",
        "            creates a unified dataset of all identified shocks.\n",
        "        3.  **Join Mapping**: Merges the combined shocks with the event-to-month\n",
        "            mapping DataFrame using their shared and explicit `event_id` index.\n",
        "            This step is critical for ensuring each shock is assigned to the\n",
        "            correct month without relying on fragile row ordering.\n",
        "        4.  **Aggregate**: Groups the merged data by the `assigned_month` column\n",
        "            and then sums the shock values within each monthly bucket.\n",
        "            Equation: `shock_m = sum(shock_t for t in month m)`\n",
        "        5.  **Densify**: Creates a complete, uninterrupted monthly date range that\n",
        "            spans the entire sample period. It then reindexes the aggregated\n",
        "            shock data to this full range, filling any months that had no\n",
        "            announcements with a value of 0.0, as is appropriate for a shock series.\n",
        "\n",
        "    Args:\n",
        "        structural_shocks (Dict[str, pd.DataFrame]): The output from the shock\n",
        "            identification task. This must be a dictionary where keys are\n",
        "            central bank names and values are DataFrames containing the\n",
        "            structural shocks, indexed by `event_id`.\n",
        "        event_to_month_map (pd.DataFrame): The output from `map_events_to_months`,\n",
        "                                           a DataFrame with an `event_id` index\n",
        "                                           and an 'assigned_month' column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by a complete monthly DatetimeIndex,\n",
        "                      with columns for each aggregated shock series (e.g.,\n",
        "                      'shock_FED_MP', 'shock_ECB_INFO').\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Check that the primary input is a dictionary.\n",
        "    if not isinstance(structural_shocks, dict):\n",
        "        raise TypeError(\"Input 'structural_shocks' must be a dictionary of DataFrames.\")\n",
        "    # Check that the DataFrames inside the dictionary are indexed correctly.\n",
        "    for bank, df in structural_shocks.items():\n",
        "        if not isinstance(df.index, pd.Index) or df.index.name != 'event_id':\n",
        "             raise ValueError(f\"DataFrame for bank '{bank}' must be indexed by 'event_id'.\")\n",
        "    # Check that the month map is indexed correctly.\n",
        "    if not isinstance(event_to_month_map.index, pd.Index) or event_to_month_map.index.name != 'event_id':\n",
        "        raise ValueError(\"Input 'event_to_month_map' must be indexed by 'event_id'.\")\n",
        "\n",
        "    # --- 2. Combine Shocks into a Single DataFrame ---\n",
        "    # Concatenate all shock DataFrames horizontally. The join is implicitly on the\n",
        "    # event_id index. This is robust and efficient.\n",
        "    try:\n",
        "        event_level_shocks = pd.concat(structural_shocks.values(), axis=1)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to concatenate shock DataFrames. Ensure they have unique columns and a consistent index. Error: {e}\")\n",
        "\n",
        "    # --- 3. Join with Month Mapping ---\n",
        "    # Join the shock data with the month mapping on their shared, explicit event_id index.\n",
        "    # This is the core step that remedies the flaw of the previous implementation.\n",
        "    df_to_agg = event_to_month_map.join(event_level_shocks, how='left')\n",
        "\n",
        "    # --- 4. Aggregate by Summation ---\n",
        "    # Group the DataFrame by the 'assigned_month' column and sum all shock values\n",
        "    # that fall into that monthly bucket.\n",
        "    monthly_aggregated = df_to_agg.groupby('assigned_month').sum()\n",
        "\n",
        "    # --- 5. Densify the Time Series ---\n",
        "    # Determine the full date range required for the final time series.\n",
        "    start_date = event_to_month_map['assigned_month'].min()\n",
        "    end_date = event_to_month_map['assigned_month'].max()\n",
        "\n",
        "    # Create a complete monthly date range with 'MS' (Month Start) frequency.\n",
        "    full_date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
        "\n",
        "    # Reindex the aggregated data to this full range. Any months that did not have\n",
        "    # announcements (and thus are missing from `monthly_aggregated`) will be\n",
        "    # filled with 0.0, which is the correct representation for a shock series.\n",
        "    monthly_shocks_final = monthly_aggregated.reindex(full_date_range, fill_value=0.0)\n",
        "\n",
        "    # Return the final, dense, monthly shock DataFrame.\n",
        "    return monthly_shocks_final\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 7, STEP 3: SHOCK SERIES STATISTICAL VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "def validate_monthly_shock_series(\n",
        "    monthly_shocks_df: pd.DataFrame,\n",
        "    ljung_box_lags: List[int] = [1, 6, 12]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs statistical validation on the aggregated monthly shock series.\n",
        "\n",
        "    Purpose:\n",
        "        This function provides a quantitative quality check on the final shock\n",
        "        series. It verifies key statistical properties that are expected of a\n",
        "        well-identified, exogenous shock series, such as a zero mean and a lack\n",
        "        of significant autocorrelation.\n",
        "\n",
        "    Process (for each shock series):\n",
        "        1.  **Descriptive Stats**: Calculates the first four moments (mean,\n",
        "            standard deviation, skewness, kurtosis).\n",
        "        2.  **Zero-Mean Test**: Performs a one-sample t-test to check if the\n",
        "            mean of the series is statistically different from zero.\n",
        "        3.  **Autocorrelation Test**: Conducts a Ljung-Box test for serial\n",
        "            correlation at multiple specified lags.\n",
        "\n",
        "    Args:\n",
        "        monthly_shocks_df (pd.DataFrame): The final monthly shock series DataFrame.\n",
        "        ljung_box_lags (List[int]): A list of lags to use for the Ljung-Box test.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame report where each row corresponds to a shock\n",
        "                      series and columns contain the results of the statistical tests.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    validation_results = []\n",
        "\n",
        "    # --- Iterate and Validate Each Shock Series ---\n",
        "    for shock_name in monthly_shocks_df.columns:\n",
        "        series = monthly_shocks_df[shock_name]\n",
        "\n",
        "        # Basic descriptive statistics\n",
        "        results = {\n",
        "            'shock_series': shock_name,\n",
        "            'mean': series.mean(),\n",
        "            'std_dev': series.std(),\n",
        "            'skewness': series.skew(),\n",
        "            'kurtosis': series.kurtosis()\n",
        "        }\n",
        "\n",
        "        # T-test for H0: mean = 0\n",
        "        # A high p-value is desired, indicating we cannot reject the zero-mean hypothesis.\n",
        "        t_stat, p_val_mean = stats.ttest_1samp(series, 0.0, nan_policy='omit')\n",
        "        results['t_test_p_value'] = p_val_mean\n",
        "\n",
        "        # Ljung-Box test for H0: no autocorrelation\n",
        "        # High p-values are desired, indicating no significant serial correlation.\n",
        "        try:\n",
        "            # Use the pandas wrapper for a cleaner output format.\n",
        "            lb_results = acorr_ljungbox(series, lags=ljung_box_lags, return_df=True)\n",
        "            for lag in ljung_box_lags:\n",
        "                results[f'lb_p_value_lag_{lag}'] = lb_results.loc[lag, 'lb_pvalue']\n",
        "        except Exception:\n",
        "            # Handle cases where the test fails (e.g., zero variance series).\n",
        "            for lag in ljung_box_lags:\n",
        "                results[f'lb_p_value_lag_{lag}'] = np.nan\n",
        "\n",
        "        validation_results.append(results)\n",
        "\n",
        "    return pd.DataFrame(validation_results).set_index('shock_series')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 7\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase3_task7_temporal_aggregation(\n",
        "    structural_shock_results: Dict[str, Dict[str, Any]],\n",
        "    clean_announcement_df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full temporal aggregation pipeline for Task 7.\n",
        "\n",
        "    This master function transforms the event-level structural shocks into a\n",
        "    validated, monthly time series ready for econometric modeling.\n",
        "\n",
        "    Args:\n",
        "        structural_shock_results (Dict[str, Dict[str, Any]]): The output from\n",
        "            Task 6, containing the identified structural shocks.\n",
        "        clean_announcement_df (pd.DataFrame): The cleansed announcement metadata.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing the final monthly\n",
        "                                 shock series and its statistical validation report.\n",
        "    \"\"\"\n",
        "    # Extract the shock arrays and corresponding event_ids.\n",
        "    # This requires careful reconstruction from the Task 6 output and the announcement df.\n",
        "    shocks_to_aggregate = {\n",
        "        bank: results['structural_shocks']\n",
        "        for bank, results in structural_shock_results.items()\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Map Events to Months ---\n",
        "    event_month_map = map_events_to_months(\n",
        "        announcement_df=clean_announcement_df\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Aggregate Shocks to Monthly Frequency ---\n",
        "    monthly_shocks = aggregate_shocks_to_monthly(\n",
        "        structural_shocks=shocks_to_aggregate,\n",
        "        event_to_month_map=event_month_map,\n",
        "        announcement_df=clean_announcement_df\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Perform Statistical Validation ---\n",
        "    validation_report = validate_monthly_shock_series(\n",
        "        monthly_shocks_df=monthly_shocks\n",
        "    )\n",
        "\n",
        "    # --- Aggregate and return all results ---\n",
        "    results = {\n",
        "        \"monthly_shocks_df\": monthly_shocks,\n",
        "        \"validation_report\": validation_report\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "FZ96oALth7EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Macroeconomic Data Transformation and Variable Construction\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 8, STEP 1: CORE VARIABLE TRANSFORMATION\n",
        "# =============================================================================\n",
        "\n",
        "def transform_core_macro_variables(\n",
        "    prepared_macro_df: pd.DataFrame,\n",
        "    transformation_map: Dict[str, str],\n",
        "    target_market: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies specified transformations and computes required cross-rates.\n",
        "\n",
        "    Purpose:\n",
        "        This function converts the prepared (interpolated, wide-format) macro\n",
        "        data into the final form required for econometric modeling. It handles\n",
        "        both direct transformations (e.g., log) and derived transformations\n",
        "        like currency cross-rates.\n",
        "\n",
        "    Process:\n",
        "        1.  **Direct Transformations**: It iterates through a mapping dictionary\n",
        "            to apply specified transformations ('log' or 'level') to each core\n",
        "            variable. It includes validation to ensure positivity for log transforms.\n",
        "            Equation: `y_t = 100 * ln(x_t)`\n",
        "        2.  **Cross-Rate Calculation**: It explicitly checks for the presence of\n",
        "            the necessary base exchange rate series (e.g., vs. USD for both\n",
        "            Canada and Euro Area) needed to compute the CAD/EUR cross-rate.\n",
        "        3.  **Cross-Rate Formula**: If the base series are present, it calculates\n",
        "            the cross-rate according to standard financial arithmetic.\n",
        "            Equation: `CAD/EUR = (USD/EUR) / (USD/CAD)`\n",
        "        4.  **Cross-Rate Transformation**: It then applies the standard log\n",
        "            transformation to the newly calculated cross-rate series.\n",
        "        5.  **Standardize Naming**: It renames all output columns to a consistent\n",
        "            convention, e.g., `CAN_gdp_log`, `CAN_exchange_rate_vs_EUR_log`.\n",
        "\n",
        "    Args:\n",
        "        prepared_macro_df (pd.DataFrame): A wide-format DataFrame of macro\n",
        "                                          series, indexed by date. This data\n",
        "                                          should already be cleaned and\n",
        "                                          interpolated.\n",
        "        transformation_map (Dict[str, str]): A dictionary mapping column names\n",
        "                                             in `prepared_macro_df` to the\n",
        "                                             desired transformation ('log' or 'level').\n",
        "        target_market (str): The country/market code (e.g., 'CAN'), used for\n",
        "                             standardizing output column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the same index but containing the\n",
        "                      transformed and newly created variables.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input DataFrame does not have a DatetimeIndex.\n",
        "        KeyError: If a variable in the transformation map or a required base\n",
        "                  exchange rate is not found in the input DataFrame.\n",
        "        ValueError: If a series marked for log-transformation contains non-positive values.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Ensure the input is a time-series DataFrame.\n",
        "    if not isinstance(prepared_macro_df.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Input DataFrame must have a DatetimeIndex.\")\n",
        "\n",
        "    # --- 2. Direct Transformations ---\n",
        "    # Create a dictionary to hold the final, transformed columns.\n",
        "    transformed_cols: Dict[str, pd.Series] = {}\n",
        "\n",
        "    # Iterate through the provided transformation map.\n",
        "    for var_name, transform_type in transformation_map.items():\n",
        "        # Ensure the variable exists in the input DataFrame.\n",
        "        if var_name not in prepared_macro_df.columns:\n",
        "            raise KeyError(f\"Variable '{var_name}' from transformation map not found in DataFrame.\")\n",
        "\n",
        "        series = prepared_macro_df[var_name]\n",
        "\n",
        "        # Generate the new, standardized column name.\n",
        "        # Remove any country prefix from the original name to avoid duplication.\n",
        "        clean_var_name = var_name.replace(f\"{target_market}_\", \"\")\n",
        "        new_name = f\"{target_market}_{clean_var_name}\"\n",
        "\n",
        "        if transform_type == 'log':\n",
        "            # Validate that all values are positive before taking the log.\n",
        "            if (series <= 0).any():\n",
        "                raise ValueError(f\"Series '{var_name}' contains non-positive values and cannot be log-transformed.\")\n",
        "            # Equation: y_t = 100 * ln(x_t)\n",
        "            transformed_cols[new_name + '_log'] = 100 * np.log(series)\n",
        "        elif transform_type == 'level':\n",
        "            # Keep the series in its original level form.\n",
        "            transformed_cols[new_name] = series\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown transformation type '{transform_type}' for variable '{var_name}'.\")\n",
        "\n",
        "    # --- 3. Cross-Rate Calculation (CAD/EUR) ---\n",
        "    # Define the required base series for the cross-rate calculation.\n",
        "    # Assuming the convention is USD per foreign currency (e.g., USD/CAD).\n",
        "    cad_vs_usd_col = f\"{target_market}_exchange_rate_vs_USD\"\n",
        "    eur_vs_usd_col = \"EUR_exchange_rate_vs_USD\" # Assuming a standard name for the Euro Area series\n",
        "\n",
        "    # Check if both required base series are present in the original data.\n",
        "    if cad_vs_usd_col in prepared_macro_df.columns and eur_vs_usd_col in prepared_macro_df.columns:\n",
        "        # Extract the base series.\n",
        "        usd_cad_series = prepared_macro_df[cad_vs_usd_col]\n",
        "        usd_eur_series = prepared_macro_df[eur_vs_usd_col]\n",
        "\n",
        "        # Equation: CAD/EUR = (USD/EUR) / (USD/CAD)\n",
        "        cad_eur_series = usd_eur_series / usd_cad_series\n",
        "\n",
        "        # Validate the new series for positivity before log-transformation.\n",
        "        if (cad_eur_series <= 0).any():\n",
        "            raise ValueError(\"Calculated CAD/EUR cross-rate contains non-positive values.\")\n",
        "\n",
        "        # Transform and add the new cross-rate to the results dictionary.\n",
        "        new_cross_rate_name = f\"{target_market}_exchange_rate_vs_EUR_log\"\n",
        "        transformed_cols[new_cross_rate_name] = 100 * np.log(cad_eur_series)\n",
        "\n",
        "    # --- 4. Assemble Final DataFrame ---\n",
        "    # Create the final DataFrame from the dictionary of transformed series.\n",
        "    df_final = pd.DataFrame(transformed_cols)\n",
        "\n",
        "    # Ensure the column order is predictable for downstream tasks.\n",
        "    return df_final.sort_index(axis=1)\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 8, STEP 2: CONTROL VARIABLE CONSTRUCTION\n",
        "# =============================================================================\n",
        "\n",
        "def construct_control_variables(\n",
        "    time_index: pd.DatetimeIndex,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs deterministic control variables for the model.\n",
        "\n",
        "    Purpose:\n",
        "        This function generates standard control variables, such as a time\n",
        "        trend and dummy variables for specific periods, that are required for\n",
        "        the BVAR and Local Projection models.\n",
        "\n",
        "    Process:\n",
        "        1.  **Linear Trend**: Creates a linear time trend series that starts at\n",
        "            1 and increments by 1 for each period in the provided `time_index`.\n",
        "        2.  **COVID-19 Dummy**: Reads the start and end dates for the COVID-19\n",
        "            period from the configuration dictionary. It then creates a binary\n",
        "            dummy variable that is 1 during this period (inclusive) and 0\n",
        "            otherwise.\n",
        "\n",
        "    Args:\n",
        "        time_index (pd.DatetimeIndex): The monthly DatetimeIndex from the\n",
        "                                       macroeconomic dataset, which defines the\n",
        "                                       sample period.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary, used\n",
        "                                 to get the COVID period definition.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the same `time_index`, containing\n",
        "                      columns for each generated control variable.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(time_index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Input 'time_index' must be a pandas DatetimeIndex.\")\n",
        "\n",
        "    # --- Initialization ---\n",
        "    num_observations = len(time_index)\n",
        "    controls_df = pd.DataFrame(index=time_index)\n",
        "\n",
        "    # --- 1. Construct Linear Time Trend ---\n",
        "    # The trend starts at 1 and increments for each observation.\n",
        "    controls_df['trend'] = np.arange(1, num_observations + 1)\n",
        "\n",
        "    # --- 2. Construct COVID-19 Dummy Variable ---\n",
        "    # Extract the period definition from the configuration.\n",
        "    try:\n",
        "        covid_period = config['bvar_specification']['covid_period']\n",
        "        start_date = pd.to_datetime(covid_period['start_date'])\n",
        "        end_date = pd.to_datetime(covid_period['end_date'])\n",
        "    except KeyError as e:\n",
        "        raise KeyError(f\"Could not find COVID period definition in config: {e}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not parse COVID period dates from config: {e}\")\n",
        "\n",
        "    # Create a boolean mask for the date range.\n",
        "    covid_mask = (time_index >= start_date) & (time_index <= end_date)\n",
        "\n",
        "    # Convert the boolean mask to an integer (0 or 1) dummy variable.\n",
        "    controls_df['covid_dummy'] = covid_mask.astype(int)\n",
        "\n",
        "    return controls_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 8, STEP 3: FINAL DATASET ASSEMBLY\n",
        "# =============================================================================\n",
        "\n",
        "def assemble_final_dataset(\n",
        "    transformed_macro_df: pd.DataFrame,\n",
        "    monthly_shocks_df: pd.DataFrame,\n",
        "    control_variables_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Assembles the final, analysis-ready dataset for econometric modeling.\n",
        "\n",
        "    Purpose:\n",
        "        This function serves as the final step in the data preparation phase.\n",
        "        It merges the transformed endogenous macroeconomic variables, the\n",
        "        aggregated monthly shock series, and the deterministic control\n",
        "        variables into a single, unified, and clean DataFrame. It follows a\n",
        "        robust procedure to handle any minor misalignments in the time indices\n",
        "        of the input datasets.\n",
        "\n",
        "    Process:\n",
        "        1.  **Outer Join**: It first performs an outer join on all three input\n",
        "            DataFrames. This creates a superset of all dates present in any\n",
        "            input, explicitly showing any misalignments as `NaN` values.\n",
        "        2.  **Limited Forward Fill**: It then applies a conservative forward-fill\n",
        "            with a limit of 1. This is designed to impute single, sporadic\n",
        "            missing observations, which might arise from minor data reporting\n",
        "            lags, without filling large, structural gaps in the data.\n",
        "        3.  **Drop Remaining NaNs**: Finally, it drops any rows that still\n",
        "            contain `NaN` values. This step effectively truncates the sample to\n",
        "            the contiguous time period where all variables are present and valid,\n",
        "            ensuring a clean, balanced panel for modeling.\n",
        "        4.  **Validation**: It performs a final check to assert that the\n",
        "            resulting DataFrame is completely free of any missing values before\n",
        "            returning it.\n",
        "\n",
        "    Args:\n",
        "        transformed_macro_df (pd.DataFrame): Transformed endogenous variables,\n",
        "                                             indexed by date.\n",
        "        monthly_shocks_df (pd.DataFrame): Aggregated monthly shock series,\n",
        "                                          indexed by date.\n",
        "        control_variables_df (pd.DataFrame): Deterministic control variables,\n",
        "                                             indexed by date.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The complete, analysis-ready dataset with a consistent\n",
        "                      and complete monthly DatetimeIndex.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the final assembled dataset still contains missing values\n",
        "                    after all processing steps, indicating a more severe data\n",
        "                    problem than the function is designed to handle.\n",
        "    \"\"\"\n",
        "    # --- 1. Input Validation ---\n",
        "    # Check that all inputs are DataFrames with a DatetimeIndex.\n",
        "    for df_name, df in zip(['transformed_macro_df', 'monthly_shocks_df', 'control_variables_df'],\n",
        "                           [transformed_macro_df, monthly_shocks_df, control_variables_df]):\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(f\"Input '{df_name}' must be a pandas DataFrame.\")\n",
        "        if not isinstance(df.index, pd.DatetimeIndex):\n",
        "            raise TypeError(f\"Input '{df_name}' must have a DatetimeIndex.\")\n",
        "\n",
        "    # --- 2. Outer Join to Combine All Data Sources ---\n",
        "    # Start with the macro data as the base.\n",
        "    combined_df = transformed_macro_df\n",
        "\n",
        "    # Join the shocks using an outer join to retain all dates from both datasets.\n",
        "    # This makes any date misalignments explicit as rows/columns with NaNs.\n",
        "    combined_df = combined_df.join(monthly_shocks_df, how='outer')\n",
        "\n",
        "    # Join the control variables using an outer join.\n",
        "    combined_df = combined_df.join(control_variables_df, how='outer')\n",
        "\n",
        "    # Sort the index to ensure chronological order before filling.\n",
        "    combined_df.sort_index(inplace=True)\n",
        "\n",
        "    # --- 3. Apply Limited Forward Fill ---\n",
        "    # This step handles minor, isolated missing values (e.g., a single month lag\n",
        "    # in a data release) without imputing data across large structural gaps.\n",
        "    # The `limit=1` is a critical parameter for this conservative approach.\n",
        "    filled_df = combined_df.fillna(method='ffill', limit=1)\n",
        "\n",
        "    # --- 4. Drop Remaining NaNs to Get Final Sample ---\n",
        "    # After the limited fill, any remaining NaNs represent either gaps larger\n",
        "    # than one month or NaNs at the beginning of a series' history.\n",
        "    # Dropping these rows truncates the dataset to the final, clean, balanced\n",
        "    # sample period where all data is present.\n",
        "    final_df = filled_df.dropna()\n",
        "\n",
        "    # --- 5. Final Validation ---\n",
        "    # This is a critical assertion to guarantee the output is clean.\n",
        "    # If this check fails, it indicates a deeper problem in the upstream data\n",
        "    # that the standard cleaning procedure could not resolve.\n",
        "    if final_df.isna().sum().sum() > 0:\n",
        "        missing_info = final_df.isna().sum()\n",
        "        raise ValueError(\n",
        "            \"Final dataset contains unexpected missing values after assembly and imputation. \"\n",
        "            f\"Problematic columns:\\n{missing_info[missing_info > 0]}\"\n",
        "        )\n",
        "\n",
        "    # Ensure the data types are numeric, as joins can sometimes change them.\n",
        "    final_df = final_df.astype(float)\n",
        "\n",
        "    # Return the final, analysis-ready DataFrame.\n",
        "    return final_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 8\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase3_task8_final_data_assembly(\n",
        "    prepared_macro_df: pd.DataFrame,\n",
        "    monthly_shocks_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full data transformation and assembly pipeline for Task 8.\n",
        "\n",
        "    This master function takes the prepared macro data and monthly shocks,\n",
        "    generates the necessary control variables, applies final transformations,\n",
        "    and assembles the complete dataset ready for modeling.\n",
        "\n",
        "    Args:\n",
        "        prepared_macro_df (pd.DataFrame): The wide-format, interpolated macro\n",
        "                                          data from the preprocessing phase.\n",
        "        monthly_shocks_df (pd.DataFrame): The aggregated monthly shock series\n",
        "                                          from Task 7.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final, complete, analysis-ready dataset.\n",
        "    \"\"\"\n",
        "    # Define the transformation map based on economic variable types.\n",
        "    # This should ideally be part of the configuration itself.\n",
        "    transformation_map = {\n",
        "        'CAN_exchange_rate': 'log',\n",
        "        'CAN_policy_rate': 'level',\n",
        "        'CAN_cpi': 'log',\n",
        "        'CAN_equity_index': 'log',\n",
        "        'CAN_gdp': 'log'\n",
        "    }\n",
        "\n",
        "    # --- Step 1: Apply Core Variable Transformations ---\n",
        "    transformed_macro = transform_core_macro_variables(\n",
        "        prepared_macro_df=prepared_macro_df,\n",
        "        transformation_map=transformation_map\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Construct Control Variables ---\n",
        "    controls = construct_control_variables(\n",
        "        time_index=transformed_macro.index,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Assemble the Final Dataset ---\n",
        "    analysis_ready_df = assemble_final_dataset(\n",
        "        transformed_macro_df=transformed_macro,\n",
        "        monthly_shocks_df=monthly_shocks_df,\n",
        "        control_variables_df=controls\n",
        "    )\n",
        "\n",
        "    return analysis_ready_df\n"
      ],
      "metadata": {
        "id": "PsWSvo7jjbJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: BVAR Model Specification and Prior Setup\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 9, STEP 1: ENDOGENOUS & EXOGENOUS MATRIX CONSTRUCTION\n",
        "# =============================================================================\n",
        "\n",
        "def create_var_matrices(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    endogenous_vars: List[str],\n",
        "    exogenous_vars: List[str],\n",
        "    lags: int\n",
        ") -> Tuple[np.ndarray, np.ndarray, pd.DatetimeIndex]:\n",
        "    \"\"\"\n",
        "    Constructs the Y and X matrices for VAR model estimation.\n",
        "\n",
        "    Purpose:\n",
        "        This function transforms a time-series DataFrame into the standard matrix\n",
        "        format required for regression analysis (Y = X*B + E). It systematically\n",
        "        creates lagged variables and aligns the endogenous (Y) and exogenous (X)\n",
        "        matrices.\n",
        "\n",
        "    Process:\n",
        "        1.  **Lag Creation**: For each endogenous variable, it creates `p` lagged\n",
        "            versions, naming them systematically (e.g., 'VAR_L1').\n",
        "        2.  **Regressor Assembly**: It combines the lagged endogenous variables,\n",
        "            the specified exogenous variables, and a constant term into a single\n",
        "            regressor DataFrame.\n",
        "        3.  **Alignment**: It selects the non-lagged endogenous variables as the\n",
        "            Y matrix.\n",
        "        4.  **Handling NaNs**: It drops all rows containing any NaN values, which\n",
        "            naturally arise from the lagging process. This ensures that Y and X\n",
        "            are perfectly aligned and have the same number of observations (T).\n",
        "        5.  **Conversion**: Converts the final DataFrames to NumPy arrays for\n",
        "            efficient numerical computation.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset from Task 8.\n",
        "        endogenous_vars (List[str]): An ordered list of column names for the\n",
        "                                     endogenous variables.\n",
        "        exogenous_vars (List[str]): A list of column names for the exogenous variables.\n",
        "        lags (int): The number of lags (p) to include in the VAR model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray, pd.DatetimeIndex]:\n",
        "        - Y: The (T x K) matrix of endogenous variables.\n",
        "        - X: The (T x M) matrix of regressors (lags, exog, constant).\n",
        "        - valid_index: The DatetimeIndex corresponding to the valid (T) observations.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if lags < 1:\n",
        "        raise ValueError(\"Number of lags must be a positive integer.\")\n",
        "\n",
        "    # --- Lag Creation ---\n",
        "    df = analysis_ready_df.copy()\n",
        "    lagged_vars = []\n",
        "    for var in endogenous_vars:\n",
        "        for i in range(1, lags + 1):\n",
        "            lag_col_name = f\"{var}_L{i}\"\n",
        "            df[lag_col_name] = df[var].shift(i)\n",
        "            lagged_vars.append(lag_col_name)\n",
        "\n",
        "    # --- Regressor Assembly ---\n",
        "    # The order of regressors is: K*p lags, then exogenous vars, then a constant.\n",
        "    regressor_cols = lagged_vars + exogenous_vars\n",
        "    df_with_lags = add_constant(df, prepend=False) # Adds a 'const' column\n",
        "    regressor_cols.append('const')\n",
        "\n",
        "    # --- Alignment and NaN Handling ---\n",
        "    # Drop rows with NaNs created by the shift() operation.\n",
        "    df_clean = df_with_lags.dropna()\n",
        "\n",
        "    # --- Matrix Conversion ---\n",
        "    # Extract the Y and X matrices as NumPy arrays.\n",
        "    Y = df_clean[endogenous_vars].values\n",
        "    X = df_clean[regressor_cols].values\n",
        "\n",
        "    # Extract the valid time index for plotting and reference.\n",
        "    valid_index = df_clean.index\n",
        "\n",
        "    return Y, X, valid_index\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 9, STEPS 2 & 3: NORMAL-WISHART PRIOR IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def construct_minnesota_prior(\n",
        "    Y: np.ndarray,\n",
        "    X: np.ndarray,\n",
        "    endogenous_vars: List[str],\n",
        "    lags: int,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs the parameters for a Minnesota-style Normal-Wishart prior.\n",
        "\n",
        "    Purpose:\n",
        "        This function translates the high-level hyperparameters from the study\n",
        "        configuration into the precise prior mean vector (`beta_0`) and prior\n",
        "        covariance matrix (`V_0`) for the VAR coefficients, as well as the\n",
        "        parameters for the Inverse-Wishart prior on the error covariance.\n",
        "\n",
        "    Process:\n",
        "        1.  **Hyperparameter Extraction**: Retrieves tightness parameters\n",
        "            (`lambda_1`, `lambda_3`) and the AR(1) coefficient prior from the config.\n",
        "        2.  **Estimate Scaling Variances**: For each endogenous variable, it\n",
        "            estimates a univariate AR(1) model to obtain the residual variance\n",
        "            (sigma_i^2), which is used to scale the prior for cross-variable lags.\n",
        "        3.  **Construct Prior Mean (`beta_0`)**: Creates a vector where the prior\n",
        "            mean is `ar_coeff_prior` (e.g., 0.8) for the first own lag of each\n",
        "            variable, and 0 for all other coefficients.\n",
        "        4.  **Construct Prior Covariance (`V_0`)**: Creates a diagonal matrix.\n",
        "            The diagonal elements (variances) are calculated using the Minnesota\n",
        "            formulas, shrinking coefficients on higher-order lags and cross-\n",
        "            variable lags more aggressively.\n",
        "        5.  **Construct Wishart Prior**: Sets the degrees of freedom (`nu_0`) and\n",
        "            scale matrix (`S_0`) for the Inverse-Wishart prior on the error\n",
        "            covariance matrix, typically to uninformative but proper values.\n",
        "\n",
        "    Args:\n",
        "        Y (np.ndarray): The (T x K) matrix of endogenous variables.\n",
        "        X (np.ndarray): The (T x M) matrix of regressors.\n",
        "        endogenous_vars (List[str]): Ordered list of endogenous variable names.\n",
        "        lags (int): The number of lags (p) in the model.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary containing all prior components:\n",
        "        'beta_0', 'V_0', 'nu_0', 'S_0'.\n",
        "    \"\"\"\n",
        "    # --- Hyperparameter Extraction ---\n",
        "    priors_config = config['bvar_priors']['hyperparameters']\n",
        "    lambda_1 = priors_config['lambda_1_overall_tightness']\n",
        "    lambda_3 = priors_config['lambda_3_lag_decay']\n",
        "    ar_coeff = priors_config['ar_coeff_prior']\n",
        "\n",
        "    K = Y.shape[1]  # Number of endogenous variables\n",
        "    M = X.shape[1]  # Total number of regressors\n",
        "    num_exog_vars = M - (K * lags) - 1 # Exclude lags and constant\n",
        "\n",
        "    # --- Estimate Scaling Variances (sigma_i^2) ---\n",
        "    sigma_sq = np.zeros(K)\n",
        "    for i in range(K):\n",
        "        y_i = Y[:, i]\n",
        "        X_ar1 = add_constant(pd.Series(y_i).shift(1).bfill())\n",
        "        model = OLS(y_i[1:], X_ar1[1:]).fit()\n",
        "        sigma_sq[i] = model.ssr / model.df_resid\n",
        "\n",
        "    # --- Construct Prior Mean (beta_0) ---\n",
        "    # beta_0 is a flattened vector of size M*K\n",
        "    beta_0 = np.zeros((M, K))\n",
        "    # Set the prior mean for the first own lag of each variable.\n",
        "    for i in range(K):\n",
        "        # The index of the first own lag for variable i is i.\n",
        "        beta_0[i, i] = ar_coeff\n",
        "    beta_0 = beta_0.flatten(order='F') # Flatten column-wise\n",
        "\n",
        "    # --- Construct Prior Covariance (V_0) ---\n",
        "    # V_0 is a diagonal matrix, so we only need to construct its diagonal.\n",
        "    V_0_diag = np.zeros(M * K)\n",
        "\n",
        "    # Iterate through each regressor (column of X)\n",
        "    for m in range(M):\n",
        "        # Iterate through each equation (column of Y)\n",
        "        for k in range(K):\n",
        "            idx = k * M + m # Index in the flattened V_0_diag\n",
        "\n",
        "            # Check if the regressor is a lagged endogenous variable\n",
        "            if m < K * lags:\n",
        "                lag = (m // K) + 1\n",
        "                lagged_var_idx = m % K\n",
        "\n",
        "                # Prior variance for endogenous lags\n",
        "                variance = (lambda_1 / (lag ** lambda_3))**2\n",
        "                if lagged_var_idx != k: # Cross-lag\n",
        "                    variance *= (sigma_sq[k] / sigma_sq[lagged_var_idx])\n",
        "                V_0_diag[idx] = variance\n",
        "\n",
        "            # Prior variance for exogenous variables (excluding constant)\n",
        "            elif m < M - 1:\n",
        "                V_0_diag[idx] = 100**2 # Diffuse prior\n",
        "\n",
        "            # Prior variance for the constant\n",
        "            else:\n",
        "                V_0_diag[idx] = 1e12 # Very diffuse prior\n",
        "\n",
        "    V_0 = np.diag(V_0_diag)\n",
        "\n",
        "    # --- Construct Inverse-Wishart Prior Parameters ---\n",
        "    # nu_0: Prior degrees of freedom\n",
        "    nu_0 = K + 2.0\n",
        "    # S_0: Prior scale matrix\n",
        "    S_0 = np.diag([s * (nu_0 - K - 1) for s in sigma_sq])\n",
        "\n",
        "    return {\n",
        "        'beta_0': beta_0,\n",
        "        'V_0': V_0,\n",
        "        'nu_0': np.array([[nu_0]]), # Ensure it's an array\n",
        "        'S_0': S_0\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 9\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase4_task9_bvar_setup(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full BVAR model setup pipeline for Task 9.\n",
        "\n",
        "    This master function prepares all the necessary components for the Gibbs\n",
        "    sampler: it constructs the data matrices (Y and X) and the detailed prior\n",
        "    matrices (beta_0, V_0, nu_0, S_0) based on the study's configuration.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset from Task 8.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all setup components:\n",
        "                        'Y', 'X', 'valid_index', and 'priors'.\n",
        "    \"\"\"\n",
        "    # --- Extract specifications from config ---\n",
        "    spec_config = config['bvar_specification']\n",
        "    endogenous_vars = spec_config['endogenous_variables']\n",
        "    # Exogenous variables for the VAR are shocks + deterministic terms\n",
        "    exogenous_vars = [\n",
        "        var for var in spec_config['exogenous_variables']\n",
        "        if 'shock' in var or 'trend' in var or 'dummy' in var\n",
        "    ]\n",
        "    lags = spec_config['lags']\n",
        "\n",
        "    # --- Step 1: Create Data Matrices ---\n",
        "    Y, X, valid_index = create_var_matrices(\n",
        "        analysis_ready_df=analysis_ready_df,\n",
        "        endogenous_vars=endogenous_vars,\n",
        "        exogenous_vars=exogenous_vars,\n",
        "        lags=lags\n",
        "    )\n",
        "\n",
        "    # --- Step 2 & 3: Construct Prior Matrices ---\n",
        "    priors = construct_minnesota_prior(\n",
        "        Y=Y,\n",
        "        X=X,\n",
        "        endogenous_vars=endogenous_vars,\n",
        "        lags=lags,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- Assemble and return all results ---\n",
        "    results = {\n",
        "        'Y': Y,\n",
        "        'X': X,\n",
        "        'valid_index': valid_index,\n",
        "        'priors': priors,\n",
        "        'meta': {\n",
        "            'endogenous_vars': endogenous_vars,\n",
        "            'exogenous_vars': [c for c in analysis_ready_df.columns if c not in endogenous_vars],\n",
        "            'lags': lags,\n",
        "            'T': Y.shape[0],\n",
        "            'K': Y.shape[1],\n",
        "            'M': X.shape[1]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "QHYucWqGkKC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Gibbs Sampler Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# CUSTOM WARNING FOR THIS TASK\n",
        "# =============================================================================\n",
        "\n",
        "class ConvergenceWarning(UserWarning):\n",
        "    \"\"\"\n",
        "    Custom warning for potential MCMC convergence issues.\n",
        "\n",
        "    Purpose:\n",
        "        This warning is raised by the Gibbs sampler when a convergence\n",
        "        diagnostic (e.g., Geweke) indicates that the Markov chain may not have\n",
        "        reached its stationary distribution. It serves as a non-fatal alert to\n",
        "        the user that the posterior draws should be inspected carefully before\n",
        "        being used for inference.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER FOR CONVERGENCE DIAGNOSTIC\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_geweke_diagnostic(\n",
        "    chain: np.ndarray,\n",
        "    first_prop: float = 0.1,\n",
        "    last_prop: float = 0.5\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Geweke (1992) diagnostic z-score for a single MCMC chain.\n",
        "\n",
        "    Purpose:\n",
        "        This function implements a standard MCMC convergence diagnostic. It tests\n",
        "        the null hypothesis that the mean of an early segment of a Markov chain\n",
        "        is equal to the mean of a later segment. If the chain has converged,\n",
        "        the means should be similar. A large z-score (e.g., > |2|) suggests\n",
        "        that the chain has not yet converged.\n",
        "\n",
        "    Process:\n",
        "        1.  **Slice Chain**: Splits the input chain of posterior draws into two\n",
        "            segments: an early part (default: first 10%) and a late part\n",
        "            (default: last 50%).\n",
        "        2.  **Calculate Means**: Computes the simple mean of each segment.\n",
        "        3.  **Estimate HAC Variance**: To account for the serial correlation\n",
        "            inherent in MCMC draws, it estimates the variance of the segment\n",
        "            means using a Heteroskedasticity and Autocorrelation Consistent\n",
        "            (HAC) estimator (Newey-West). This is done by calculating the HAC\n",
        "            variance of the intercept in a regression of the segment on a constant.\n",
        "        4.  **Compute Z-Score**: The z-score is calculated as the difference in\n",
        "            means divided by the square root of the sum of their HAC variances.\n",
        "\n",
        "    Args:\n",
        "        chain (np.ndarray): A 1D array of posterior draws for a single parameter.\n",
        "        first_prop (float): Proportion of the start of the chain to use.\n",
        "        last_prop (float): Proportion of the end of the chain to use.\n",
        "\n",
        "    Returns:\n",
        "        float: The Geweke z-score. Returns np.nan if the chain is too short\n",
        "               or if the HAC estimation fails.\n",
        "    \"\"\"\n",
        "    # Ensure the chain is long enough for a meaningful comparison.\n",
        "    if len(chain) < 100:\n",
        "        return np.nan\n",
        "\n",
        "    # Define the number of observations in each segment of the chain.\n",
        "    n_obs: int = len(chain)\n",
        "    n_first: int = int(n_obs * first_prop)\n",
        "    n_last: int = int(n_obs * last_prop)\n",
        "\n",
        "    # Slice the chain into the two segments for comparison.\n",
        "    first_segment: np.ndarray = chain[:n_first]\n",
        "    last_segment: np.ndarray = chain[-n_last:]\n",
        "\n",
        "    # Calculate the mean of each segment.\n",
        "    mean_first: float = np.mean(first_segment)\n",
        "    mean_last: float = np.mean(last_segment)\n",
        "\n",
        "    # To calculate the standard error of the means robustly, we need the spectral\n",
        "    # density at frequency zero, which is estimated using a HAC variance estimator.\n",
        "    try:\n",
        "        # Estimate HAC variance for the first segment's mean.\n",
        "        var_first: float = sm.stats.sandwich_covariance.cov_hac(\n",
        "            sm.OLS(first_segment, np.ones_like(first_segment)).fit()\n",
        "        )[0, 0]\n",
        "\n",
        "        # Estimate HAC variance for the last segment's mean.\n",
        "        var_last: float = sm.stats.sandwich_covariance.cov_hac(\n",
        "            sm.OLS(last_segment, np.ones_like(last_segment)).fit()\n",
        "        )[0, 0]\n",
        "\n",
        "        # The Geweke z-score is the difference in means standardized by the HAC standard error.\n",
        "        z_score: float = (mean_first - mean_last) / np.sqrt(var_first + var_last)\n",
        "        return z_score\n",
        "    except Exception:\n",
        "        # If HAC estimation fails for any reason (e.g., zero variance segment), return NaN.\n",
        "        return np.nan\n",
        "\n",
        "# =============================================================================\n",
        "# IMPLEMENTATION OF `run_bvar_gibbs_sampler` (Task 10)\n",
        "# =============================================================================\n",
        "\n",
        "def run_bvar_gibbs_sampler(\n",
        "    bvar_setup: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Estimates a Bayesian VAR model using a Gibbs sampler with convergence monitoring.\n",
        "\n",
        "    Purpose:\n",
        "        This function is the computational core of the BVAR estimation. It\n",
        "        implements a Markov Chain Monte Carlo (MCMC) algorithm to draw samples\n",
        "        from the joint posterior distribution of the VAR coefficients (B) and\n",
        "        the error covariance matrix (Sigma). It now includes an intra-run\n",
        "        convergence check to provide assurance that the generated samples are\n",
        "        from a stable posterior distribution.\n",
        "\n",
        "    Process:\n",
        "        1.  **Initialization**: Sets the random seed for reproducibility, extracts\n",
        "            data (Y, X), priors, and MCMC settings. Initializes the parameter\n",
        "            values using OLS estimates to start the chain in a high-probability region.\n",
        "        2.  **Gibbs Sampling Loop**: Iterates for a specified number of draws,\n",
        "            alternately sampling from the two conditional posterior distributions:\n",
        "            a. **Draw Sigma | Y, X, B**: Samples the error covariance matrix\n",
        "               `Sigma` from its Inverse-Wishart conditional posterior.\n",
        "            b. **Draw B | Y, X, Sigma**: Samples the VAR coefficients `B` (in\n",
        "               vectorized form) from their Multivariate Normal conditional posterior.\n",
        "        3.  **Burn-in and Storage**: Discards the initial burn-in draws and\n",
        "            stores the subsequent draws.\n",
        "        4.  **Convergence Monitoring**: Periodically (e.g., every 1000 draws\n",
        "            after burn-in), it calculates the Geweke diagnostic on a subset of\n",
        "            key parameters (the main diagonal of the first lag coefficient\n",
        "            matrix). It issues a `ConvergenceWarning` if the diagnostic scores\n",
        "            are consistently large, alerting the user to potential issues.\n",
        "\n",
        "    Args:\n",
        "        bvar_setup (Dict[str, Any]): The output from `run_phase4_task9_bvar_setup`.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary containing the posterior draws:\n",
        "        - 'B_draws': A (num_retained, M, K) array of coefficient draws.\n",
        "        - 'Sigma_draws': A (num_retained, K, K) array of covariance matrix draws.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialization and Parameter Setup ---\n",
        "    # Extract MCMC settings from the configuration dictionary.\n",
        "    mcmc_config: Dict[str, int] = config['computation_settings']['mcmc_sampler']\n",
        "    total_draws: int = mcmc_config['total_draws']\n",
        "    burn_in: int = mcmc_config['burn_in_draws']\n",
        "    random_seed: int = mcmc_config['random_seed']\n",
        "    num_retained: int = total_draws - burn_in\n",
        "\n",
        "    # Set the random seed for perfect reproducibility of the MCMC chain.\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Extract data and prior matrices from the setup dictionary.\n",
        "    Y: np.ndarray = bvar_setup['Y']\n",
        "    X: np.ndarray = bvar_setup['X']\n",
        "    priors: Dict[str, np.ndarray] = bvar_setup['priors']\n",
        "    beta_0: np.ndarray = priors['beta_0']\n",
        "    V_0: np.ndarray = priors['V_0']\n",
        "    nu_0: float = priors['nu_0'].item()\n",
        "    S_0: np.ndarray = priors['S_0']\n",
        "\n",
        "    # Get model dimensions from metadata for clarity.\n",
        "    T, K, M = bvar_setup['meta']['T'], bvar_setup['meta']['K'], bvar_setup['meta']['M']\n",
        "\n",
        "    # Pre-compute constant matrices for efficiency inside the loop.\n",
        "    XtX: np.ndarray = X.T @ X\n",
        "    V_0_inv: np.ndarray = np.linalg.inv(V_0)\n",
        "\n",
        "    # Initialize the parameters using OLS estimates to start the chain in a reasonable place.\n",
        "    B_ols: np.ndarray = np.linalg.solve(XtX, X.T @ Y)\n",
        "    U_ols: np.ndarray = Y - X @ B_ols\n",
        "    Sigma_ols: np.ndarray = (U_ols.T @ U_ols) / (T - M)\n",
        "\n",
        "    # Set initial draws for the Gibbs sampler.\n",
        "    B_draw: np.ndarray = B_ols\n",
        "    Sigma_draw: np.ndarray = Sigma_ols\n",
        "\n",
        "    # Use lists for temporary storage during the run for easier appending.\n",
        "    temp_B_draws: List[np.ndarray] = []\n",
        "    temp_Sigma_draws: List[np.ndarray] = []\n",
        "\n",
        "    # --- 2. Iterative Sampling Procedure (The Gibbs Loop) ---\n",
        "    # Use tqdm for a progress bar to monitor the long-running loop.\n",
        "    for i in tqdm(range(total_draws), desc=\"Running BVAR Gibbs Sampler\"):\n",
        "\n",
        "        # === Draw Sigma | Y, X, B ~ IW(nu_post, S_post) ===\n",
        "        # Calculate residuals from the current coefficient draw.\n",
        "        U: np.ndarray = Y - X @ B_draw\n",
        "        # Update the parameters of the Inverse-Wishart distribution.\n",
        "        nu_post: float = nu_0 + T\n",
        "        S_post: np.ndarray = S_0 + (U.T @ U)\n",
        "        # Draw Sigma from IW(nu_post, S_post).\n",
        "        Sigma_draw = stats.invwishart.rvs(df=nu_post, scale=S_post)\n",
        "\n",
        "        # === Draw B | Y, X, Sigma ~ N(beta_bar, V_bar) ===\n",
        "        # Invert the newly drawn Sigma.\n",
        "        Sigma_inv: np.ndarray = np.linalg.inv(Sigma_draw)\n",
        "        # Update the parameters of the Multivariate Normal distribution for vec(B).\n",
        "        V_bar: np.ndarray = np.linalg.inv(np.kron(Sigma_inv, XtX) + V_0_inv)\n",
        "\n",
        "        beta_bar_term1: np.ndarray = (Sigma_inv @ Y.T @ X).flatten(order='F')\n",
        "        beta_bar_term2: np.ndarray = V_0_inv @ beta_0\n",
        "        beta_bar: np.ndarray = V_bar @ (beta_bar_term1 + beta_bar_term2)\n",
        "\n",
        "        # Draw vec(B) from N(beta_bar, V_bar) using a stable Cholesky method.\n",
        "        try:\n",
        "            chol_V_bar: np.ndarray = np.linalg.cholesky(V_bar)\n",
        "            beta_draw_flat: np.ndarray = beta_bar + chol_V_bar @ np.random.randn(M * K)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Fallback to standard multivariate normal if Cholesky fails.\n",
        "            beta_draw_flat = stats.multivariate_normal.rvs(mean=beta_bar, cov=V_bar)\n",
        "\n",
        "        # Reshape the flattened vector back into the (M x K) coefficient matrix.\n",
        "        B_draw = beta_draw_flat.reshape((M, K), order='F')\n",
        "\n",
        "        # --- 3. Storage ---\n",
        "        # Store all draws temporarily. Burn-in will be discarded after the loop.\n",
        "        temp_B_draws.append(B_draw)\n",
        "        temp_Sigma_draws.append(Sigma_draw)\n",
        "\n",
        "        # --- 4. Convergence Monitoring (as per blueprint) ---\n",
        "        # Run the diagnostic periodically after the burn-in phase has started.\n",
        "        current_post_burnin_draws = i - burn_in + 1\n",
        "        if i >= burn_in and current_post_burnin_draws % 1000 == 0 and current_post_burnin_draws > 100:\n",
        "            # Extract the post-burn-in chain so far.\n",
        "            current_chain_B: np.ndarray = np.array(temp_B_draws[burn_in:i+1])\n",
        "\n",
        "            # Check a few key parameters (diagonal of A_1 matrix).\n",
        "            z_scores: List[float] = []\n",
        "            for k_var in range(K):\n",
        "                # The index in the B matrix for the first own lag of variable k_var is (k_var, k_var).\n",
        "                param_chain: np.ndarray = current_chain_B[:, k_var, k_var]\n",
        "                z_scores.append(_calculate_geweke_diagnostic(param_chain))\n",
        "\n",
        "            # Check if any z-scores are large (e.g., > 2), indicating potential non-convergence.\n",
        "            if any(np.abs(z) > 2.0 for z in z_scores if not np.isnan(z)):\n",
        "                warnings.warn(\n",
        "                    f\"Geweke diagnostic z-score > 2.0 at iteration {i+1}. \"\n",
        "                    f\"Chain may not have converged. Z-scores: {[round(z, 2) for z in z_scores]}\",\n",
        "                    ConvergenceWarning\n",
        "                )\n",
        "\n",
        "    # --- Finalize Storage ---\n",
        "    # Discard the burn-in draws and convert lists to final NumPy arrays.\n",
        "    B_draws_storage: np.ndarray = np.array(temp_B_draws[burn_in:])\n",
        "    Sigma_draws_storage: np.ndarray = np.array(temp_Sigma_draws[burn_in:])\n",
        "\n",
        "    # Return a dictionary containing the final, retained posterior draws.\n",
        "    return {\n",
        "        'B_draws': B_draws_storage,\n",
        "        'Sigma_draws': Sigma_draws_storage\n",
        "    }\n"
      ],
      "metadata": {
        "id": "FPgTtYb8lBgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Local Projections Estimation\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 11: LOCAL PROJECTIONS ESTIMATION\n",
        "# =============================================================================\n",
        "\n",
        "def _prepare_lp_regression_data(\n",
        "    df: pd.DataFrame,\n",
        "    dependent_var: str,\n",
        "    shock_vars: List[str],\n",
        "    control_vars: List[str],\n",
        "    horizon: int,\n",
        "    lags: int\n",
        ") -> Tuple[pd.Series, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Prepares the y vector and X matrix for a single LP regression.\n",
        "\n",
        "    This helper function constructs the specific dataset required for estimating\n",
        "    the local projection for one variable at one horizon.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The full analysis-ready dataset.\n",
        "        dependent_var (str): The name of the endogenous variable to project.\n",
        "        shock_vars (List[str]): Names of the contemporaneous shock variables.\n",
        "        control_vars (List[str]): Names of all control variables (other endog,\n",
        "                                  lags, deterministics).\n",
        "        horizon (int): The projection horizon `h`.\n",
        "        lags (int): The number of lags to include for all controls.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.DataFrame]:\n",
        "        - y: The dependent variable vector (y_{t+h}).\n",
        "        - X: The matrix of regressors at time t.\n",
        "    \"\"\"\n",
        "    # Create a new DataFrame for this regression to avoid side effects.\n",
        "    reg_df = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # --- Create the dependent variable y_{t+h} ---\n",
        "    reg_df['y_h'] = df[dependent_var].shift(-horizon)\n",
        "\n",
        "    # --- Create the regressors at time t ---\n",
        "    # 1. Contemporaneous shocks\n",
        "    reg_df[shock_vars] = df[shock_vars]\n",
        "\n",
        "    # 2. Lagged controls\n",
        "    for var in control_vars:\n",
        "        for i in range(1, lags + 1):\n",
        "            reg_df[f\"{var}_L{i}\"] = df[var].shift(i)\n",
        "\n",
        "    # 3. Add a constant for the intercept.\n",
        "    reg_df = sm.add_constant(reg_df, prepend=True)\n",
        "\n",
        "    # --- Align data by dropping all rows with NaNs ---\n",
        "    reg_df.dropna(inplace=True)\n",
        "\n",
        "    # Separate and return the final y vector and X matrix.\n",
        "    y = reg_df['y_h']\n",
        "    X = reg_df.drop(columns=['y_h'])\n",
        "\n",
        "    return y, X\n",
        "\n",
        "\n",
        "def estimate_local_projections(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    endogenous_vars: List[str],\n",
        "    shock_vars: List[str],\n",
        "    horizon: int,\n",
        "    lags: int,\n",
        "    confidence_level: float = 0.90\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Estimates impulse response functions using Local Projections (Jordà, 2005).\n",
        "\n",
        "    Purpose:\n",
        "        This function provides an alternative, often more robust, method to VARs\n",
        "        for estimating impulse responses. It runs a separate OLS regression for\n",
        "        each variable and each forecast horizon.\n",
        "\n",
        "    Process:\n",
        "        For each endogenous variable and for each horizon `h` from 0 to `H`:\n",
        "        1.  **Construct Data**: Creates the dependent variable `y_{t+h}` and the\n",
        "            matrix of regressors `X_t`. `X_t` includes the contemporaneous\n",
        "            shocks of interest, plus lags of the dependent variable, lags of\n",
        "            all other control variables, and deterministic terms.\n",
        "            Equation: `y_{t+h} = beta_h * shock_t + controls_t + e_{t+h}`\n",
        "        2.  **Estimate Model**: Fits the regression using Ordinary Least Squares (OLS).\n",
        "        3.  **Calculate HAC Errors**: Computes Newey-West Heteroskedasticity and\n",
        "            Autocorrelation Consistent (HAC) standard errors. This is critical\n",
        "            because the regression residuals are serially correlated by\n",
        "            construction for horizons `h > 0`. An automatic bandwidth selection\n",
        "            rule is used.\n",
        "        4.  **Extract Results**: Stores the estimated coefficient on the shock\n",
        "            variable (`beta_h`), which is the impulse response at horizon `h`.\n",
        "        5.  **Compute Confidence Bands**: Calculates the confidence interval\n",
        "            around the point estimate using the HAC standard errors.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset.\n",
        "        endogenous_vars (List[str]): List of endogenous variable names.\n",
        "        shock_vars (List[str]): List of shock variable names to estimate responses to.\n",
        "        horizon (int): The maximum forecast horizon `H`.\n",
        "        lags (int): The number of lags `p` for all control variables.\n",
        "        confidence_level (float): The confidence level for the error bands (e.g., 0.90 for 90%).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary containing the results arrays:\n",
        "        - 'irf_point': Point estimates of the IRFs.\n",
        "        - 'irf_lower': Lower bounds of the confidence intervals.\n",
        "        - 'irf_upper': Upper bounds of the confidence intervals.\n",
        "        The shape of each array is (num_endog_vars, num_shocks, horizon + 1).\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    num_endog = len(endogenous_vars)\n",
        "    num_shocks = len(shock_vars)\n",
        "\n",
        "    # Pre-allocate NumPy arrays to store the results.\n",
        "    irf_point = np.zeros((num_endog, num_shocks, horizon + 1))\n",
        "    irf_lower = np.zeros((num_endog, num_shocks, horizon + 1))\n",
        "    irf_upper = np.zeros((num_endog, num_shocks, horizon + 1))\n",
        "\n",
        "    # Define the set of all possible control variables.\n",
        "    # This includes all endogenous variables and the shocks themselves.\n",
        "    all_controls = endogenous_vars + shock_vars\n",
        "\n",
        "    # Calculate the critical value from the normal distribution for the confidence interval.\n",
        "    alpha = 1 - confidence_level\n",
        "    z_critical = stats.norm.ppf(1 - alpha / 2)\n",
        "\n",
        "    # --- Main Loop: Iterate over variables, shocks, and horizons ---\n",
        "    # Use tqdm for progress tracking as this can be computationally intensive.\n",
        "    with tqdm(total=num_endog * (horizon + 1), desc=\"Running Local Projections\") as pbar:\n",
        "        for i, dep_var in enumerate(endogenous_vars):\n",
        "            for h in range(horizon + 1):\n",
        "                # --- Step 1: Prepare data for this specific regression ---\n",
        "                y, X = _prepare_lp_regression_data(\n",
        "                    df=analysis_ready_df,\n",
        "                    dependent_var=dep_var,\n",
        "                    shock_vars=shock_vars,\n",
        "                    control_vars=all_controls,\n",
        "                    horizon=h,\n",
        "                    lags=lags\n",
        "                )\n",
        "\n",
        "                # --- Step 2: Estimate OLS model ---\n",
        "                model = sm.OLS(y, X).fit()\n",
        "\n",
        "                # --- Step 3: Calculate HAC standard errors ---\n",
        "                # Use statsmodels' robust implementation of Newey-West.\n",
        "                # `maxlags` is chosen automatically based on sample size if None.\n",
        "                hac_cov = sm.stats.sandwich_covariance.cov_hac(model, nlags=None)\n",
        "                hac_se = np.sqrt(np.diag(hac_cov))\n",
        "\n",
        "                # --- Step 4 & 5: Extract results and compute CIs ---\n",
        "                for j, shock_var in enumerate(shock_vars):\n",
        "                    # Get the index of the shock coefficient in the results.\n",
        "                    shock_idx = X.columns.get_loc(shock_var)\n",
        "\n",
        "                    # The IRF is the coefficient on the contemporaneous shock.\n",
        "                    point_estimate = model.params[shock_idx]\n",
        "\n",
        "                    # The standard error is the HAC-adjusted SE.\n",
        "                    std_err = hac_se[shock_idx]\n",
        "\n",
        "                    # Store the point estimate.\n",
        "                    irf_point[i, j, h] = point_estimate\n",
        "                    # Store the lower confidence band.\n",
        "                    irf_lower[i, j, h] = point_estimate - z_critical * std_err\n",
        "                    # Store the upper confidence band.\n",
        "                    irf_upper[i, j, h] = point_estimate + z_critical * std_err\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    return {\n",
        "        'irf_point': irf_point,\n",
        "        'irf_lower': irf_lower,\n",
        "        'irf_upper': irf_upper\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FUNCTION FOR TASK 11\n",
        "# =============================================================================\n",
        "\n",
        "def run_phase4_task11_local_projections(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full Local Projections estimation pipeline for Task 11.\n",
        "\n",
        "    This master function extracts the necessary model specifications from the\n",
        "    configuration and runs the local projection estimator to generate impulse\n",
        "    response functions and their confidence bands.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset from Task 8.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary containing the IRF point estimates\n",
        "                               and confidence bands from the LP estimation.\n",
        "    \"\"\"\n",
        "    # --- Extract specifications from config ---\n",
        "    lp_config = config['local_projection_specification']\n",
        "    bvar_config = config['bvar_specification']\n",
        "    comp_config = config['computation_settings']['impulse_response_functions']\n",
        "\n",
        "    # Identify endogenous variables and shocks to analyze.\n",
        "    endogenous_vars = bvar_config['endogenous_variables']\n",
        "    shock_vars = [var for var in bvar_config['exogenous_variables'] if 'shock' in var]\n",
        "\n",
        "    # Get model parameters.\n",
        "    horizon = comp_config['horizon_months']\n",
        "    lags = lp_config['lags_dependent_variable'] # Assuming all lags are the same\n",
        "\n",
        "    # Get inference parameters.\n",
        "    confidence_level = comp_config['credible_interval_level'] # Use same level for CIs\n",
        "\n",
        "    # --- Run the Local Projections estimator ---\n",
        "    lp_results = estimate_local_projections(\n",
        "        analysis_ready_df=analysis_ready_df,\n",
        "        endogenous_vars=endogenous_vars,\n",
        "        shock_vars=shock_vars,\n",
        "        horizon=horizon,\n",
        "        lags=lags,\n",
        "        confidence_level=confidence_level\n",
        "    )\n",
        "\n",
        "    return lp_results\n"
      ],
      "metadata": {
        "id": "xhCgKQCdlx4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: VAR-Based Impulse Response Functions\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 12: VAR-BASED IMPULSE RESPONSE FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_irfs_for_single_draw(\n",
        "    B_draw: np.ndarray,\n",
        "    shock_indices: List[int],\n",
        "    num_vars: int,\n",
        "    lags: int,\n",
        "    horizon: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Helper to compute IRFs for a single posterior draw of coefficients.\n",
        "\n",
        "    Args:\n",
        "        B_draw (np.ndarray): A single (M, K) coefficient matrix draw.\n",
        "        shock_indices (List[int]): The column indices in X corresponding to the shocks.\n",
        "        num_vars (int): The number of endogenous variables (K).\n",
        "        lags (int): The number of lags in the VAR (p).\n",
        "        horizon (int): The maximum horizon for the IRFs (H).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An (H+1, K, num_shocks) array of IRFs for this draw.\n",
        "    \"\"\"\n",
        "    # --- 1. Construct Companion Matrix F ---\n",
        "    # The companion matrix has dimensions (Kp x Kp).\n",
        "    F = np.zeros((num_vars * lags, num_vars * lags))\n",
        "    # The first block-row contains the K x K coefficient matrices A_1, ..., A_p.\n",
        "    for i in range(lags):\n",
        "        # Extract A_i from the top of the B_draw matrix.\n",
        "        A_i = B_draw[i * num_vars : (i + 1) * num_vars, :].T\n",
        "        F[0:num_vars, i * num_vars : (i + 1) * num_vars] = A_i\n",
        "    # The lower blocks form an identity matrix to shift the lags.\n",
        "    if lags > 1:\n",
        "        I_block = np.eye(num_vars * (lags - 1))\n",
        "        F[num_vars:, 0 : num_vars * (lags - 1)] = I_block\n",
        "\n",
        "    # --- 2. Extract Impact Matrix B_0 for shocks ---\n",
        "    # The impact matrix is the set of coefficients on the shock variables.\n",
        "    B0 = B_draw[shock_indices, :] # Shape: (num_shocks, K)\n",
        "\n",
        "    # --- 3. Recursive IRF Calculation ---\n",
        "    # Pre-allocate storage for the IRFs for this draw.\n",
        "    irfs = np.zeros((horizon + 1, num_vars, len(shock_indices)))\n",
        "    # The impact at horizon 0 is simply the B0 matrix (transposed).\n",
        "    irfs[0, :, :] = B0.T\n",
        "\n",
        "    # Use the companion form to iterate through horizons.\n",
        "    # F_power will hold F^h.\n",
        "    F_power = np.eye(F.shape[0])\n",
        "    for h in range(1, horizon + 1):\n",
        "        # Update F_power to F^h.\n",
        "        F_power = F_power @ F\n",
        "        # The IRF is the top-left (K x K) block of F^h, post-multiplied by B0.\n",
        "        # Equation: Psi_h = J * F^h * J' * B0, where J selects the top block.\n",
        "        response_matrix = F_power[0:num_vars, 0:num_vars]\n",
        "        irfs[h, :, :] = (response_matrix @ B0.T)\n",
        "\n",
        "    return irfs\n",
        "\n",
        "\n",
        "def calculate_var_impulse_responses(\n",
        "    posterior_draws: Dict[str, np.ndarray],\n",
        "    bvar_setup: Dict[str, Any],\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes and summarizes Bayesian impulse response functions from posterior draws.\n",
        "\n",
        "    Purpose:\n",
        "        This function translates the posterior distribution of VAR coefficients\n",
        "        into a posterior distribution of impulse responses. It then summarizes\n",
        "        this distribution to provide point estimates (posterior median) and\n",
        "        uncertainty bands (credible intervals).\n",
        "\n",
        "    Process:\n",
        "        1.  **Iterate Through Draws**: For each saved draw from the Gibbs sampler:\n",
        "            a. **Construct Companion Matrix**: The VAR(p) model is cast into its\n",
        "               VAR(1) state-space (companion) form.\n",
        "            b. **Identify Impact Matrix**: The coefficients corresponding to the\n",
        "               exogenous shocks are extracted.\n",
        "            c. **Compute IRFs**: The impulse responses are calculated recursively\n",
        "               up to the specified horizon using the companion matrix.\n",
        "        2.  **Store All Draws**: The computed IRFs for all draws are stored in a\n",
        "            large multi-dimensional array.\n",
        "        3.  **Summarize Posterior**:\n",
        "            a. **Point Estimate**: The posterior median is calculated for each\n",
        "               point of the IRF, providing a robust central tendency estimate.\n",
        "            b. **Credible Intervals**: The desired percentiles (e.g., 5th and\n",
        "               95th for a 90% interval) are calculated to represent the range\n",
        "               of uncertainty.\n",
        "            c. **Significance**: A boolean mask is created to indicate where the\n",
        "               credible interval does not contain zero.\n",
        "\n",
        "    Args:\n",
        "        posterior_draws (Dict[str, np.ndarray]): The output from the Gibbs\n",
        "            sampler, containing 'B_draws' and 'Sigma_draws'.\n",
        "        bvar_setup (Dict[str, Any]): The setup dictionary from Task 9, containing\n",
        "            model metadata like variable names and lag order.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, np.ndarray]: A dictionary of summary arrays:\n",
        "        - 'irf_median': The posterior median of the IRFs.\n",
        "        - 'irf_lower': The lower bound of the credible interval.\n",
        "        - 'irf_upper': The upper bound of the credible interval.\n",
        "        - 'irf_significant': Boolean mask for statistical significance.\n",
        "        Shape of each array is (num_endog_vars, num_shocks, horizon + 1).\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    B_draws = posterior_draws['B_draws']\n",
        "    num_draws, M, K = B_draws.shape\n",
        "\n",
        "    meta = bvar_setup['meta']\n",
        "    lags = meta['lags']\n",
        "\n",
        "    irf_config = config['computation_settings']['impulse_response_functions']\n",
        "    horizon = irf_config['horizon_months']\n",
        "    ci_level = config['computation_settings']['credible_interval_level']\n",
        "\n",
        "    # Identify which columns of X correspond to the shocks.\n",
        "    shock_vars = [var for var in meta['exogenous_vars'] if 'shock' in var]\n",
        "    # The regressor order is [lags..., exog..., const]\n",
        "    shock_indices = [K * lags + meta['exogenous_vars'].index(var) for var in shock_vars]\n",
        "    num_shocks = len(shock_vars)\n",
        "\n",
        "    # --- Step 1 & 2: Calculate IRFs for all draws ---\n",
        "    # Pre-allocate a 4D array to store all IRF draws.\n",
        "    # Dimensions: (draw, horizon, responding_variable, shock)\n",
        "    all_irf_draws = np.zeros((num_draws, horizon + 1, K, num_shocks))\n",
        "\n",
        "    for i in tqdm(range(num_draws), desc=\"Calculating VAR Impulse Responses\"):\n",
        "        all_irf_draws[i, :, :, :] = _calculate_irfs_for_single_draw(\n",
        "            B_draw=B_draws[i, :, :],\n",
        "            shock_indices=shock_indices,\n",
        "            num_vars=K,\n",
        "            lags=lags,\n",
        "            horizon=horizon\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Summarize the Posterior Distribution of IRFs ---\n",
        "    # Calculate the lower and upper percentile bounds for the credible interval.\n",
        "    lower_p = (1 - ci_level) / 2 * 100\n",
        "    upper_p = (1 - (1 - ci_level) / 2) * 100\n",
        "\n",
        "    # Calculate median and percentiles across the 'draws' axis (axis=0).\n",
        "    irf_median = np.median(all_irf_draws, axis=0)\n",
        "    irf_lower = np.percentile(all_irf_draws, lower_p, axis=0)\n",
        "    irf_upper = np.percentile(all_irf_draws, upper_p, axis=0)\n",
        "\n",
        "    # Assess significance: the interval does not contain zero.\n",
        "    irf_significant = (irf_lower > 0) | (irf_upper < 0)\n",
        "\n",
        "    # Reshape results to the desired output format: (K, S, H+1)\n",
        "    # Current shape is (H+1, K, S). Transpose axes.\n",
        "    final_shape_order = (1, 2, 0) # K, S, H+1\n",
        "\n",
        "    return {\n",
        "        'irf_median': irf_median.transpose(final_shape_order),\n",
        "        'irf_lower': irf_lower.transpose(final_shape_order),\n",
        "        'irf_upper': irf_upper.transpose(final_shape_order),\n",
        "        'irf_significant': irf_significant.transpose(final_shape_order),\n",
        "        'all_irf_draws': all_irf_draws # Optional: return all draws for further analysis\n",
        "    }\n"
      ],
      "metadata": {
        "id": "r6IRudK4mmvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Transmission Channel Analysis Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 13: TRANSMISSION CHANNEL ANALYSIS IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_transmission_channel_analysis(\n",
        "    full_analysis_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    channel_specifications: List[Dict[str, Any]]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs BVAR analysis on augmented models to study transmission channels.\n",
        "\n",
        "    Purpose:\n",
        "        This function is a high-level orchestrator that systematically investigates\n",
        "        different economic transmission channels. It manages a \"campaign\" of\n",
        "        BVAR model estimations, where each model is an augmentation of a baseline\n",
        "        specification. It is designed to be flexible, supporting both the\n",
        "        addition of a group of variables (e.g., a \"financial channel\") and an\n",
        "        iterative analysis over a list of variables (e.g., a sectoral \"export\n",
        "        disaggregation\").\n",
        "\n",
        "    Process:\n",
        "        1.  **Input Validation**: It first validates the structure of the\n",
        "            `channel_specifications` list to ensure each entry is well-formed.\n",
        "        2.  **Load Baseline**: Retrieves the list of core endogenous variables\n",
        "            from the main configuration.\n",
        "        3.  **Iterate Through Specifications**: For each specification dictionary\n",
        "            in the input list:\n",
        "            a. **Augment Specification**: If the type is 'augment', it creates a\n",
        "               single new list of endogenous variables by adding a group of new\n",
        "               variables to the baseline.\n",
        "            b. **Iterate Specification**: If the type is 'iterate', it enters a\n",
        "               nested loop. In each iteration, it creates a new list of\n",
        "               endogenous variables by adding just *one* of the specified\n",
        "               variables to the baseline.\n",
        "            c. **Execute Pipeline**: For each generated model specification, it\n",
        "               creates a deep copy of the configuration, updates the list of\n",
        "               endogenous variables, and executes the full, trusted BVAR\n",
        "               estimation and IRF calculation pipeline (Tasks 9, 10, and 12).\n",
        "            d. **Store Results**: The complete results of each model run are\n",
        "               stored in a master dictionary, keyed by a unique and descriptive\n",
        "               channel name.\n",
        "\n",
        "    Args:\n",
        "        full_analysis_df (pd.DataFrame): An analysis-ready DataFrame that\n",
        "            contains the baseline variables PLUS all potential channel variables.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        channel_specifications (List[Dict[str, Any]]): A list of dictionaries,\n",
        "            each defining an analysis to run. Required keys per dict:\n",
        "            - 'channel_name' (str): Base name for the analysis.\n",
        "            - 'type' (str): Method of analysis, either 'augment' or 'iterate'.\n",
        "            - 'variables' (List[str]): The list of variables for the analysis.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A nested dictionary where top-level keys are\n",
        "        the unique channel names. Each inner dictionary contains the full IRF\n",
        "        results and metadata for that specific augmented model run.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the `channel_specifications` structure is invalid or if\n",
        "                    a specified variable is not found in the `full_analysis_df`.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Retrieve the list of core endogenous variables from the baseline specification.\n",
        "    baseline_endog_vars: List[str] = config['bvar_specification']['endogenous_variables']\n",
        "\n",
        "    # This dictionary will store the final results for all channel analyses.\n",
        "    all_channel_results: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    # --- Main Loop: Iterate through each specified channel analysis ---\n",
        "    for spec in tqdm(channel_specifications, desc=\"Analyzing Transmission Channels\"):\n",
        "        # --- 1. Validate the specification entry ---\n",
        "        if not all(k in spec for k in ['channel_name', 'type', 'variables']):\n",
        "            raise ValueError(f\"Invalid channel specification found: {spec}. Must contain 'channel_name', 'type', and 'variables'.\")\n",
        "        if spec['type'] not in ['augment', 'iterate']:\n",
        "            raise ValueError(f\"Invalid spec type '{spec['type']}'. Must be 'augment' or 'iterate'.\")\n",
        "\n",
        "        # Check that all specified additional variables exist in the main DataFrame.\n",
        "        for var in spec['variables']:\n",
        "            if var not in full_analysis_df.columns:\n",
        "                raise ValueError(f\"Variable '{var}' from channel spec '{spec['channel_name']}' not found in the analysis DataFrame.\")\n",
        "\n",
        "        # --- 2. Generate Model Runs based on Specification Type ---\n",
        "        runs_to_execute: List[Tuple[str, List[str]]] = []\n",
        "\n",
        "        if spec['type'] == 'augment':\n",
        "            # For 'augment', create one run that adds all specified variables to the baseline.\n",
        "            channel_name = spec['channel_name']\n",
        "            # Use a set for efficient union and to prevent duplicates.\n",
        "            augmented_vars = list(dict.fromkeys(baseline_endog_vars + spec['variables']))\n",
        "            runs_to_execute.append((channel_name, augmented_vars))\n",
        "\n",
        "        elif spec['type'] == 'iterate':\n",
        "            # For 'iterate', create a separate run for each variable in the list.\n",
        "            for var in spec['variables']:\n",
        "                # Create a unique, descriptive name for this specific run.\n",
        "                channel_name = f\"{spec['channel_name']}_{var}\"\n",
        "                # The model includes the baseline plus this single additional variable.\n",
        "                augmented_vars = baseline_endog_vars + [var]\n",
        "                runs_to_execute.append((channel_name, augmented_vars))\n",
        "\n",
        "        # --- 3. Execute the BVAR pipeline for each generated run ---\n",
        "        for run_name, endog_vars_for_run in runs_to_execute:\n",
        "\n",
        "            # Create a deep copy of the configuration to prevent side effects.\n",
        "            # This is critical for ensuring each run is independent.\n",
        "            run_config = copy.deepcopy(config)\n",
        "            run_config['bvar_specification']['endogenous_variables'] = endog_vars_for_run\n",
        "\n",
        "            # Announce which model is being run.\n",
        "            tqdm.write(f\"--- Running analysis for: {run_name} ---\")\n",
        "\n",
        "            # Execute the full, trusted BVAR estimation and IRF pipeline.\n",
        "            # Task 9: Set up the BVAR with the augmented variable set.\n",
        "            bvar_setup = run_phase4_task9_bvar_setup(\n",
        "                analysis_ready_df=full_analysis_df,\n",
        "                config=run_config\n",
        "            )\n",
        "\n",
        "            # Task 10: Run the Gibbs sampler for the augmented model.\n",
        "            posterior_draws = run_bvar_gibbs_sampler(\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=run_config\n",
        "            )\n",
        "\n",
        "            # Task 12: Calculate and summarize impulse responses for the augmented model.\n",
        "            irf_results = calculate_var_impulse_responses(\n",
        "                posterior_draws=posterior_draws,\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=run_config\n",
        "            )\n",
        "\n",
        "            # --- 4. Store the results for this run ---\n",
        "            all_channel_results[run_name] = {\n",
        "                'irf_results': irf_results,\n",
        "                'model_metadata': {\n",
        "                    'channel_name': run_name,\n",
        "                    'endogenous_variables': endog_vars_for_run,\n",
        "                    'num_variables': len(endog_vars_for_run),\n",
        "                    'lags': bvar_setup['meta']['lags']\n",
        "                }\n",
        "            }\n",
        "\n",
        "    # Return the aggregated dictionary containing results from all runs.\n",
        "    return all_channel_results\n",
        "\n"
      ],
      "metadata": {
        "id": "u-Y5z_vRnWDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Cross-Validation and Model Comparison\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 14, STEP 1: IN-SAMPLE FIT ASSESSMENT HELPERS\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_bvar_log_marginal_likelihood(\n",
        "    Y: np.ndarray,\n",
        "    X: np.ndarray,\n",
        "    posterior_draws: Dict[str, np.ndarray]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Log Marginal Likelihood via the Harmonic Mean Estimator.\n",
        "\n",
        "    Purpose:\n",
        "        This function provides an estimate of the model evidence, P(Y|M), which\n",
        "        is a key metric for Bayesian model comparison. It uses the Harmonic\n",
        "        Mean Estimator (HME), which is simple to compute from posterior draws\n",
        "        but can be numerically unstable.\n",
        "\n",
        "    Process:\n",
        "        1.  **Iterate Draws**: For each posterior draw of the parameters (B, Sigma),\n",
        "            it calculates the log-likelihood of the data given those parameters.\n",
        "            The log-likelihood is based on the multivariate normal distribution:\n",
        "            `ln p(Y|B, Sigma) = sum_{t=1 to T} ln N(U_t | 0, Sigma)`\n",
        "            where `U = Y - XB`.\n",
        "        2.  **Handle Instability**: It includes a `try-except` block to handle\n",
        "            rare cases where a drawn `Sigma` matrix is not positive definite,\n",
        "            assigning an infinitely poor likelihood to such draws.\n",
        "        3.  **Stable Mean Calculation**: It computes the harmonic mean in log-space\n",
        "            using the log-sum-exp trick to prevent numerical underflow/overflow,\n",
        "            which is a major issue with the naive HME.\n",
        "            `log(HME) = max(logL) - log(mean(exp(max(logL) - logL)))`\n",
        "\n",
        "    Args:\n",
        "        Y (np.ndarray): The (T x K) matrix of endogenous variables.\n",
        "        X (np.ndarray): The (T x M) matrix of regressors.\n",
        "        posterior_draws (Dict[str, np.ndarray]): A dictionary containing the\n",
        "            'B_draws' and 'Sigma_draws' from the Gibbs sampler.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated log marginal likelihood of the BVAR model.\n",
        "    \"\"\"\n",
        "    # Extract the posterior draws for coefficients (B) and covariance (Sigma).\n",
        "    B_draws: np.ndarray = posterior_draws['B_draws']\n",
        "    Sigma_draws: np.ndarray = posterior_draws['Sigma_draws']\n",
        "\n",
        "    # Get dimensions from the draws array.\n",
        "    num_draws, _, K = B_draws.shape\n",
        "\n",
        "    # Pre-allocate an array to store the log-likelihood for each posterior draw.\n",
        "    log_likelihoods: np.ndarray = np.zeros(num_draws)\n",
        "\n",
        "    # Iterate through each of the posterior draws.\n",
        "    for i in range(num_draws):\n",
        "        # Calculate the residuals for the i-th draw: U_i = Y - X * B_i\n",
        "        U_i: np.ndarray = Y - X @ B_draws[i, :, :]\n",
        "\n",
        "        try:\n",
        "            # Calculate the sum of log-PDFs of the multivariate normal distribution\n",
        "            # for the residuals, given the i-th draw of the covariance matrix.\n",
        "            log_likelihoods[i] = np.sum(\n",
        "                multivariate_normal.logpdf(U_i, mean=np.zeros(K), cov=Sigma_draws[i, :, :], allow_singular=False)\n",
        "            )\n",
        "        except np.linalg.LinAlgError:\n",
        "            # If a Sigma draw is not positive definite, its likelihood is zero (log-likelihood is -inf).\n",
        "            log_likelihoods[i] = -np.inf\n",
        "\n",
        "    # To compute the harmonic mean stably, use the log-sum-exp trick.\n",
        "    # This prevents numerical underflow when exponentiating large negative log-likelihoods.\n",
        "    # Find the maximum log-likelihood to serve as a scaling factor.\n",
        "    max_log_lik: float = np.max(log_likelihoods)\n",
        "\n",
        "    # Calculate the log of the mean of the inverse likelihoods.\n",
        "    log_mean_inv_lik: float = np.log(np.mean(np.exp(-log_likelihoods + max_log_lik)))\n",
        "\n",
        "    # Combine the terms to get the final log marginal likelihood.\n",
        "    log_marginal_lik: float = max_log_lik - log_mean_inv_lik\n",
        "\n",
        "    # Return the final scalar value.\n",
        "    return float(log_marginal_lik)\n",
        "\n",
        "\n",
        "def _perform_diebold_mariano_test(\n",
        "    errors1: np.ndarray,\n",
        "    errors2: np.ndarray\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Performs a Diebold-Mariano test for equal forecast accuracy.\n",
        "\n",
        "    Purpose:\n",
        "        This test formally compares the predictive accuracy of two forecast models.\n",
        "        The null hypothesis is that both models have the same forecast accuracy.\n",
        "        A low p-value suggests a statistically significant difference in performance.\n",
        "\n",
        "    Process:\n",
        "        1.  **Define Loss**: Uses squared error as the loss function.\n",
        "        2.  **Calculate Loss Differential**: Computes the series `d_t = L(e1_t) - L(e2_t)`,\n",
        "            where `L(e) = e^2`.\n",
        "        3.  **Test Mean**: The test statistic is a t-test for whether the mean of `d_t`\n",
        "            is zero.\n",
        "        4.  **HAC Standard Errors**: Critically, the standard error of `mean(d_t)` is\n",
        "            calculated using a Newey-West Heteroskedasticity and Autocorrelation\n",
        "            Consistent (HAC) estimator, which is necessary because forecast errors\n",
        "            can be serially correlated.\n",
        "\n",
        "    Args:\n",
        "        errors1 (np.ndarray): A 1D array of forecast errors from Model 1.\n",
        "        errors2 (np.ndarray): A 1D array of forecast errors from Model 2, aligned\n",
        "                              with `errors1`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        - The Diebold-Mariano t-statistic.\n",
        "        - The corresponding p-value.\n",
        "    \"\"\"\n",
        "    # Calculate the loss differential series using squared error loss.\n",
        "    loss_diff: np.ndarray = errors1**2 - errors2**2\n",
        "\n",
        "    # Calculate the mean of the loss differential.\n",
        "    mean_loss_diff: float = np.mean(loss_diff)\n",
        "\n",
        "    # To get the HAC standard error of the mean, we can regress the loss\n",
        "    # differential on a constant and compute the HAC variance of the intercept.\n",
        "    X_const: np.ndarray = np.ones_like(loss_diff)\n",
        "\n",
        "    # Fit a simple OLS model: loss_diff = const + error\n",
        "    model: sm.regression.linear_model.RegressionResultsWrapper = sm.OLS(loss_diff, X_const).fit()\n",
        "\n",
        "    # Compute the HAC covariance matrix for the estimated parameters (just the constant).\n",
        "    hac_cov: np.ndarray = sm.stats.sandwich_covariance.cov_hac(model)\n",
        "\n",
        "    # The HAC standard error of the mean is the square root of the variance of the constant.\n",
        "    hac_se: float = np.sqrt(hac_cov[0, 0])\n",
        "\n",
        "    # The Diebold-Mariano statistic is the t-statistic for the mean.\n",
        "    dm_stat: float = mean_loss_diff / hac_se\n",
        "\n",
        "    # Calculate the two-sided p-value using the t-distribution.\n",
        "    p_value: float = 2 * (1 - t.cdf(np.abs(dm_stat), df=len(loss_diff) - 1))\n",
        "\n",
        "    # Return the statistic and its p-value.\n",
        "    return float(dm_stat), float(p_value)\n",
        "\n",
        "\n",
        "def assess_in_sample_fit(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    bvar_setup: Dict[str, Any],\n",
        "    posterior_draws: Dict[str, np.ndarray],\n",
        "    config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a comprehensive in-sample fit and comparison of BVAR and LP models.\n",
        "\n",
        "    Purpose:\n",
        "        This function computes several key metrics to evaluate how well the\n",
        "        estimated BVAR and Local Projection models fit the data they were\n",
        "        trained on.\n",
        "\n",
        "    Process:\n",
        "        1.  **BVAR Metrics**:\n",
        "            - Calculates in-sample fitted values using the posterior mean of\n",
        "              the BVAR coefficients.\n",
        "            - Computes the Root Mean Squared Error (RMSE) for each endogenous variable.\n",
        "            - Estimates the Log Marginal Likelihood using the Harmonic Mean Estimator.\n",
        "        2.  **LP Metrics**:\n",
        "            - For each endogenous variable, it re-constructs the `h=0` (in-sample)\n",
        "              Local Projection regression.\n",
        "            - It fits this regression and calculates the in-sample RMSE.\n",
        "        3.  **Model Comparison**:\n",
        "            - It performs a Diebold-Mariano test to formally compare the\n",
        "              in-sample forecast accuracy (based on squared errors) of the BVAR\n",
        "              and LP models for each variable.\n",
        "        4.  **Reporting**: Compiles all metrics into a single, well-organized\n",
        "            DataFrame for easy comparison.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The full dataset used for estimation.\n",
        "        bvar_setup (Dict[str, Any]): The setup dictionary from Task 9.\n",
        "        posterior_draws (Dict[str, np.ndarray]): The posterior draws from Task 10.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A report summarizing in-sample fit metrics, indexed by\n",
        "                      the endogenous variable names.\n",
        "    \"\"\"\n",
        "    # --- BVAR In-Sample Fit ---\n",
        "    # Extract data matrices and posterior mean of coefficients.\n",
        "    Y, X = bvar_setup['Y'], bvar_setup['X']\n",
        "    B_mean = np.mean(posterior_draws['B_draws'], axis=0)\n",
        "\n",
        "    # Calculate fitted values and residuals: Y_hat = X * B_mean\n",
        "    Y_fit_bvar = X @ B_mean\n",
        "    errors_bvar = Y - Y_fit_bvar\n",
        "\n",
        "    # Calculate RMSE for each endogenous variable.\n",
        "    rmse_bvar = np.sqrt(np.mean(errors_bvar**2, axis=0))\n",
        "\n",
        "    # Calculate the log marginal likelihood for the BVAR model.\n",
        "    lml_bvar = _calculate_bvar_log_marginal_likelihood(Y, X, posterior_draws)\n",
        "\n",
        "    # --- LP In-Sample Fit (h=0) ---\n",
        "    # Pre-allocate array for LP errors.\n",
        "    errors_lp = np.full_like(Y, np.nan)\n",
        "    endog_vars = bvar_setup['meta']['endogenous_vars']\n",
        "    shock_vars = [var for var in config['bvar_specification']['exogenous_variables'] if 'shock' in var]\n",
        "\n",
        "    # Loop through each endogenous variable to run its h=0 LP regression.\n",
        "    for i, var in enumerate(endog_vars):\n",
        "        # Prepare the specific y and X for this regression.\n",
        "        y_lp, X_lp = _prepare_lp_regression_data(\n",
        "            df=analysis_ready_df,\n",
        "            dependent_var=var,\n",
        "            shock_vars=shock_vars,\n",
        "            control_vars=endog_vars + shock_vars,\n",
        "            horizon=0,\n",
        "            lags=config['local_projection_specification']['lags_dependent_variable']\n",
        "        )\n",
        "        # Ensure alignment between the BVAR and LP samples.\n",
        "        common_index = y_lp.index.intersection(bvar_setup['valid_index'])\n",
        "        y_lp, X_lp = y_lp.loc[common_index], X_lp.loc[common_index]\n",
        "\n",
        "        # Fit the h=0 LP model via OLS.\n",
        "        model_lp = sm.OLS(y_lp, X_lp).fit()\n",
        "\n",
        "        # Calculate fitted values and errors.\n",
        "        y_fit_lp = model_lp.predict(X_lp)\n",
        "        errors_lp[np.isin(bvar_setup['valid_index'], common_index), i] = y_lp - y_fit_lp\n",
        "\n",
        "    # Calculate RMSE for the LP model, ignoring NaNs from any misalignment.\n",
        "    rmse_lp = np.sqrt(np.nanmean(errors_lp**2, axis=0))\n",
        "\n",
        "    # --- Diebold-Mariano Test for Model Comparison ---\n",
        "    dm_results = {}\n",
        "    for i, var in enumerate(endog_vars):\n",
        "        # Create a mask to select only the common, valid observations for both error series.\n",
        "        valid_mask = ~np.isnan(errors_lp[:, i])\n",
        "        # Perform the test on the aligned error vectors.\n",
        "        dm_stat, p_val = _perform_diebold_mariano_test(errors_bvar[valid_mask, i], errors_lp[valid_mask, i])\n",
        "        dm_results[var] = p_val\n",
        "\n",
        "    # --- Assemble Final Report ---\n",
        "    # Create a DataFrame to hold the comparative metrics.\n",
        "    report = pd.DataFrame({\n",
        "        'BVAR_RMSE': rmse_bvar,\n",
        "        'LP_RMSE': rmse_lp,\n",
        "        'DM_Test_p_value': pd.Series(dm_results)\n",
        "    }, index=endog_vars)\n",
        "\n",
        "    # Add the scalar log marginal likelihood to the report.\n",
        "    report['BVAR_LogMarginalLikelihood'] = lml_bvar\n",
        "\n",
        "    return report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 14, STEP 2: OUT-OF-SAMPLE VALIDATION HELPERS\n",
        "# =============================================================================\n",
        "\n",
        "def _generate_bvar_h_step_forecast(\n",
        "    train_df: pd.DataFrame,\n",
        "    B_mean: np.ndarray,\n",
        "    bvar_meta: Dict[str, Any],\n",
        "    config: Dict[str, Any],\n",
        "    h: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates a rigorous h-step ahead forecast from an estimated BVAR model.\n",
        "\n",
        "    Purpose:\n",
        "        This function performs a true multi-step forecast by iterating the\n",
        "        VAR equations forward in time. It is a critical component for any\n",
        "        out-of-sample evaluation, simulating the generation of a forecast for\n",
        "        a future period `t+h` using only information available at time `t`.\n",
        "\n",
        "    Process:\n",
        "        1.  **Initialize History**: Extracts the last `p` observations of the\n",
        "            endogenous variables from the training data, which are needed to\n",
        "            start the forecast.\n",
        "        2.  **Project Exogenous Path**: Critically, it projects the future path\n",
        "            of all deterministic exogenous variables.\n",
        "            - **Shocks**: Future shocks are assumed to be their expected value, zero.\n",
        "            - **Trend**: The time trend is projected linearly.\n",
        "            - **Dummies**: The value of dummy variables (e.g., COVID dummy) is\n",
        "              determined for each future date based on the model configuration.\n",
        "        3.  **Iterate Forward**: Loops `h` times to generate the forecast path.\n",
        "            - In each step `s` from 1 to `h`, it constructs the full regressor\n",
        "              vector `X_{t+s}` for the next forecast.\n",
        "            - The lagged endogenous values in `X_{t+s}` are taken from the\n",
        "              history matrix, which contains actual data and previous forecasts.\n",
        "            - The exogenous values are taken from the pre-computed projected path.\n",
        "            - It calculates the one-step-ahead forecast `y_hat_{t+s}`.\n",
        "            - It updates the history matrix by dropping the oldest observation\n",
        "              and appending the new forecast, for use in the next iteration.\n",
        "        4.  **Return Final Forecast**: After `h` iterations, it returns the final\n",
        "            forecast for horizon `h`, which is `y_hat_{t+h}`.\n",
        "\n",
        "    Args:\n",
        "        train_df (pd.DataFrame): The training data up to time `t`.\n",
        "        B_mean (np.ndarray): The (M x K) posterior mean coefficient matrix.\n",
        "        bvar_meta (Dict[str, Any]): Metadata about the BVAR model, including\n",
        "                                   variable names, dimensions, and lag order.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary, needed\n",
        "                                 for dummy variable definitions.\n",
        "        h (int): The forecast horizon.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D array of length K containing the h-step-ahead forecast.\n",
        "    \"\"\"\n",
        "    # --- 1. Initialization and Parameter Extraction ---\n",
        "    # Extract model dimensions and variable names from metadata.\n",
        "    p: int = bvar_meta['lags']\n",
        "    K: int = bvar_meta['K']\n",
        "    M: int = bvar_meta['M']\n",
        "    endog_vars: List[str] = bvar_meta['endogenous_vars']\n",
        "\n",
        "    # The full list of regressors in the correct order.\n",
        "    # The last element is 'const'.\n",
        "    full_regressor_list: List[str] = bvar_meta['regressor_cols']\n",
        "\n",
        "    # Extract the last `p` observations of endogenous variables needed to start the forecast.\n",
        "    Y_hist: np.ndarray = train_df[endog_vars].values[-p:]\n",
        "\n",
        "    # --- 2. Project the Future Path of Exogenous Variables ---\n",
        "    # Pre-allocate a matrix to hold the projected path.\n",
        "    future_exog: np.ndarray = np.zeros((h, M))\n",
        "\n",
        "    # Get the last timestamp from the training data to project dates forward.\n",
        "    last_date: pd.Timestamp = train_df.index[-1]\n",
        "\n",
        "    # Get the last known value of the trend.\n",
        "    last_trend: float = train_df['trend'].iloc[-1] if 'trend' in train_df.columns else 0\n",
        "\n",
        "    # Get the COVID period definition from the configuration.\n",
        "    covid_period = config['bvar_specification']['covid_period']\n",
        "    covid_start = pd.to_datetime(covid_period['start_date'])\n",
        "    covid_end = pd.to_datetime(covid_period['end_date'])\n",
        "\n",
        "    # Populate the future exogenous matrix step-by-step.\n",
        "    for s in range(1, h + 1):\n",
        "        # The index for the current forecast step (s-1 for 0-based indexing).\n",
        "        step_idx = s - 1\n",
        "\n",
        "        # Project each exogenous variable.\n",
        "        for i, var_name in enumerate(full_regressor_list):\n",
        "            if 'shock' in var_name:\n",
        "                # Future shocks are their expectation: zero.\n",
        "                future_exog[step_idx, i] = 0.0\n",
        "            elif 'trend' == var_name:\n",
        "                # The trend increases by 1 each period.\n",
        "                future_exog[step_idx, i] = last_trend + s\n",
        "            elif 'dummy' in var_name:\n",
        "                # Project the dummy based on the future date.\n",
        "                future_date = last_date + pd.DateOffset(months=s)\n",
        "                is_in_covid_period = (future_date >= covid_start) and (future_date <= covid_end)\n",
        "                future_exog[step_idx, i] = 1.0 if is_in_covid_period else 0.0\n",
        "            elif 'const' == var_name:\n",
        "                # The constant is always 1.\n",
        "                future_exog[step_idx, i] = 1.0\n",
        "\n",
        "    # --- 3. Iterate Forward to Generate Forecast Path ---\n",
        "    # Pre-allocate array for the sequence of forecasts.\n",
        "    forecasts: np.ndarray = np.zeros((h, K))\n",
        "\n",
        "    # This is the core iterative loop.\n",
        "    for i in range(h):\n",
        "        # Construct the full regressor vector X_{t+i+1} for the next forecast.\n",
        "        X_t_plus_i: np.ndarray = np.zeros(M)\n",
        "\n",
        "        # a. Fill in the lagged endogenous variables from the history matrix.\n",
        "        # The history matrix is ordered [y_{t-p+1}, ..., y_t]. We flatten it.\n",
        "        # The order 'F' (column-major) is crucial to match the BVAR setup.\n",
        "        lagged_endog_values = Y_hist.flatten(order='F')\n",
        "\n",
        "        # Find the indices corresponding to the lagged variables.\n",
        "        lag_indices = [idx for idx, name in enumerate(full_regressor_list) if '_L' in name]\n",
        "        X_t_plus_i[lag_indices] = lagged_endog_values\n",
        "\n",
        "        # b. Fill in the exogenous and deterministic variables from the projected path.\n",
        "        exog_indices = [idx for idx, name in enumerate(full_regressor_list) if '_L' not in name]\n",
        "        X_t_plus_i[exog_indices] = future_exog[i, exog_indices]\n",
        "\n",
        "        # Generate the one-step-ahead forecast: y_hat = X * B\n",
        "        y_hat: np.ndarray = X_t_plus_i @ B_mean\n",
        "\n",
        "        # Store the forecast for this step.\n",
        "        forecasts[i, :] = y_hat\n",
        "\n",
        "        # Update the history matrix for the next iteration: drop the oldest\n",
        "        # observation and append the newest forecast.\n",
        "        Y_hist = np.vstack([Y_hist[1:], y_hat])\n",
        "\n",
        "    # --- 4. Return the Final h-step-ahead Forecast ---\n",
        "    # The final forecast is the last one generated in the loop.\n",
        "    return forecasts[h-1, :]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPLEMENTATION OF `_run_single_forecast_iteration` (Task 14)\n",
        "# =============================================================================\n",
        "\n",
        "def _run_single_forecast_iteration(\n",
        "    t: int,\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    forecast_horizons: List[int]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs one complete step of the recursive forecasting exercise.\n",
        "\n",
        "    Purpose:\n",
        "        This function is the core worker for the parallelized out-of-sample\n",
        "        validation. For a single forecast origin `t`, it re-estimates both the\n",
        "        BVAR and LP models on all data available up to `t`, generates forecasts\n",
        "        for all specified horizons, and returns the calculated forecast errors.\n",
        "\n",
        "    Process:\n",
        "        1.  **Slice Data**: Creates the training dataset using all data up to time `t`.\n",
        "        2.  **Re-estimate BVAR**: Runs the entire BVAR pipeline (setup and Gibbs\n",
        "            sampling) on the training data to get a new posterior mean `B_mean_t`.\n",
        "        3.  **Re-estimate LP**: Runs the entire LP estimation pipeline on the\n",
        "            training data to get new horizon-specific coefficients.\n",
        "        4.  **Forecast**: For each endogenous variable and each horizon `h`:\n",
        "            - Generates the h-step-ahead BVAR forecast using the iterative method.\n",
        "            - Generates the h-step-ahead LP forecast using the direct method.\n",
        "            - Retrieves the actual outcome `y_{t+h}`.\n",
        "            - Calculates and stores the forecast errors for both models in a\n",
        "              \"tidy\" format (one row per observation).\n",
        "\n",
        "    Args:\n",
        "        t (int): The forecast origin time index (integer location).\n",
        "        analysis_ready_df (pd.DataFrame): The full dataset.\n",
        "        config (Dict[str, Any]): The main study configuration.\n",
        "        forecast_horizons (List[int]): The horizons to forecast.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Any]]: A list of dictionaries, each representing a single\n",
        "                              forecast error observation.\n",
        "    \"\"\"\n",
        "    # --- 1. Slice Data ---\n",
        "    # Define the training data for this iteration.\n",
        "    train_df: pd.DataFrame = analysis_ready_df.iloc[:t]\n",
        "\n",
        "    # --- 2. Re-estimate BVAR ---\n",
        "    # Run the full BVAR pipeline on the current training data slice.\n",
        "    bvar_setup_t: Dict[str, Any] = run_phase4_task9_bvar_setup(train_df, config)\n",
        "    posterior_draws_t: Dict[str, np.ndarray] = run_bvar_gibbs_sampler(bvar_setup_t, config)\n",
        "    B_mean_t: np.ndarray = np.mean(posterior_draws_t['B_draws'], axis=0)\n",
        "\n",
        "    # --- 3. Re-estimate LP ---\n",
        "    # Run the full LP pipeline on the current training data slice.\n",
        "    lp_results_t: Dict[str, np.ndarray] = run_phase4_task11_local_projections(train_df, config)\n",
        "\n",
        "    # --- 4. Generate Forecasts and Errors ---\n",
        "    # A list to store the tidy error records calculated in this iteration.\n",
        "    iteration_errors: List[Dict[str, Any]] = []\n",
        "    endog_vars: List[str] = config['bvar_specification']['endogenous_variables']\n",
        "\n",
        "    # Loop through each required forecast horizon.\n",
        "    for h in forecast_horizons:\n",
        "        # Ensure the actual outcome is within the dataset bounds.\n",
        "        if t + h > len(analysis_ready_df):\n",
        "            continue\n",
        "\n",
        "        # Get the true values that we are trying to forecast.\n",
        "        actual_values: np.ndarray = analysis_ready_df[endog_vars].iloc[t + h - 1].values\n",
        "\n",
        "        # --- BVAR Forecast ---\n",
        "        # Generate the h-step-ahead BVAR forecast using the corrected helper.\n",
        "        y_hat_bvar: np.ndarray = _generate_bvar_h_step_forecast(train_df, B_mean_t, bvar_setup_t['meta'], config, h)\n",
        "        error_bvar: np.ndarray = actual_values - y_hat_bvar\n",
        "\n",
        "        # --- LP Forecast ---\n",
        "        # The LP forecast is direct, using the coefficients for horizon `h`.\n",
        "        # First, construct the X_t vector from the last row of the training data.\n",
        "        _, X_lp_t = _prepare_lp_regression_data(\n",
        "            df=train_df,\n",
        "            dependent_var=endog_vars[0], # Dependent var doesn't matter for X construction\n",
        "            shock_vars=[var for var in config['bvar_specification']['exogenous_variables'] if 'shock' in var],\n",
        "            control_vars=endog_vars + [var for var in config['bvar_specification']['exogenous_variables'] if 'shock' in var],\n",
        "            horizon=h-1, # Use h-1 to get regressors at time t for forecast at t+h\n",
        "            lags=config['local_projection_specification']['lags_dependent_variable']\n",
        "        )\n",
        "        # Extract the last row of the constructed X matrix, which corresponds to time t.\n",
        "        X_t_vector = X_lp_t.iloc[-1].values\n",
        "\n",
        "        # Pre-allocate forecast and error vectors for the LP model.\n",
        "        y_hat_lp = np.zeros(len(endog_vars))\n",
        "\n",
        "        # For each variable, get its specific h-step-ahead coefficients and predict.\n",
        "        for i, var in enumerate(endog_vars):\n",
        "            # The LP IRF is the coefficient on the shock. The forecast needs all coefficients.\n",
        "            # This requires re-fitting the model to get all coefficients.\n",
        "            y_lp_h, X_lp_h = _prepare_lp_regression_data(train_df, var, [], [], h-1, config['local_projection_specification']['lags_dependent_variable'])\n",
        "            lp_model_h = sm.OLS(y_lp_h, X_lp_h).fit()\n",
        "            y_hat_lp[i] = lp_model_h.predict(X_lp_h.iloc[-1])\n",
        "\n",
        "        error_lp: np.ndarray = actual_values - y_hat_lp\n",
        "\n",
        "        # --- Store errors in tidy format ---\n",
        "        for i, var_name in enumerate(endog_vars):\n",
        "            iteration_errors.append({\n",
        "                't': t, 'h': h, 'variable_name': var_name, 'model': 'BVAR', 'error': error_bvar[i]\n",
        "            })\n",
        "            iteration_errors.append({\n",
        "                't': t, 'h': h, 'variable_name': var_name, 'model': 'LP', 'error': error_lp[i]\n",
        "            })\n",
        "\n",
        "    return iteration_errors\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPLEMENTATION OF `evaluate_forecasting_performance_full` (Task 14)\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_forecasting_performance_full(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    validation_split_ratio: float = 0.7,\n",
        "    forecast_horizons: List[int] = [1, 6, 12],\n",
        "    n_jobs: int = -1\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluates out-of-sample forecasting performance using a full recursive scheme.\n",
        "\n",
        "    Purpose:\n",
        "        This function provides a rigorous out-of-sample test of the models'\n",
        "        forecasting capabilities. It simulates how the models would have\n",
        "        performed in real-time by re-estimating them at each point in the\n",
        "        evaluation period. This process is computationally intensive and is\n",
        "        parallelized to run efficiently on multi-core systems.\n",
        "\n",
        "    Process:\n",
        "        1.  **Split Data**: Divides the data into an initial training set and an\n",
        "            evaluation set.\n",
        "        2.  **Parallel Loop**: It iterates through each time step in the\n",
        "            evaluation period. This loop is parallelized using `joblib`, where\n",
        "            each time step `t` is a separate job.\n",
        "        3.  **Worker Function**: Each parallel job calls the\n",
        "            `_run_single_forecast_iteration` function, which performs the\n",
        "            computationally heavy task of re-estimating both BVAR and LP models\n",
        "            and computing forecast errors for that single time step.\n",
        "        4.  **Aggregate Results**: After all parallel jobs are complete, it\n",
        "            collects the forecast errors from all time steps.\n",
        "        5.  **Calculate Metrics**: It converts the list of error records into a\n",
        "            DataFrame and calculates the overall Root Mean Squared Error (RMSE)\n",
        "            and Mean Absolute Error (MAE) for each model, variable, and forecast\n",
        "            horizon.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The full analysis-ready dataset.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        validation_split_ratio (float): The proportion of data for the initial\n",
        "                                        training window.\n",
        "        forecast_horizons (List[int]): Horizons to evaluate.\n",
        "        n_jobs (int): The number of CPU cores to use for parallelization.\n",
        "                      -1 means use all available cores.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing forecast accuracy (RMSE, MAE),\n",
        "                      multi-indexed by variable, horizon, and model.\n",
        "    \"\"\"\n",
        "    # --- 1. Split Data ---\n",
        "    # Determine the number of observations and the split point for the evaluation.\n",
        "    n_obs: int = len(analysis_ready_df)\n",
        "    n_initial: int = int(n_obs * validation_split_ratio)\n",
        "\n",
        "    # --- 2. Parallel Loop ---\n",
        "    # Use joblib.Parallel to run the forecast iterations in parallel.\n",
        "    # This significantly speeds up the computationally expensive recursive scheme.\n",
        "    # `delayed` is used to create a lightweight \"promise\" of the function call.\n",
        "    results_list: List[List[Dict[str, Any]]] = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(_run_single_forecast_iteration)(\n",
        "            t, analysis_ready_df, config, forecast_horizons\n",
        "        ) for t in tqdm(range(n_initial, n_obs - max(forecast_horizons)), desc=\"Recursive OOS Forecasting\")\n",
        "    )\n",
        "\n",
        "    # --- 3. Aggregate Results ---\n",
        "    # The result is a list of lists; flatten it into a single list of dictionaries.\n",
        "    flat_results: List[Dict[str, Any]] = [item for sublist in results_list for item in sublist]\n",
        "\n",
        "    # If no results were generated, return an empty DataFrame.\n",
        "    if not flat_results:\n",
        "        return pd.DataFrame(columns=['RMSE', 'MAE'])\n",
        "\n",
        "    # Convert the list of tidy error records into a DataFrame.\n",
        "    error_df = pd.DataFrame.from_records(flat_results)\n",
        "\n",
        "    # --- 4. Calculate Metrics ---\n",
        "    # Define the aggregation functions for RMSE and MAE.\n",
        "    agg_funcs = {\n",
        "        'RMSE': ('error', lambda x: np.sqrt(np.mean(x**2))),\n",
        "        'MAE': ('error', lambda x: np.mean(np.abs(x)))\n",
        "    }\n",
        "\n",
        "    # Group by the dimensions of interest and apply the aggregation functions.\n",
        "    summary_report = error_df.groupby(['variable_name', 'h', 'model']).agg(**agg_funcs)\n",
        "\n",
        "    # Return the final, clean summary report.\n",
        "    return summary_report\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 14, ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def run_full_model_validation_suite(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    lag_specs_to_test: List[int] = [1, 2, 3, 4, 5, 6]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full suite of validation, comparison, and diagnostics.\n",
        "\n",
        "    Purpose:\n",
        "        This is the master function for Task 14. It executes all three major\n",
        "        validation components: in-sample fit assessment, a full-scale out-of-\n",
        "        sample forecasting horse race, and model diagnostics including lag\n",
        "        selection.\n",
        "\n",
        "    Process:\n",
        "        1.  **Full-Sample Estimation**: It first runs the BVAR and LP estimations\n",
        "            on the entire dataset. These full-sample results are used for the\n",
        "            in-sample fit and residual diagnostic checks.\n",
        "        2.  **In-Sample Assessment**: Calls `assess_in_sample_fit` to compute\n",
        "            RMSE, LML, and Diebold-Mariano test statistics.\n",
        "        3.  **Out-of-Sample Assessment**: Calls `evaluate_forecasting_performance_full`\n",
        "            to run the computationally intensive recursive forecasting exercise.\n",
        "        4.  **Lag Selection**: It iterates through a list of possible lag lengths,\n",
        "            re-estimating the BVAR for each, and calculates AIC and BIC to help\n",
        "            determine the optimal lag order.\n",
        "        5.  **Residual Diagnostics**: It runs a standard battery of tests\n",
        "            (Ljung-Box, ARCH-LM, Jarque-Bera) on the residuals of the full-sample\n",
        "            baseline BVAR model.\n",
        "        6.  **Compile Reports**: Gathers all results into a final, comprehensive\n",
        "            dictionary of report DataFrames.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The full analysis-ready dataset.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        lag_specs_to_test (List[int]): A list of lag lengths (p) to evaluate\n",
        "                                       for the BVAR model.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary of comprehensive validation reports.\n",
        "    \"\"\"\n",
        "    # --- Full-Sample Estimation (Baseline for In-Sample and Diagnostics) ---\n",
        "    bvar_setup: Dict[str, Any] = run_phase4_task9_bvar_setup(analysis_ready_df, config)\n",
        "    posterior_draws: Dict[str, np.ndarray] = run_bvar_gibbs_sampler(bvar_setup, config)\n",
        "\n",
        "    # --- Step 1: In-Sample Fit Assessment ---\n",
        "    in_sample_report: pd.DataFrame = assess_in_sample_fit(\n",
        "        analysis_ready_df, bvar_setup, posterior_draws, config\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Out-of-Sample Validation ---\n",
        "    out_of_sample_report: pd.DataFrame = evaluate_forecasting_performance_full(\n",
        "        analysis_ready_df, config\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Lag Selection and Residual Diagnostics ---\n",
        "    info_criteria: List[Dict[str, Any]] = []\n",
        "    for p in tqdm(lag_specs_to_test, desc=\"Testing Lag Specifications\"):\n",
        "        # Create a temporary config with the new lag length.\n",
        "        temp_config = config.copy()\n",
        "        temp_config['bvar_specification'] = config['bvar_specification'].copy()\n",
        "        temp_config['bvar_specification']['lags'] = p\n",
        "        try:\n",
        "            # Re-run the BVAR pipeline for this lag length.\n",
        "            bvar_setup_p = run_phase4_task9_bvar_setup(analysis_ready_df, temp_config)\n",
        "            posterior_draws_p = run_bvar_gibbs_sampler(bvar_setup_p, temp_config)\n",
        "\n",
        "            # Calculate AIC and BIC using the posterior mean estimates.\n",
        "            Y_p, X_p = bvar_setup_p['Y'], bvar_setup_p['X']\n",
        "            B_mean_p = np.mean(posterior_draws_p['B_draws'], axis=0)\n",
        "            Sigma_mean_p = np.mean(posterior_draws_p['Sigma_draws'], axis=0)\n",
        "            residuals_p = Y_p - X_p @ B_mean_p\n",
        "\n",
        "            log_lik_p = np.sum(multivariate_normal.logpdf(residuals_p, np.zeros(Y_p.shape[1]), Sigma_mean_p))\n",
        "            n_params_p = B_mean_p.size\n",
        "            T_p = Y_p.shape[0]\n",
        "\n",
        "            aic = -2 * log_lik_p + 2 * n_params_p\n",
        "            bic = -2 * log_lik_p + np.log(T_p) * n_params_p\n",
        "\n",
        "            info_criteria.append({'lags': p, 'AIC': aic, 'BIC': bic})\n",
        "        except Exception:\n",
        "            # If estimation fails for a given lag, record as NaN.\n",
        "            info_criteria.append({'lags': p, 'AIC': np.nan, 'BIC': np.nan})\n",
        "\n",
        "    lag_selection_report: pd.DataFrame = pd.DataFrame(info_criteria).set_index('lags')\n",
        "\n",
        "    # Run residual diagnostics on the main, full-sample model.\n",
        "    residual_report: pd.DataFrame = run_model_diagnostics(bvar_setup, posterior_draws)['residual_diagnostics']\n",
        "\n",
        "    # --- Compile Final Reports ---\n",
        "    return {\n",
        "        \"in_sample_fit_report\": in_sample_report,\n",
        "        \"out_of_sample_forecast_report\": out_of_sample_report,\n",
        "        \"lag_selection_report\": lag_selection_report,\n",
        "        \"residual_diagnostics_report\": residual_report\n",
        "    }\n"
      ],
      "metadata": {
        "id": "67xN3vWSoPoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: End-to-End Research Pipeline Orchestrator\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 15: END-TO-END RESEARCH PIPELINE ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def run_transatlantic_spillovers_analysis(\n",
        "    equity_tick_df: pd.DataFrame,\n",
        "    rate_tick_df: pd.DataFrame,\n",
        "    macro_df: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    target_market: str,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the study.\n",
        "\n",
        "    Purpose:\n",
        "        This master orchestrator function serves as the single entry point to\n",
        "        run the entire research project. It ensures a reproducible, robust, and\n",
        "        logically sound execution of all analytical phases, from raw data\n",
        "        ingestion to final model validation, encapsulating the full workflow.\n",
        "\n",
        "    Process:\n",
        "        The pipeline proceeds through a series of well-defined phases with a\n",
        "        strict separation of concerns:\n",
        "        1.  **Setup**: Initializes logging and a master results dictionary to\n",
        "            store all artifacts from the run.\n",
        "        2.  **Phase I (Data Ingestion & Cleaning)**:\n",
        "            - Validates all inputs, configurations, and schemas.\n",
        "            - Performs a deep data quality assessment to detect anomalies.\n",
        "            - Cleanses and prepares all raw data into consistently structured\n",
        "              DataFrames, with macro data prepared in its original levels.\n",
        "        3.  **Phase II (Shock Identification)**:\n",
        "            - Operates on the clean high-frequency data to identify structural shocks\n",
        "              using the rotational decomposition method.\n",
        "        4.  **Phase III (Model Preparation)**:\n",
        "            - Aggregates the high-frequency shocks to a monthly frequency.\n",
        "            - Applies model-specific numerical transformations (e.g., logs) to the\n",
        "              clean macro data.\n",
        "            - Constructs deterministic control variables (trend, dummies).\n",
        "            - Assembles all components into the final, analysis-ready dataset.\n",
        "        5.  **Phase IV (Econometric Estimation)**:\n",
        "            - Estimates the primary BVAR model via Gibbs sampling.\n",
        "            - Estimates the robust Local Projections model as a cross-check.\n",
        "        6.  **Phase V (Results & Validation)**:\n",
        "            - Calculates impulse response functions from the BVAR posterior.\n",
        "            - Performs a full suite of in-sample and out-of-sample validation tests.\n",
        "        7.  **Finalization**: Compiles all intermediate data, final results, and\n",
        "            validation reports into a single, comprehensive output dictionary,\n",
        "            and handles any exceptions gracefully.\n",
        "\n",
        "    Args:\n",
        "        equity_tick_df (pd.DataFrame): Raw high-frequency equity tick data.\n",
        "        rate_tick_df (pd.DataFrame): Raw high-frequency interest rate tick data.\n",
        "        macro_df (pd.DataFrame): Raw long-format macroeconomic time series data.\n",
        "        announcement_df (pd.DataFrame): Raw central bank announcement metadata.\n",
        "        target_market (str): The market to study (e.g., 'CAN').\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive, nested dictionary containing all inputs,\n",
        "                        intermediate data products, final results, and validation\n",
        "                        reports generated during the pipeline run.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup: Logging and Master Results Dictionary ---\n",
        "    # Configure basic logging to provide a detailed execution trace for the user.\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Initialize the master dictionary to store all artifacts of the run.\n",
        "    # This structure ensures a clean, organized, and auditable output.\n",
        "    master_results: Dict[str, Any] = {\n",
        "        \"metadata\": {\"run_start_utc\": pd.Timestamp.now(tz='UTC'), \"target_market\": target_market},\n",
        "        \"inputs\": {\"config\": study_config}, \"phase_1_cleaning\": {}, \"phase_2_identification\": {},\n",
        "        \"phase_3_model_prep\": {}, \"phase_4_estimation\": {}, \"phase_5_results\": {}\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Record the start time for performance monitoring.\n",
        "        start_time = time.time()\n",
        "        logging.info(\"=== STARTING END-TO-END RESEARCH PIPELINE ===\")\n",
        "\n",
        "        # --- 2. Phase I: Data Ingestion & Cleaning ---\n",
        "        logging.info(\"--- Phase 1: Validating, Assessing, and Cleaning Data ---\")\n",
        "\n",
        "        # Task 1: Validate all inputs, schemas, and configurations. This is the first quality gate.\n",
        "        validation_reports = run_phase1_task1_validation(\n",
        "            equity_tick_df, rate_tick_df, macro_df, announcement_df, target_market, study_config\n",
        "        )\n",
        "        master_results['phase_1_cleaning']['validation_reports'] = validation_reports\n",
        "        logging.info(\"Step 1.1: All inputs and configurations validated successfully.\")\n",
        "\n",
        "        # Task 2: Perform deep data quality assessment to detect anomalies.\n",
        "        quality_reports = run_phase1_task2_quality_assessment(\n",
        "            equity_tick_df, rate_tick_df, announcement_df, macro_df\n",
        "        )\n",
        "        master_results['phase_1_cleaning']['quality_assessment_reports'] = quality_reports\n",
        "        logging.info(\"Step 1.2: Data quality assessment complete.\")\n",
        "\n",
        "        # Task 3: Cleanse and preprocess all data sources based on the validation and QA.\n",
        "        preprocessed_data = run_phase1_task3_preprocessing(\n",
        "            raw_equity_df=equity_tick_df, raw_rate_df=rate_tick_df, raw_announcement_df=announcement_df,\n",
        "            raw_macro_df=macro_df, equity_anomaly_flags=quality_reports['equity_anomaly_flags'],\n",
        "            rate_anomaly_flags=quality_reports['rate_anomaly_flags'], target_market=target_market\n",
        "        )\n",
        "        master_results['phase_1_cleaning']['cleaned_data'] = preprocessed_data\n",
        "        logging.info(\"Step 1.3: Data cleansing and initial preparation complete.\")\n",
        "\n",
        "        # --- 3. Phase II: High-Frequency Shock Identification ---\n",
        "        logging.info(\"--- Phase 2: Identifying Structural Shocks ---\")\n",
        "\n",
        "        # Task 4: Define event windows and extract prices from the clean tick data.\n",
        "        price_extraction = run_phase2_task4_data_extraction(\n",
        "            preprocessed_data['clean_announcement_df'],\n",
        "            preprocessed_data['clean_equity_df'],\n",
        "            preprocessed_data['clean_rate_df'],\n",
        "            study_config\n",
        "        )\n",
        "        master_results['phase_2_identification']['price_extraction'] = price_extraction\n",
        "        logging.info(\"Step 2.1: Event windows defined and prices extracted.\")\n",
        "\n",
        "        # Task 5: Calculate raw surprises from the extracted prices.\n",
        "        surprises = run_phase2_task5_surprise_calculation(\n",
        "            price_extraction['equity_price_extraction_results'],\n",
        "            price_extraction['rate_price_extraction_results'],\n",
        "            preprocessed_data['clean_announcement_df']\n",
        "        )\n",
        "        master_results['phase_2_identification']['raw_surprises'] = surprises\n",
        "        logging.info(\"Step 2.2: Raw surprises calculated.\")\n",
        "\n",
        "        # Task 6: Perform rotational decomposition to get structural shocks.\n",
        "        structural_shocks = run_phase2_task6_shock_identification(\n",
        "            surprises['surprise_vectors'], study_config\n",
        "        )\n",
        "        master_results['phase_2_identification']['structural_shocks'] = structural_shocks\n",
        "        logging.info(\"Step 2.3: Structural shocks identified successfully.\")\n",
        "\n",
        "        # --- 4. Phase III: Model Preparation (Refactored Logic) ---\n",
        "        logging.info(\"--- Phase 3: Transforming and Assembling Data for Monthly Models ---\")\n",
        "\n",
        "        # Task 7: Aggregate the event-level shocks to a monthly frequency.\n",
        "        monthly_data = run_phase3_task7_temporal_aggregation(\n",
        "            structural_shocks, preprocessed_data['clean_announcement_df']\n",
        "        )\n",
        "        master_results['phase_3_model_prep']['monthly_shocks'] = monthly_data\n",
        "        logging.info(\"Step 3.1: Shocks aggregated to monthly frequency.\")\n",
        "\n",
        "        # Task 8.1: Apply model-specific transformations to the clean macro data.\n",
        "        transformation_map = {var.replace(f\"{target_market}_\", \"\"): ('log' if 'log' in var else 'level') for var in study_config['bvar_specification']['endogenous_variables']}\n",
        "        transformed_macro_df = transform_core_macro_variables(\n",
        "            prepared_macro_df=preprocessed_data['prepared_macro_df_levels'],\n",
        "            transformation_map=transformation_map,\n",
        "            target_market=target_market\n",
        "        )\n",
        "        master_results['phase_3_model_prep']['transformed_macro_df'] = transformed_macro_df\n",
        "        logging.info(\"Step 3.2: Macroeconomic variables transformed.\")\n",
        "\n",
        "        # Task 8.2: Construct deterministic control variables.\n",
        "        control_variables_df = construct_control_variables(\n",
        "            time_index=transformed_macro_df.index,\n",
        "            config=study_config\n",
        "        )\n",
        "        master_results['phase_3_model_prep']['control_variables_df'] = control_variables_df\n",
        "        logging.info(\"Step 3.3: Control variables constructed.\")\n",
        "\n",
        "        # Task 8.3: Assemble the final, analysis-ready dataset.\n",
        "        analysis_ready_df = assemble_final_dataset(\n",
        "            transformed_macro_df=transformed_macro_df,\n",
        "            monthly_shocks_df=monthly_data['monthly_shocks_df'],\n",
        "            control_variables_df=control_variables_df\n",
        "        )\n",
        "        master_results['phase_3_model_prep']['analysis_ready_df'] = analysis_ready_df\n",
        "        logging.info(\"Step 3.4: Final analysis-ready dataset assembled.\")\n",
        "\n",
        "        # --- 5. Phase IV: Econometric Estimation ---\n",
        "        logging.info(\"--- Phase 4: Estimating Econometric Models ---\")\n",
        "\n",
        "        # Task 9: Set up the BVAR model matrices and priors.\n",
        "        bvar_setup = run_phase4_task9_bvar_setup(analysis_ready_df, study_config)\n",
        "        master_results['phase_4_estimation']['bvar_setup'] = bvar_setup\n",
        "        logging.info(\"Step 4.1: BVAR model specified.\")\n",
        "\n",
        "        # Task 10: Run the Gibbs sampler to estimate the BVAR.\n",
        "        bvar_posterior_draws = run_bvar_gibbs_sampler(bvar_setup, study_config)\n",
        "        master_results['phase_4_estimation']['bvar_posterior_draws'] = bvar_posterior_draws\n",
        "        logging.info(\"Step 4.2: BVAR estimated.\")\n",
        "\n",
        "        # Task 11: Estimate the Local Projections model for robustness.\n",
        "        lp_results = run_phase4_task11_local_projections(analysis_ready_df, study_config)\n",
        "        master_results['phase_4_estimation']['lp_results'] = lp_results\n",
        "        logging.info(\"Step 4.3: Local Projections estimated.\")\n",
        "\n",
        "        # --- 6. Phase V: Results Generation and Validation ---\n",
        "        logging.info(\"--- Phase 5: Generating Results and Validating Models ---\")\n",
        "\n",
        "        # Task 12: Calculate BVAR-based impulse responses from the posterior draws.\n",
        "        bvar_irfs = calculate_var_impulse_responses(bvar_posterior_draws, bvar_setup, study_config)\n",
        "        master_results['phase_5_results']['bvar_irfs'] = bvar_irfs\n",
        "        logging.info(\"Step 5.1: BVAR impulse responses calculated.\")\n",
        "\n",
        "        # Task 14: Run the full suite of model validation checks.\n",
        "        model_validation_suite = run_full_model_validation_suite(analysis_ready_df, study_config)\n",
        "        master_results['phase_5_results']['model_validation_reports'] = model_validation_suite\n",
        "        logging.info(\"Step 5.2: Model validation suite complete.\")\n",
        "\n",
        "        # --- 7. Finalization ---\n",
        "        # Record the end time and total runtime.\n",
        "        end_time = time.time()\n",
        "        master_results['metadata']['run_end_utc'] = pd.Timestamp.now(tz='UTC')\n",
        "        master_results['metadata']['total_runtime_seconds'] = round(end_time - start_time, 2)\n",
        "        master_results['metadata']['final_status'] = \"SUCCESS\"\n",
        "        logging.info(f\"=== PIPELINE COMPLETED SUCCESSFULLY in {master_results['metadata']['total_runtime_seconds']} seconds. ===\")\n",
        "\n",
        "    except (Exception) as e:\n",
        "        # A top-level exception handler to ensure the pipeline fails gracefully\n",
        "        # and provides a clear error message in the final results object.\n",
        "        end_time = time.time()\n",
        "        logging.error(f\"!!! PIPELINE FAILED: {type(e).__name__} - {e}\", exc_info=True)\n",
        "        master_results['metadata']['run_end_utc'] = pd.Timestamp.now(tz='UTC')\n",
        "        master_results['metadata']['total_runtime_seconds'] = round(end_time - start_time, 2)\n",
        "        master_results['metadata']['final_status'] = \"FAIL\"\n",
        "        master_results['metadata']['error_message'] = str(e)\n",
        "\n",
        "    # Return the final, comprehensive results object.\n",
        "    return master_results\n"
      ],
      "metadata": {
        "id": "raXdwh59pgsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Identification Robustness Analysis\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 16, STEP 1: POOR MAN'S SIGN RESTRICTION IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def identify_shocks_pmsr(\n",
        "    raw_surprises_df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Identifies structural shocks using the Poor Man's Sign Restriction (PMSR).\n",
        "\n",
        "    Purpose:\n",
        "        This function provides an alternative, simpler identification scheme for\n",
        "        robustness checks. It classifies each announcement as either a \"monetary\n",
        "        policy\" or \"information\" event based on the sign of the correlation\n",
        "        between the raw interest rate and equity surprises.\n",
        "\n",
        "    Process:\n",
        "        1.  **Group by Event**: The function operates on each announcement event individually.\n",
        "        2.  **Check Correlation Sign**: For each event, it checks if the product\n",
        "            of the rate and equity surprise is negative (`s_rate * s_equity < 0`).\n",
        "            This is a proxy for the required negative co-movement of a pure\n",
        "            monetary policy shock.\n",
        "        3.  **Assign Shocks**:\n",
        "            - If the co-movement is negative, the event is classified as a\n",
        "              monetary policy shock. The `shock_MP` is set to the raw rate\n",
        "              surprise, and `shock_INFO` is set to zero.\n",
        "            - If the co-movement is non-negative, the event is classified as an\n",
        "              information shock. The `shock_MP` is set to zero, and `shock_INFO`\n",
        "              is set to the raw rate surprise.\n",
        "        4.  **Normalize**: The resulting shock series for each central bank are\n",
        "            grouped and normalized to have a unit standard deviation, ensuring\n",
        "            comparability with the benchmark rotational decomposition shocks.\n",
        "\n",
        "    Args:\n",
        "        raw_surprises_df (pd.DataFrame): A DataFrame containing the raw surprises,\n",
        "            indexed by 'event_id', with columns 'rate_surprise_bps',\n",
        "            'equity_surprise_pct', and 'central_bank'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are central bank names\n",
        "        and values are DataFrames containing the identified and normalized\n",
        "        structural shocks, indexed by 'event_id'.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not all(col in raw_surprises_df.columns for col in ['rate_surprise_bps', 'equity_surprise_pct', 'central_bank']):\n",
        "        raise ValueError(\"Input DataFrame is missing required columns.\")\n",
        "\n",
        "    # --- 1. & 2. Classify Events based on Correlation Sign ---\n",
        "    # A negative product of surprises indicates negative co-movement.\n",
        "    is_mp_shock = (raw_surprises_df['rate_surprise_bps'] * raw_surprises_df['equity_surprise_pct']) < 0\n",
        "\n",
        "    # --- 3. Assign Shocks ---\n",
        "    # Create columns for the raw structural shocks.\n",
        "    shocks_df = pd.DataFrame(index=raw_surprises_df.index)\n",
        "\n",
        "    # Assign the rate surprise to the MP shock column if it's an MP event, else 0.\n",
        "    shocks_df['shock_MP'] = raw_surprises_df['rate_surprise_bps'].where(is_mp_shock, 0.0)\n",
        "\n",
        "    # Assign the rate surprise to the INFO shock column if it's an INFO event, else 0.\n",
        "    shocks_df['shock_INFO'] = raw_surprises_df['rate_surprise_bps'].where(~is_mp_shock, 0.0)\n",
        "\n",
        "    # Add the central bank column for grouping.\n",
        "    shocks_df['central_bank'] = raw_surprises_df['central_bank']\n",
        "\n",
        "    # --- 4. Normalize Shocks per Central Bank ---\n",
        "    # Calculate the standard deviation for each shock type within each central bank group.\n",
        "    # Use .transform() to broadcast the std dev back to the original shape for division.\n",
        "    std_devs = shocks_df.groupby('central_bank')[['shock_MP', 'shock_INFO']].transform('std')\n",
        "\n",
        "    # Avoid division by zero if a shock series has zero variance.\n",
        "    std_devs[std_devs < 1e-9] = 1.0\n",
        "\n",
        "    # Normalize the shock series.\n",
        "    shocks_df['shock_MP'] /= std_devs['shock_MP']\n",
        "    shocks_df['shock_INFO'] /= std_devs['shock_INFO']\n",
        "\n",
        "    # --- 5. Structure Output ---\n",
        "    # Split the final DataFrame into a dictionary keyed by central bank.\n",
        "    pmsr_results = {}\n",
        "    for bank, group in shocks_df.groupby('central_bank'):\n",
        "        # The output format must match the benchmark identification function's output.\n",
        "        pmsr_results[bank] = group[['shock_MP', 'shock_INFO']].rename(\n",
        "            columns={'shock_MP': f'shock_{bank}_MP', 'shock_INFO': f'shock_{bank}_INFO'}\n",
        "        )\n",
        "\n",
        "    return pmsr_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# REFACTORED CORE PIPELINE LOGIC\n",
        "# =============================================================================\n",
        "\n",
        "def _run_core_pipeline(\n",
        "    structural_shocks: Dict[str, pd.DataFrame],\n",
        "    preprocessed_data: Dict[str, Any],\n",
        "    analysis_ready_df_no_shocks: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Internal function that runs the main analysis pipeline from shock aggregation onwards.\n",
        "    This function is designed to be called by different orchestrators.\n",
        "    \"\"\"\n",
        "    # This dictionary will store the results of this specific pipeline run.\n",
        "    results = {}\n",
        "\n",
        "    # --- Phase III (cont.): Model Preparation ---\n",
        "    logging.info(\"--- (Core Pipeline) Phase 3: Aggregating Data for Monthly Models ---\")\n",
        "    monthly_data = run_phase3_task7_temporal_aggregation(\n",
        "        structural_shocks, preprocessed_data['clean_announcement_df']\n",
        "    )\n",
        "    results['monthly_shocks'] = monthly_data\n",
        "\n",
        "    # The original `analysis_ready_df` is passed in without shocks, so we join them here.\n",
        "    analysis_ready_df = analysis_ready_df_no_shocks.join(monthly_data['monthly_shocks_df'], how='inner')\n",
        "    results['analysis_ready_df'] = analysis_ready_df\n",
        "    logging.info(\"--- (Core Pipeline) Final analysis-ready dataset assembled. ---\")\n",
        "\n",
        "    # --- Phase IV: Econometric Estimation ---\n",
        "    logging.info(\"--- (Core Pipeline) Phase 4: Estimating Econometric Models ---\")\n",
        "    bvar_setup = run_phase4_task9_bvar_setup(analysis_ready_df, config)\n",
        "    results['bvar_setup'] = bvar_setup\n",
        "    bvar_posterior_draws = run_bvar_gibbs_sampler(bvar_setup, config)\n",
        "    results['bvar_posterior_draws'] = bvar_posterior_draws\n",
        "\n",
        "    lp_results = run_phase4_task11_local_projections(analysis_ready_df, config)\n",
        "    results['lp_results'] = lp_results\n",
        "    logging.info(\"--- (Core Pipeline) Models estimated. ---\")\n",
        "\n",
        "    # --- Phase V: Results Generation and Validation ---\n",
        "    logging.info(\"--- (Core Pipeline) Phase 5: Generating Results and Validating Models ---\")\n",
        "    bvar_irfs = calculate_var_impulse_responses(bvar_posterior_draws, bvar_setup, config)\n",
        "    results['bvar_irfs'] = bvar_irfs\n",
        "\n",
        "    model_validation_suite = run_full_model_validation_suite(analysis_ready_df, config)\n",
        "    results['model_validation_reports'] = model_validation_suite\n",
        "    logging.info(\"--- (Core Pipeline) Results generated and validated. ---\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORCHESTRATOR FOR THE ROBUSTNESS CHECK\n",
        "# =============================================================================\n",
        "\n",
        "def run_pmsr_robustness_analysis(\n",
        "    master_results_benchmark: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs the full analysis pipeline using the PMSR identification method.\n",
        "\n",
        "    Purpose:\n",
        "        This function serves as an orchestrator for the \"Poor Man's Sign\n",
        "        Restriction\" robustness check. It takes the results of a benchmark\n",
        "        run, extracts the necessary early-stage data, re-identifies shocks\n",
        "        using the PMSR method, and then runs the entire downstream analysis\n",
        "        pipeline on these new shocks for comparison.\n",
        "\n",
        "    Process:\n",
        "        1.  **Extract Data**: It retrieves the raw surprises and other essential\n",
        "            preprocessed data from a completed benchmark run's results object.\n",
        "        2.  **PMSR Identification**: It calls the `identify_shocks_pmsr` function\n",
        "            to generate the alternative set of structural shocks.\n",
        "        3.  **Execute Core Pipeline**: It calls the internal `_run_core_pipeline`\n",
        "            function, passing in the new PMSR shocks. This ensures that the\n",
        "            exact same aggregation, estimation, and validation logic is applied\n",
        "            as in the benchmark run.\n",
        "        4.  **Return Results**: It returns a new, complete results dictionary\n",
        "            for the PMSR robustness check.\n",
        "\n",
        "    Args:\n",
        "        master_results_benchmark (Dict[str, Any]): The full results dictionary\n",
        "            from a completed run of the main `run_transatlantic_spillovers_analysis`\n",
        "            pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive results dictionary for the PMSR analysis.\n",
        "    \"\"\"\n",
        "    logging.info(\"=== STARTING PMSR ROBUSTNESS ANALYSIS PIPELINE ===\")\n",
        "\n",
        "    # --- 1. Extract necessary data from the benchmark run ---\n",
        "    # This avoids re-running all the initial data cleaning steps.\n",
        "    try:\n",
        "        # Combine raw surprises into the required DataFrame format.\n",
        "        raw_surprises_df = pd.concat([\n",
        "            master_results_benchmark['phase_2_identification']['raw_surprises']['intermediate_results']['rate_surprises_raw'],\n",
        "            master_results_benchmark['phase_2_identification']['raw_surprises']['intermediate_results']['equity_surprises_raw']\n",
        "        ], axis=1).dropna()\n",
        "\n",
        "        # Add the central bank column for grouping.\n",
        "        ann_df = master_results_benchmark['phase_1_cleaning']['cleaned_data']['clean_announcement_df']\n",
        "        raw_surprises_df = raw_surprises_df.join(ann_df.set_index('event_id')['central_bank'])\n",
        "\n",
        "        preprocessed_data = master_results_benchmark['phase_1_cleaning']['cleaned_data']\n",
        "\n",
        "        # We need the analysis-ready data *before* the shocks were added.\n",
        "        analysis_ready_df_no_shocks = master_results_benchmark['phase_3_model_prep']['analysis_ready_df'].drop(\n",
        "            columns=[c for c in master_results_benchmark['phase_3_model_prep']['analysis_ready_df'].columns if 'shock' in c]\n",
        "        )\n",
        "\n",
        "        config = master_results_benchmark['inputs']['config']\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"The benchmark results object is missing a required key: {e}\")\n",
        "\n",
        "    # --- 2. PMSR Identification ---\n",
        "    # Generate the alternative structural shocks using the PMSR method.\n",
        "    logging.info(\"--- (PMSR) Identifying shocks using Poor Man's Sign Restriction... ---\")\n",
        "    pmsr_shocks = identify_shocks_pmsr(raw_surprises_df)\n",
        "\n",
        "    # --- 3. Execute Core Pipeline with PMSR shocks ---\n",
        "    # Call the refactored core logic with the new shocks.\n",
        "    pmsr_pipeline_results = _run_core_pipeline(\n",
        "        structural_shocks=pmsr_shocks,\n",
        "        preprocessed_data=preprocessed_data,\n",
        "        analysis_ready_df_no_shocks=analysis_ready_df_no_shocks,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- 4. Compile and Return Final Results ---\n",
        "    # Add the identification-specific results to the main dictionary.\n",
        "    pmsr_pipeline_results['pmsr_identified_shocks'] = pmsr_shocks\n",
        "    logging.info(\"=== PMSR ROBUSTNESS ANALYSIS COMPLETED SUCCESSFULLY ===\")\n",
        "\n",
        "    return pmsr_pipeline_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 16, STEP 2: ROTATIONAL UNCERTAINTY ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def _run_pipeline_for_single_angle(\n",
        "    angle_draw: float,\n",
        "    bank_name: str,\n",
        "    benchmark_results: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Worker function to run the analysis pipeline for one sampled rotation angle.\n",
        "\n",
        "    This function is designed to be called in parallel. It takes a single angle,\n",
        "    generates the corresponding structural shocks, and runs the entire downstream\n",
        "    BVAR estimation and IRF calculation, returning all posterior draws of the IRFs.\n",
        "\n",
        "    Args:\n",
        "        angle_draw (float): A single rotation angle sampled from the admissible set.\n",
        "        bank_name (str): The name of the central bank ('FED' or 'ECB').\n",
        "        benchmark_results (Dict[str, Any]): The full results object from the\n",
        "            initial benchmark run, used to access necessary data and configs.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A (num_mcmc_draws, H+1, K, S) array of all IRF posterior\n",
        "                    draws corresponding to this single identification angle.\n",
        "    \"\"\"\n",
        "    # --- 1. Extract necessary data from the benchmark run ---\n",
        "    # This avoids passing large DataFrames to each parallel worker.\n",
        "    config = benchmark_results['inputs']['config']\n",
        "    raw_surprises_df = pd.concat([\n",
        "        benchmark_results['phase_2_identification']['raw_surprises']['intermediate_results']['rate_surprises_raw'],\n",
        "        benchmark_results['phase_2_identification']['raw_surprises']['intermediate_results']['equity_surprises_raw']\n",
        "    ], axis=1).dropna()\n",
        "    preprocessed_data = benchmark_results['phase_1_cleaning']['cleaned_data']\n",
        "    analysis_ready_df_no_shocks = benchmark_results['phase_3_model_prep']['analysis_ready_df'].drop(\n",
        "        columns=[c for c in benchmark_results['phase_3_model_prep']['analysis_ready_df'].columns if 'shock' in c]\n",
        "    )\n",
        "\n",
        "    # --- 2. Generate Shocks for this Specific Angle ---\n",
        "    # We need to manually apply the rotation, normalization, and sign convention.\n",
        "    q_matrix = np.array([\n",
        "        [np.cos(angle_draw), -np.sin(angle_draw)],\n",
        "        [np.sin(angle_draw), np.cos(angle_draw)]\n",
        "    ])\n",
        "\n",
        "    surprises_array = raw_surprises_df.loc[raw_surprises_df['central_bank'] == bank_name, ['rate_surprise_bps', 'equity_surprise_pct']].values\n",
        "    raw_shocks = (q_matrix @ surprises_array.T).T\n",
        "\n",
        "    # Enforce sign convention\n",
        "    if np.corrcoef(raw_shocks[:, 0], surprises_array[:, 0])[0, 1] < 0:\n",
        "        raw_shocks *= -1\n",
        "\n",
        "    # Normalize\n",
        "    std_devs = raw_shocks.std(axis=0)\n",
        "    safe_std_devs = np.where(std_devs > 1e-9, std_devs, 1.0)\n",
        "    normalized_shocks = raw_shocks / safe_std_devs\n",
        "\n",
        "    # Create the shock DataFrame in the required format (indexed by event_id).\n",
        "    event_ids = raw_surprises_df[raw_surprises_df['central_bank'] == bank_name].index\n",
        "    shock_df = pd.DataFrame(\n",
        "        normalized_shocks,\n",
        "        index=event_ids,\n",
        "        columns=[f'shock_{bank_name}_MP', f'shock_{bank_name}_INFO']\n",
        "    )\n",
        "\n",
        "    # The input to the core pipeline is a dictionary.\n",
        "    # We assume the other bank's shocks are taken from the benchmark median run for simplicity.\n",
        "    other_bank = 'ECB' if bank_name == 'FED' else 'FED'\n",
        "    structural_shocks_for_run = {\n",
        "        bank_name: shock_df,\n",
        "        other_bank: benchmark_results['phase_2_identification']['structural_shocks'][other_bank]\n",
        "    }\n",
        "\n",
        "    # --- 3. Run the Core Pipeline ---\n",
        "    # This re-estimates the BVAR and calculates IRFs for this specific shock series.\n",
        "    pipeline_results = _run_core_pipeline(\n",
        "        structural_shocks=structural_shocks_for_run,\n",
        "        preprocessed_data=preprocessed_data,\n",
        "        analysis_ready_df_no_shocks=analysis_ready_df_no_shocks,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # --- 4. Return All IRF Draws ---\n",
        "    # We need the full posterior distribution of IRFs for this angle.\n",
        "    return pipeline_results['bvar_irfs']['all_irf_draws']\n",
        "\n",
        "\n",
        "def run_rotational_uncertainty_analysis(\n",
        "    benchmark_results: Dict[str, Any],\n",
        "    num_angle_draws: int = 999,\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Performs a robustness check on identification uncertainty.\n",
        "\n",
        "    Purpose:\n",
        "        This function quantifies the uncertainty in the final impulse responses\n",
        "        that arises from the choice of a specific rotation angle for identification.\n",
        "        It integrates over the set of all plausible identification schemes to\n",
        "        produce credible intervals that account for both identification and\n",
        "        parameter estimation uncertainty.\n",
        "\n",
        "    Process:\n",
        "        1.  **Extract Admissible Angles**: It retrieves the set of all admissible\n",
        "            rotation angles that were identified in the benchmark model run.\n",
        "        2.  **Sample Angles**: It draws a large number of angles (e.g., 999)\n",
        "            uniformly with replacement from this admissible set.\n",
        "        3.  **Parallel Re-estimation**: For each sampled angle, it runs the\n",
        "            entire downstream analysis pipeline (shock generation, aggregation,\n",
        "            BVAR estimation, IRF calculation) in parallel using `joblib`. Each\n",
        "            parallel job returns the full set of posterior draws of the IRFs\n",
        "            for its specific angle.\n",
        "        4.  **Aggregate All Draws**: It collects the IRF draws from all parallel\n",
        "            runs into a single, massive array. This array now represents the\n",
        "            joint distribution of IRFs over both parameter uncertainty and\n",
        "            identification uncertainty.\n",
        "        5.  **Summarize Total Uncertainty**: It calculates the median and\n",
        "            percentiles (e.g., 5th and 95th) of this combined set of draws to\n",
        "            produce the final point estimates and credible intervals that\n",
        "            incorporate total uncertainty.\n",
        "\n",
        "    Args:\n",
        "        benchmark_results (Dict[str, Any]): The full results dictionary from a\n",
        "            completed run of the main pipeline.\n",
        "        num_angle_draws (int): The number of rotation angles to sample for the\n",
        "                               uncertainty analysis.\n",
        "        n_jobs (int): The number of CPU/GPU cores to use for parallelization.\n",
        "                      -1 means use all available cores.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, np.ndarray]]: A dictionary where keys are central\n",
        "        bank names. Each inner dictionary contains the summary of the total\n",
        "        uncertainty IRFs ('irf_median', 'irf_lower', 'irf_upper').\n",
        "    \"\"\"\n",
        "    logging.info(\"=== STARTING ROTATIONAL UNCERTAINTY ANALYSIS PIPELINE ===\")\n",
        "\n",
        "    # --- 1. & 2. Extract and Sample Admissible Angles ---\n",
        "    final_results = {}\n",
        "    config = benchmark_results['inputs']['config']\n",
        "    ci_level = config['computation_settings']['impulse_response_functions']['credible_interval_level']\n",
        "\n",
        "    for bank in ['FED', 'ECB']:\n",
        "        logging.info(f\"--- Processing uncertainty for {bank} ---\")\n",
        "        admissible_angles = benchmark_results['phase_2_identification']['structural_shocks'][bank]['admissible_angles']\n",
        "\n",
        "        if len(admissible_angles) == 0:\n",
        "            logging.warning(f\"No admissible angles found for {bank}. Skipping uncertainty analysis.\")\n",
        "            continue\n",
        "\n",
        "        # Set seed for reproducible sampling of angles.\n",
        "        np.random.seed(config['computation_settings']['mcmc_sampler']['random_seed'])\n",
        "        angle_samples = np.random.choice(admissible_angles, size=num_angle_draws, replace=True)\n",
        "\n",
        "        # --- 3. Parallel Re-estimation ---\n",
        "        # Use joblib to run the pipeline for each angle in parallel.\n",
        "        all_irf_draws_list = Parallel(n_jobs=n_jobs)(\n",
        "            delayed(_run_pipeline_for_single_angle)(\n",
        "                angle, bank, benchmark_results\n",
        "            ) for angle in tqdm(angle_samples, desc=f\"Simulating IRFs for {bank}\")\n",
        "        )\n",
        "\n",
        "        # --- 4. Aggregate All Draws ---\n",
        "        # Concatenate the list of (num_mcmc, H, K, S) arrays into one large array.\n",
        "        # Shape: (num_angle_draws * num_mcmc_draws, H, K, S)\n",
        "        total_irf_draws = np.concatenate(all_irf_draws_list, axis=0)\n",
        "\n",
        "        # --- 5. Summarize Total Uncertainty ---\n",
        "        lower_p = (1 - ci_level) / 2 * 100\n",
        "        upper_p = (1 - (1 - ci_level) / 2) * 100\n",
        "\n",
        "        # Calculate median and percentiles across the combined 'draws' axis (axis=0).\n",
        "        irf_median = np.median(total_irf_draws, axis=0)\n",
        "        irf_lower = np.percentile(total_irf_draws, lower_p, axis=0)\n",
        "        irf_upper = np.percentile(total_irf_draws, upper_p, axis=0)\n",
        "\n",
        "        # Reshape for standard output format (K, S, H+1)\n",
        "        final_shape_order = (1, 2, 0)\n",
        "        final_results[bank] = {\n",
        "            'irf_median': irf_median.transpose(final_shape_order),\n",
        "            'irf_lower': irf_lower.transpose(final_shape_order),\n",
        "            'irf_upper': irf_upper.transpose(final_shape_order)\n",
        "        }\n",
        "        logging.info(f\"--- Uncertainty analysis for {bank} complete. ---\")\n",
        "\n",
        "    logging.info(\"=== ROTATIONAL UNCERTAINTY ANALYSIS COMPLETED SUCCESSFULLY ===\")\n",
        "    return final_results\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 16, STEP 3: ALTERNATIVE SAMPLE PERIOD ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def run_alternative_sample_analysis(\n",
        "    raw_equity_df: pd.DataFrame,\n",
        "    raw_rate_df: pd.DataFrame,\n",
        "    raw_macro_df: pd.DataFrame,\n",
        "    raw_announcement_df: pd.DataFrame,\n",
        "    target_market: str,\n",
        "    config: Dict[str, Any],\n",
        "    sample_definitions: Dict[str, Dict[str, str]]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs a robustness check by re-running the entire analysis on alternative sample periods.\n",
        "\n",
        "    Purpose:\n",
        "        This function assesses the stability of the model's findings over time\n",
        "        by systematically re-estimating the entire pipeline on different sub-samples\n",
        "        of the data. This is a critical test for parameter constancy and helps\n",
        "        determine if the results are driven by specific economic regimes (e.g.,\n",
        "        the post-2008 crisis period or the COVID-19 era).\n",
        "\n",
        "    Process:\n",
        "        1.  **Iterate Through Samples**: The function loops through a dictionary\n",
        "            of sample period definitions. Each definition provides a name (e.g.,\n",
        "            'pre_covid') and an end date.\n",
        "        2.  **Truncate All Data**: For each defined sample period, it creates\n",
        "            a complete, new set of input data by filtering all four raw DataFrames\n",
        "            (tick data, announcements, macro data) to include only observations\n",
        "            up to the specified end date. This is a critical step to ensure the\n",
        "            entire analysis, including the shock identification, is performed\n",
        "            consistently on the truncated sample.\n",
        "        3.  **Execute Full Pipeline**: It calls the main master orchestrator,\n",
        "            `run_transatlantic_spillovers_analysis`, passing in the truncated\n",
        "            set of data. This re-runs the entire end-to-end analysis from\n",
        "            scratch for the sub-sample.\n",
        "        4.  **Store Results**: The complete results object returned by the master\n",
        "            orchestrator for each sub-sample run is stored in a dictionary,\n",
        "            keyed by the sample period's name.\n",
        "\n",
        "    Args:\n",
        "        raw_equity_df (pd.DataFrame): The full raw high-frequency equity data.\n",
        "        raw_rate_df (pd.DataFrame): The full raw high-frequency rate data.\n",
        "        raw_macro_df (pd.DataFrame): The full raw macroeconomic time series data.\n",
        "        raw_announcement_df (pd.DataFrame): The full raw announcement metadata.\n",
        "        target_market (str): The market to study (e.g., 'CAN').\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        sample_definitions (Dict[str, Dict[str, str]]): A dictionary defining\n",
        "            the sample periods to analyze. Example:\n",
        "            {'pre_covid': {'end_date': '2020-02-29'},\n",
        "             'pre_gfc': {'end_date': '2007-08-31'}}\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary where keys are the names of the\n",
        "        sample periods. Each value is the complete master results object from\n",
        "        running the full pipeline on that data sub-sample.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # This dictionary will store the full results object for each sample run.\n",
        "    all_sample_results: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    # --- Main Loop: Iterate through each defined sample period ---\n",
        "    for sample_name, definition in sample_definitions.items():\n",
        "        logging.info(f\"=== STARTING ANALYSIS FOR SAMPLE PERIOD: '{sample_name}' (ends {definition['end_date']}) ===\")\n",
        "\n",
        "        # --- 1. Truncate All Raw Data Sources ---\n",
        "        # Convert the end date string to a timestamp for comparison.\n",
        "        end_date = pd.to_datetime(definition['end_date'])\n",
        "\n",
        "        # Filter the high-frequency equity data.\n",
        "        equity_df_sample = raw_equity_df[raw_equity_df['timestamp_micros_utc'].dt.date <= end_date.date()].copy()\n",
        "\n",
        "        # Filter the high-frequency rate data.\n",
        "        rate_df_sample = raw_rate_df[raw_rate_df['timestamp_micros_utc'].dt.date <= end_date.date()].copy()\n",
        "\n",
        "        # Filter the announcement metadata.\n",
        "        announcement_df_sample = raw_announcement_df[raw_announcement_df['announcement_date_local'] <= end_date].copy()\n",
        "\n",
        "        # Filter the macroeconomic data.\n",
        "        macro_df_sample = raw_macro_df[raw_macro_df['date'] <= end_date].copy()\n",
        "\n",
        "        # --- 2. Execute the Full End-to-End Pipeline on the Truncated Data ---\n",
        "        # A deep copy of the config is used to prevent any potential side effects\n",
        "        # if the pipeline were to modify it in place (which it shouldn't).\n",
        "        run_config = copy.deepcopy(config)\n",
        "\n",
        "        # Call the main master orchestrator with the filtered datasets.\n",
        "        sample_result_object = run_transatlantic_spillovers_analysis(\n",
        "            equity_tick_df=equity_df_sample,\n",
        "            rate_tick_df=rate_df_sample,\n",
        "            macro_df=macro_df_sample,\n",
        "            announcement_df=announcement_df_sample,\n",
        "            target_market=target_market,\n",
        "            study_config=run_config\n",
        "        )\n",
        "\n",
        "        # --- 3. Store the Results ---\n",
        "        # Store the entire results object for this sample period.\n",
        "        all_sample_results[sample_name] = sample_result_object\n",
        "\n",
        "        logging.info(f\"=== COMPLETED ANALYSIS FOR SAMPLE PERIOD: '{sample_name}' ===\")\n",
        "\n",
        "    return all_sample_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 16: IDENTIFICATION ROBUSTNESS ANALYSIS ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def run_identification_robustness_suite(\n",
        "    benchmark_results: Dict[str, Any],\n",
        "    raw_data: Dict[str, pd.DataFrame]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a full suite of robustness checks for the shock identification.\n",
        "\n",
        "    Purpose:\n",
        "        This master orchestrator for Task 16 executes a series of analyses to\n",
        "        test the robustness of the main findings to alternative identification\n",
        "        schemes and sample periods. It serves as a comprehensive validation of\n",
        "        the core econometric identification strategy.\n",
        "\n",
        "    Process:\n",
        "        The function sequentially executes the three main robustness checks,\n",
        "        using the results and data from the initial benchmark run as a starting point.\n",
        "        1.  **Poor Man's Sign Restriction (PMSR) Analysis**:\n",
        "            - It calls `run_pmsr_robustness_analysis`, which re-identifies shocks\n",
        "              using the simpler PMSR rule and re-runs the entire downstream\n",
        "              pipeline for comparison.\n",
        "        2.  **Rotational Uncertainty Analysis**:\n",
        "            - It calls `run_rotational_uncertainty_analysis`, which samples from\n",
        "              the set of all admissible rotation angles identified in the\n",
        "              benchmark run.\n",
        "            - For each sampled angle, it re-runs the pipeline in parallel to\n",
        "              compute credible intervals that incorporate both estimation and\n",
        "              identification uncertainty.\n",
        "        3.  **Alternative Sample Period Analysis**:\n",
        "            - It calls `run_alternative_sample_analysis`, which systematically\n",
        "              truncates the original raw datasets to different time periods\n",
        "              (e.g., pre-GFC, pre-COVID) and re-runs the entire end-to-end\n",
        "              pipeline for each sub-sample.\n",
        "\n",
        "    Args:\n",
        "        benchmark_results (Dict[str, Any]): The complete, comprehensive results\n",
        "            object from a successful run of the main pipeline orchestrator\n",
        "            (`run_transatlantic_spillovers_analysis`). This object contains all\n",
        "            necessary intermediate data products (like raw surprises and\n",
        "            admissible angles).\n",
        "        raw_data (Dict[str, pd.DataFrame]): A dictionary containing the original,\n",
        "            unfiltered raw DataFrames ('equity_tick_df', 'rate_tick_df',\n",
        "            'macro_df', 'announcement_df'). This is required for the sample\n",
        "            period analysis.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the detailed results from each\n",
        "                        of the three robustness analyses.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    # Initialize a dictionary to store the results of all robustness checks.\n",
        "    robustness_suite_results: Dict[str, Any] = {}\n",
        "\n",
        "    # Extract the main configuration from the benchmark results for reuse.\n",
        "    config = benchmark_results['inputs']['config']\n",
        "    target_market = benchmark_results['metadata']['target_market']\n",
        "\n",
        "    logging.info(\"====== STARTING IDENTIFICATION ROBUSTNESS SUITE ======\")\n",
        "\n",
        "    # --- 1. Step 1: Poor Man's Sign Restriction (PMSR) Analysis ---\n",
        "    # This check tests sensitivity to a simpler, alternative identification rule.\n",
        "    logging.info(\"--- Task 16.1: Running Poor Man's Sign Restriction (PMSR) Analysis ---\")\n",
        "    try:\n",
        "        # Execute the PMSR-specific orchestrator.\n",
        "        pmsr_results = run_pmsr_robustness_analysis(\n",
        "            master_results_benchmark=benchmark_results\n",
        "        )\n",
        "        # Store the complete results object from this run.\n",
        "        robustness_suite_results['pmsr_analysis'] = pmsr_results\n",
        "        logging.info(\"--- PMSR Analysis completed successfully. ---\")\n",
        "    except Exception as e:\n",
        "        # Log any failure in this specific check but allow the suite to continue.\n",
        "        logging.error(f\"!!! PMSR Analysis failed: {e}\", exc_info=True)\n",
        "        robustness_suite_results['pmsr_analysis'] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    # --- 2. Step 2: Rotational Uncertainty Analysis ---\n",
        "    # This check quantifies the uncertainty arising from the choice of rotation angle.\n",
        "    logging.info(\"--- Task 16.2: Running Rotational Uncertainty Analysis ---\")\n",
        "    try:\n",
        "        # Execute the rotational uncertainty orchestrator.\n",
        "        rotational_uncertainty_results = run_rotational_uncertainty_analysis(\n",
        "            benchmark_results=benchmark_results,\n",
        "            # These parameters could be moved to the config file for more flexibility.\n",
        "            num_angle_draws=config['computation_settings']['robustness_checks']['identification_uncertainty_draws'],\n",
        "            n_jobs=-1 # Use all available cores.\n",
        "        )\n",
        "        # Store the results (the new, wider credible bands).\n",
        "        robustness_suite_results['rotational_uncertainty_analysis'] = rotational_uncertainty_results\n",
        "        logging.info(\"--- Rotational Uncertainty Analysis completed successfully. ---\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"!!! Rotational Uncertainty Analysis failed: {e}\", exc_info=True)\n",
        "        robustness_suite_results['rotational_uncertainty_analysis'] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    # --- 3. Step 3: Alternative Sample Period Analysis ---\n",
        "    # This check tests the stability of the results over different time periods.\n",
        "    logging.info(\"--- Task 16.3: Running Alternative Sample Period Analysis ---\")\n",
        "    try:\n",
        "        # Define the sample periods for the analysis.\n",
        "        sample_definitions = {\n",
        "            'pre_covid': {'end_date': config['computation_settings']['robustness_checks']['truncated_sample_end_date']},\n",
        "            'pre_gfc': {'end_date': '2007-08-31'} # A standard pre-GFC cutoff\n",
        "        }\n",
        "\n",
        "        # Execute the sample period analysis orchestrator.\n",
        "        sample_period_results = run_alternative_sample_analysis(\n",
        "            raw_equity_df=raw_data['equity_tick_df'],\n",
        "            raw_rate_df=raw_data['rate_tick_df'],\n",
        "            raw_macro_df=raw_data['macro_df'],\n",
        "            raw_announcement_df=raw_data['announcement_df'],\n",
        "            target_market=target_market,\n",
        "            config=config,\n",
        "            sample_definitions=sample_definitions\n",
        "        )\n",
        "        # Store the results, which will be a dictionary of full result objects.\n",
        "        robustness_suite_results['sample_period_analysis'] = sample_period_results\n",
        "        logging.info(\"--- Alternative Sample Period Analysis completed successfully. ---\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"!!! Alternative Sample Period Analysis failed: {e}\", exc_info=True)\n",
        "        robustness_suite_results['sample_period_analysis'] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    logging.info(\"====== IDENTIFICATION ROBUSTNESS SUITE COMPLETE ======\")\n",
        "\n",
        "    # Return the dictionary containing all robustness check results.\n",
        "    return robustness_suite_results\n"
      ],
      "metadata": {
        "id": "P0DnnM7wXVGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Estimation Method Robustness\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 17, STEP 1: PRIOR SENSITIVITY ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "def run_prior_sensitivity_analysis(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    prior_scenarios: Dict[str, Dict[str, float]]\n",
        ") -> Dict[str, Dict[str, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Performs a sensitivity analysis on the BVAR model's prior hyperparameters.\n",
        "\n",
        "    Purpose:\n",
        "        This function systematically assesses the robustness of the main impulse\n",
        "        response findings to changes in the BVAR prior specification. It re-\n",
        "        estimates the BVAR model under various alternative hyperparameter\n",
        "        settings and stores the resulting IRFs for comparison.\n",
        "\n",
        "    Process:\n",
        "        1.  **Iterate Through Scenarios**: The function loops through a dictionary\n",
        "            of pre-defined prior scenarios. Each scenario has a descriptive name\n",
        "            (e.g., 'tighter_prior') and a dictionary of hyperparameter values\n",
        "            to override in the main configuration.\n",
        "        2.  **Modify Configuration**: For each scenario, it creates a deep copy\n",
        "            of the main study configuration to ensure that runs are independent\n",
        "            and do not have side effects. It then updates the relevant prior\n",
        "            hyperparameters in this temporary configuration.\n",
        "        3.  **Execute BVAR Pipeline**: It executes the core BVAR estimation and\n",
        "            impulse response calculation pipeline (Tasks 9, 10, and 12) using\n",
        "            the modified configuration for the current scenario.\n",
        "        4.  **Store Results**: The complete impulse response function object\n",
        "            (containing the median, lower, and upper credible bands) for each\n",
        "            scenario is stored in a master dictionary, keyed by the scenario name.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset ready for\n",
        "                                          modeling.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary, which\n",
        "                                 serves as the baseline.\n",
        "        prior_scenarios (Dict[str, Dict[str, float]]): A dictionary defining\n",
        "            the sensitivity analyses to run. Keys are descriptive scenario names.\n",
        "            Values are dictionaries of hyperparameter names and their alternative\n",
        "            values to be tested. Example:\n",
        "            {'tighter_prior': {'lambda_1_overall_tightness': 0.05}}\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, np.ndarray]]: A dictionary where keys are the\n",
        "        scenario names. Each value is the full impulse response results\n",
        "        dictionary ('irf_median', 'irf_lower', etc.) from the BVAR run under\n",
        "        that prior specification.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    # This dictionary will store the final IRF results for all scenarios.\n",
        "    all_sensitivity_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    logging.info(\"====== STARTING BVAR PRIOR SENSITIVITY ANALYSIS ======\")\n",
        "\n",
        "    # --- 1. Iterate Through Scenarios ---\n",
        "    for scenario_name, overrides in prior_scenarios.items():\n",
        "        logging.info(f\"--- Running sensitivity analysis for scenario: '{scenario_name}' ---\")\n",
        "\n",
        "        # --- 2. Modify Configuration Safely ---\n",
        "        # Create a deep copy of the original configuration to avoid modifying it in place.\n",
        "        # This is critical for ensuring each run is independent.\n",
        "        scenario_config = copy.deepcopy(config)\n",
        "\n",
        "        # Update the hyperparameters in the copied configuration.\n",
        "        for param_name, value in overrides.items():\n",
        "            # This assumes hyperparameters are in a specific nested location.\n",
        "            if param_name in scenario_config['bvar_priors']['hyperparameters']:\n",
        "                scenario_config['bvar_priors']['hyperparameters'][param_name] = value\n",
        "                logging.info(f\"    Overriding '{param_name}' with value: {value}\")\n",
        "            else:\n",
        "                # Raise an error if the parameter to override is not found.\n",
        "                raise KeyError(f\"Hyperparameter '{param_name}' not found in configuration.\")\n",
        "\n",
        "        # --- 3. Execute BVAR Pipeline for the Scenario ---\n",
        "        try:\n",
        "            # Task 9: Set up the BVAR with the modified prior configuration.\n",
        "            bvar_setup = run_phase4_task9_bvar_setup(\n",
        "                analysis_ready_df=analysis_ready_df,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Task 10: Run the Gibbs sampler.\n",
        "            posterior_draws = run_bvar_gibbs_sampler(\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Task 12: Calculate and summarize impulse responses.\n",
        "            irf_results = calculate_var_impulse_responses(\n",
        "                posterior_draws=posterior_draws,\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # --- 4. Store Results ---\n",
        "            # Store the final IRF object for this scenario.\n",
        "            all_sensitivity_results[scenario_name] = irf_results\n",
        "            logging.info(f\"--- Scenario '{scenario_name}' completed successfully. ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # If any step of the pipeline fails for a given scenario, log the\n",
        "            # error and continue to the next scenario.\n",
        "            logging.error(f\"!!! Scenario '{scenario_name}' failed: {e}\", exc_info=True)\n",
        "            all_sensitivity_results[scenario_name] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    logging.info(\"====== BVAR PRIOR SENSITIVITY ANALYSIS COMPLETE ======\")\n",
        "\n",
        "    return all_sensitivity_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 17, STEP 2: LAG LENGTH ROBUSTNESS\n",
        "# =============================================================================\n",
        "\n",
        "def run_lag_length_robustness_analysis(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    lags_to_test: List[int]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a robustness check on the BVAR model's lag length specification.\n",
        "\n",
        "    Purpose:\n",
        "        This function systematically evaluates the BVAR model's performance and\n",
        "        resulting impulse responses across a range of different lag lengths.\n",
        "        It provides formal information criteria to guide the selection of an\n",
        "        optimal lag length and allows for a qualitative comparison of how the\n",
        "        model's dynamic implications change with the specification.\n",
        "\n",
        "    Process:\n",
        "        1.  **Iterate Through Lag Lengths**: The function loops through a list\n",
        "            of specified lag lengths to be tested (e.g., [2, 4, 5]).\n",
        "        2.  **Modify Configuration**: For each lag length `p`, it creates a deep\n",
        "            copy of the main study configuration and updates the `lags` parameter\n",
        "            within the `bvar_specification`.\n",
        "        3.  **Execute BVAR Pipeline**: It executes the core BVAR estimation and\n",
        "            impulse response calculation pipeline (Tasks 9, 10, and 12) using\n",
        "            the modified configuration. This is critical, as changing the lag\n",
        "            length alters the regressor matrix, the prior structure, and the\n",
        "            effective sample size.\n",
        "        4.  **Calculate Information Criteria**: After each estimation, it\n",
        "            calculates the Akaike (AIC), Bayesian (BIC), and Hannan-Quinn (HQ)\n",
        "            information criteria using the posterior mean of the parameters.\n",
        "            These metrics provide a formal basis for comparing model fit versus complexity.\n",
        "        5.  **Store Results**: It stores both the calculated information criteria\n",
        "            and the full impulse response function object for each tested lag\n",
        "            length in a master results dictionary.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset ready for\n",
        "                                          modeling.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary, which\n",
        "                                 serves as the baseline.\n",
        "        lags_to_test (List[int]): A list of integer lag lengths to test.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing two main items:\n",
        "        - 'information_criteria_report': A DataFrame summarizing AIC, BIC, and\n",
        "          HQ for each tested lag length.\n",
        "        - 'irf_results_by_lag': A dictionary where keys are lag lengths (e.g.,\n",
        "          'lag_2') and values are the full IRF results for that model.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    # Dictionaries to store the final results.\n",
        "    info_criteria_results: List[Dict[str, Any]] = []\n",
        "    irf_results_by_lag: Dict[str, Dict[str, np.ndarray]] = {}\n",
        "\n",
        "    logging.info(\"====== STARTING BVAR LAG LENGTH ROBUSTNESS ANALYSIS ======\")\n",
        "\n",
        "    # --- 1. Iterate Through Lag Lengths ---\n",
        "    for p in lags_to_test:\n",
        "        logging.info(f\"--- Running analysis for lag length p = {p} ---\")\n",
        "\n",
        "        # --- 2. Modify Configuration Safely ---\n",
        "        # Create a deep copy of the configuration for this specific run.\n",
        "        scenario_config = copy.deepcopy(config)\n",
        "        scenario_config['bvar_specification']['lags'] = p\n",
        "\n",
        "        try:\n",
        "            # --- 3. Execute BVAR Pipeline for the Scenario ---\n",
        "            # Task 9: Set up the BVAR with the new lag length.\n",
        "            bvar_setup = run_phase4_task9_bvar_setup(\n",
        "                analysis_ready_df=analysis_ready_df,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Task 10: Run the Gibbs sampler.\n",
        "            posterior_draws = run_bvar_gibbs_sampler(\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Task 12: Calculate and summarize impulse responses.\n",
        "            irf_results = calculate_var_impulse_responses(\n",
        "                posterior_draws=posterior_draws,\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Store the IRF results for this lag length.\n",
        "            irf_results_by_lag[f'lag_{p}'] = irf_results\n",
        "\n",
        "            # --- 4. Calculate Information Criteria ---\n",
        "            # Use the posterior mean of the parameters for the calculation.\n",
        "            Y, X = bvar_setup['Y'], bvar_setup['X']\n",
        "            B_mean = np.mean(posterior_draws['B_draws'], axis=0)\n",
        "            Sigma_mean = np.mean(posterior_draws['Sigma_draws'], axis=0)\n",
        "\n",
        "            # Calculate residuals at the posterior mean.\n",
        "            residuals = Y - X @ B_mean\n",
        "\n",
        "            # Calculate the log-likelihood at the posterior mean.\n",
        "            log_lik = np.sum(multivariate_normal.logpdf(residuals, mean=np.zeros(Y.shape[1]), cov=Sigma_mean))\n",
        "\n",
        "            # Get the number of parameters (k) and observations (T) for this specific model.\n",
        "            k = B_mean.size\n",
        "            T = Y.shape[0]\n",
        "\n",
        "            # AIC = -2*ln(L) + 2*k\n",
        "            aic = -2 * log_lik + 2 * k\n",
        "            # BIC = -2*ln(L) + k*ln(T)\n",
        "            bic = -2 * log_lik + k * np.log(T)\n",
        "            # HQ = -2*ln(L) + 2*k*ln(ln(T))\n",
        "            hq = -2 * log_lik + 2 * k * np.log(np.log(T))\n",
        "\n",
        "            # Store the criteria for this lag length.\n",
        "            info_criteria_results.append({'lags': p, 'AIC': aic, 'BIC': bic, 'HQ': hq, 'n_obs': T, 'n_params': k})\n",
        "            logging.info(f\"--- Lag p={p} completed successfully. BIC: {bic:.2f} ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # If any step fails, log the error and continue.\n",
        "            logging.error(f\"!!! Analysis for lag p={p} failed: {e}\", exc_info=True)\n",
        "            info_criteria_results.append({'lags': p, 'AIC': np.nan, 'BIC': np.nan, 'HQ': np.nan, 'n_obs': np.nan, 'n_params': np.nan})\n",
        "\n",
        "    logging.info(\"====== BVAR LAG LENGTH ROBUSTNESS ANALYSIS COMPLETE ======\")\n",
        "\n",
        "    # --- 5. Assemble and Return Final Reports ---\n",
        "    # Create the final report DataFrame for the information criteria.\n",
        "    info_criteria_report = pd.DataFrame(info_criteria_results).set_index('lags')\n",
        "\n",
        "    return {\n",
        "        'information_criteria_report': info_criteria_report,\n",
        "        'irf_results_by_lag': irf_results_by_lag\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 17, STEP 3: SPECIFICATION ROBUSTNESS TESTS\n",
        "# =============================================================================\n",
        "\n",
        "def run_specification_robustness_analysis(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any],\n",
        "    specification_scenarios: List[Dict[str, Any]]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Performs robustness checks against alternative model and data specifications.\n",
        "\n",
        "    Purpose:\n",
        "        This function systematically tests the sensitivity of the main BVAR\n",
        "        results to various changes in the model specification. This includes\n",
        "        using different variable transformations (e.g., first differences),\n",
        "        substituting alternative data series (e.g., industrial production for\n",
        "        GDP), and modifying the set of deterministic regressors (e.g., removing\n",
        "        the time trend).\n",
        "\n",
        "    Process:\n",
        "        1.  **Iterate Through Scenarios**: The function loops through a list of\n",
        "            pre-defined specification scenarios. Each scenario dictionary\n",
        "            contains a descriptive name and instructions on how to modify the\n",
        "            data or configuration.\n",
        "        2.  **Modify Data/Configuration**: For each scenario, it creates a\n",
        "            modified version of the `analysis_ready_df` or the `config` object.\n",
        "            - For 'first_differences', it differences the endogenous variables.\n",
        "            - For 'variable_swap', it replaces one column with another.\n",
        "            - For 'config_change', it modifies the list of exogenous variables.\n",
        "        3.  **Execute BVAR Pipeline**: It executes the core BVAR estimation and\n",
        "            impulse response calculation pipeline (Tasks 9, 10, and 12) using\n",
        "            the modified data and/or configuration for the current scenario.\n",
        "        4.  **Store Results**: The complete impulse response function object for\n",
        "            each scenario is stored in a master dictionary, keyed by the\n",
        "            scenario name, for later comparison.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset from the\n",
        "                                          baseline preparation.\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        specification_scenarios (List[Dict[str, Any]]): A list of dictionaries,\n",
        "            each defining a robustness check. Example:\n",
        "            [{'name': 'first_differences', 'type': 'data_transform',\n",
        "              'transform_func': lambda df: df.diff().dropna()},\n",
        "             {'name': 'no_trend', 'type': 'config_change',\n",
        "              'config_path': ['bvar_specification', 'exogenous_variables'],\n",
        "              'value': ['shock_FED', 'shock_ECB', 'covid_dummy']}]\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A dictionary where keys are the scenario\n",
        "        names. Each value is either the full IRF results dictionary or an\n",
        "        error message if the run failed.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    all_spec_results: Dict[str, Dict[str, Any]] = {}\n",
        "    logging.info(\"====== STARTING BVAR SPECIFICATION ROBUSTNESS ANALYSIS ======\")\n",
        "\n",
        "    # --- 1. Iterate Through Scenarios ---\n",
        "    for scenario in specification_scenarios:\n",
        "        scenario_name = scenario['name']\n",
        "        scenario_type = scenario['type']\n",
        "        logging.info(f\"--- Running specification scenario: '{scenario_name}' ---\")\n",
        "\n",
        "        # --- 2. Modify Data and/or Configuration ---\n",
        "        # Start with copies of the baseline data and config.\n",
        "        scenario_df = analysis_ready_df.copy()\n",
        "        scenario_config = copy.deepcopy(config)\n",
        "\n",
        "        try:\n",
        "            if scenario_type == 'data_transform':\n",
        "                # Apply a function to transform the data.\n",
        "                transform_func = scenario['transform_func']\n",
        "                scenario_df = transform_func(scenario_df)\n",
        "\n",
        "            elif scenario_type == 'variable_swap':\n",
        "                # Replace one variable with another.\n",
        "                original_var = scenario['original_var']\n",
        "                new_var = scenario['new_var']\n",
        "                if new_var not in scenario_df.columns:\n",
        "                    raise ValueError(f\"Swap variable '{new_var}' not found in DataFrame.\")\n",
        "                # Perform the swap.\n",
        "                scenario_df[original_var] = scenario_df[new_var]\n",
        "                # Update the list of endogenous variables in the config.\n",
        "                scenario_config['bvar_specification']['endogenous_variables'] = [\n",
        "                    v if v != original_var else new_var for v in config['bvar_specification']['endogenous_variables']\n",
        "                ]\n",
        "\n",
        "            elif scenario_type == 'config_change':\n",
        "                # Modify a nested value in the configuration dictionary.\n",
        "                path = scenario['config_path']\n",
        "                value = scenario['value']\n",
        "                temp_dict = scenario_config\n",
        "                for key in path[:-1]:\n",
        "                    temp_dict = temp_dict[key]\n",
        "                temp_dict[path[-1]] = value\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown scenario type: {scenario_type}\")\n",
        "\n",
        "            # --- 3. Execute BVAR Pipeline for the Scenario ---\n",
        "            # Task 9: Set up the BVAR with the modified data/config.\n",
        "            bvar_setup = run_phase4_task9_bvar_setup(\n",
        "                analysis_ready_df=scenario_df,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Task 10: Run the Gibbs sampler.\n",
        "            posterior_draws = run_bvar_gibbs_sampler(\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # Task 12: Calculate and summarize impulse responses.\n",
        "            irf_results = calculate_var_impulse_responses(\n",
        "                posterior_draws=posterior_draws,\n",
        "                bvar_setup=bvar_setup,\n",
        "                config=scenario_config\n",
        "            )\n",
        "\n",
        "            # --- 4. Store Results ---\n",
        "            all_spec_results[scenario_name] = {\n",
        "                'irf_results': irf_results,\n",
        "                'status': 'SUCCESS'\n",
        "            }\n",
        "            logging.info(f\"--- Scenario '{scenario_name}' completed successfully. ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # If any step fails, log the error and continue.\n",
        "            logging.error(f\"!!! Scenario '{scenario_name}' failed: {e}\", exc_info=True)\n",
        "            all_spec_results[scenario_name] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    logging.info(\"====== BVAR SPECIFICATION ROBUSTNESS ANALYSIS COMPLETE ======\")\n",
        "\n",
        "    return all_spec_results\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TASK 17: ESTIMATION METHOD ROBUSTNESS ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def run_estimation_robustness_suite(\n",
        "    analysis_ready_df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a full suite of robustness checks for the BVAR estimation method.\n",
        "\n",
        "    Purpose:\n",
        "        This master orchestrator for Task 17 executes a series of analyses to\n",
        "        test the robustness of the main findings to the specific choices made\n",
        "        during the BVAR model estimation. It systematically explores the impact\n",
        "        of alternative prior beliefs, dynamic specifications (lag length), and\n",
        "        data definitions.\n",
        "\n",
        "    Process:\n",
        "        The function sequentially executes the three main estimation robustness checks:\n",
        "        1.  **Prior Sensitivity Analysis**:\n",
        "            - Defines a set of alternative prior hyperparameter settings (e.g.,\n",
        "              tighter/looser priors).\n",
        "            - Calls `run_prior_sensitivity_analysis` to re-estimate the BVAR\n",
        "              and compute IRFs for each alternative prior.\n",
        "        2.  **Lag Length Robustness**:\n",
        "            - Defines a list of alternative lag lengths to test.\n",
        "            - Calls `run_lag_length_robustness_analysis` to re-estimate the BVAR\n",
        "              for each lag length, calculating information criteria and IRFs.\n",
        "        3.  **Specification Robustness**:\n",
        "            - Defines a list of alternative model specifications (e.g., using\n",
        "              first-differenced data, removing the time trend).\n",
        "            - Calls `run_specification_robustness_analysis` to re-estimate the\n",
        "              BVAR for each alternative specification.\n",
        "\n",
        "    Args:\n",
        "        analysis_ready_df (pd.DataFrame): The final, complete dataset from the\n",
        "                                          baseline preparation, potentially\n",
        "                                          including extra columns for robustness\n",
        "                                          checks (e.g., Industrial Production).\n",
        "        config (Dict[str, Any]): The main study configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the detailed results from each\n",
        "                        of the three robustness analyses.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    # This dictionary will store the results of all robustness checks.\n",
        "    robustness_suite_results: Dict[str, Any] = {}\n",
        "\n",
        "    logging.info(\"====== STARTING ESTIMATION METHOD ROBUSTNESS SUITE ======\")\n",
        "\n",
        "    # --- 1. Step 1: Prior Sensitivity Analysis ---\n",
        "    logging.info(\"--- Task 17.1: Running Prior Sensitivity Analysis ---\")\n",
        "    try:\n",
        "        # Define the scenarios for prior sensitivity. These are based on the\n",
        "        # instructions and represent meaningful deviations from the baseline.\n",
        "        prior_scenarios = {\n",
        "            'tighter_prior': {'lambda_1_overall_tightness': 0.05},\n",
        "            'looser_prior': {'lambda_1_overall_tightness': 0.2},\n",
        "            'faster_lag_decay': {'lambda_3_lag_decay': 0.5}\n",
        "        }\n",
        "\n",
        "        # Execute the prior sensitivity orchestrator.\n",
        "        prior_results = run_prior_sensitivity_analysis(\n",
        "            analysis_ready_df=analysis_ready_df,\n",
        "            config=config,\n",
        "            prior_scenarios=prior_scenarios\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_suite_results['prior_sensitivity_analysis'] = prior_results\n",
        "        logging.info(\"--- Prior Sensitivity Analysis completed successfully. ---\")\n",
        "    except Exception as e:\n",
        "        # Log any failure but allow the suite to continue.\n",
        "        logging.error(f\"!!! Prior Sensitivity Analysis failed: {e}\", exc_info=True)\n",
        "        robustness_suite_results['prior_sensitivity_analysis'] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    # --- 2. Step 2: Lag Length Robustness ---\n",
        "    logging.info(\"--- Task 17.2: Running Lag Length Robustness Analysis ---\")\n",
        "    try:\n",
        "        # Define the alternative lag lengths to test, excluding the baseline.\n",
        "        baseline_lags = config['bvar_specification']['lags']\n",
        "        lags_to_test = [p for p in [2, 4, 5] if p != baseline_lags]\n",
        "\n",
        "        # Execute the lag length robustness orchestrator.\n",
        "        lag_length_results = run_lag_length_robustness_analysis(\n",
        "            analysis_ready_df=analysis_ready_df,\n",
        "            config=config,\n",
        "            lags_to_test=lags_to_test\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_suite_results['lag_length_robustness_analysis'] = lag_length_results\n",
        "        logging.info(\"--- Lag Length Robustness Analysis completed successfully. ---\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"!!! Lag Length Robustness Analysis failed: {e}\", exc_info=True)\n",
        "        robustness_suite_results['lag_length_robustness_analysis'] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    # --- 3. Step 3: Specification Robustness ---\n",
        "    logging.info(\"--- Task 17.3: Running Specification Robustness Analysis ---\")\n",
        "    try:\n",
        "        # Define the list of alternative specification scenarios.\n",
        "        # This requires careful definition of data transformations and config changes.\n",
        "        endog_vars = config['bvar_specification']['endogenous_variables']\n",
        "        exog_vars_baseline = config['bvar_specification']['exogenous_variables']\n",
        "\n",
        "        specification_scenarios = [\n",
        "            {\n",
        "                'name': 'first_differences',\n",
        "                'type': 'data_transform',\n",
        "                'transform_func': lambda df: df[endog_vars].diff().dropna()\n",
        "            },\n",
        "            {\n",
        "                'name': 'no_trend',\n",
        "                'type': 'config_change',\n",
        "                'config_path': ['bvar_specification', 'exogenous_variables'],\n",
        "                'value': [v for v in exog_vars_baseline if v != 'trend']\n",
        "            }\n",
        "            # Note: Adding a 'variable_swap' test for IP vs GDP would require\n",
        "            # ensuring the IP data is present in `analysis_ready_df` and then\n",
        "            # adding a scenario dict here.\n",
        "        ]\n",
        "\n",
        "        # Execute the specification robustness orchestrator.\n",
        "        spec_results = run_specification_robustness_analysis(\n",
        "            analysis_ready_df=analysis_ready_df,\n",
        "            config=config,\n",
        "            specification_scenarios=specification_scenarios\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_suite_results['specification_robustness_analysis'] = spec_results\n",
        "        logging.info(\"--- Specification Robustness Analysis completed successfully. ---\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"!!! Specification Robustness Analysis failed: {e}\", exc_info=True)\n",
        "        robustness_suite_results['specification_robustness_analysis'] = {\"status\": \"FAIL\", \"error\": str(e)}\n",
        "\n",
        "    logging.info(\"====== ESTIMATION METHOD ROBUSTNESS SUITE COMPLETE ======\")\n",
        "\n",
        "    # Return the dictionary containing all robustness check results.\n",
        "    return robustness_suite_results\n",
        "\n"
      ],
      "metadata": {
        "id": "Au2q5P_obhnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL TOP-LEVEL ORCHESTRATOR\n",
        "# =============================================================================\n",
        "\n",
        "def execute_full_study_pipeline(\n",
        "    equity_tick_df: pd.DataFrame,\n",
        "    rate_tick_df: pd.DataFrame,\n",
        "    macro_df: pd.DataFrame,\n",
        "    announcement_df: pd.DataFrame,\n",
        "    target_market: str,\n",
        "    study_config: Dict[str, Any],\n",
        "    run_identification_robustness: bool = True,\n",
        "    run_estimation_robustness: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire research study, including the main analysis and all robustness checks.\n",
        "\n",
        "    Purpose:\n",
        "        This top-level orchestrator is the single entry point for the entire\n",
        "        analytical project. It provides a complete, end-to-end, reproducible\n",
        "        workflow, from raw data ingestion to the final, fully validated results.\n",
        "        It is designed to be the master controller that a researcher would call\n",
        "        to replicate the entire study.\n",
        "\n",
        "    Process:\n",
        "        1.  **Execute Benchmark Pipeline**: It first calls the main orchestrator,\n",
        "            `run_transatlantic_spillovers_analysis` (Task 15), to perform the\n",
        "            complete baseline analysis. This includes data cleaning, shock\n",
        "            identification, model estimation, and results generation.\n",
        "        2.  **Handle Benchmark Failure**: If the benchmark pipeline fails for any\n",
        "            reason, the entire process halts, as the robustness checks are\n",
        "            contingent on a successful benchmark run.\n",
        "        3.  **Execute Identification Robustness Suite**: If enabled, it calls the\n",
        "            `run_identification_robustness_suite` orchestrator (Task 16). This\n",
        "            function uses the benchmark results and the original raw data to\n",
        "            perform a series of checks on the stability of the shock\n",
        "            identification method (PMSR, rotational uncertainty, sub-samples).\n",
        "        4.  **Execute Estimation Robustness Suite**: If enabled, it calls the\n",
        "            `run_estimation_robustness_suite` orchestrator (Task 17). This\n",
        "            function uses the final analysis-ready dataset from the benchmark\n",
        "            run to test the sensitivity of the results to alternative BVAR\n",
        "            specifications (priors, lag length, variable definitions).\n",
        "        5.  **Compile Final Results**: It assembles the outputs from the benchmark\n",
        "            run and all robustness suites into a final, comprehensive master\n",
        "            results dictionary.\n",
        "\n",
        "    Args:\n",
        "        equity_tick_df (pd.DataFrame): Raw high-frequency equity tick data.\n",
        "        rate_tick_df (pd.DataFrame): Raw high-frequency interest rate tick data.\n",
        "        macro_df (pd.DataFrame): Raw long-format macroeconomic time series data.\n",
        "        announcement_df (pd.DataFrame): Raw central bank announcement metadata.\n",
        "        target_market (str): The market to study (e.g., 'CAN').\n",
        "        study_config (Dict[str, Any]): The main study configuration dictionary.\n",
        "        run_identification_robustness (bool): If True, runs the full suite of\n",
        "            identification robustness checks (Task 16).\n",
        "        run_estimation_robustness (bool): If True, runs the full suite of\n",
        "            estimation method robustness checks (Task 17).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing the complete results of\n",
        "                        the benchmark analysis and all executed robustness checks.\n",
        "    \"\"\"\n",
        "    # --- 0. Initialization ---\n",
        "    # Configure logging for the entire run.\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # This dictionary will hold all outputs of the entire study.\n",
        "    study_results: Dict[str, Any] = {}\n",
        "\n",
        "    # --- 1. Execute Benchmark Pipeline (Task 15) ---\n",
        "    logging.info(\"=\"*80)\n",
        "    logging.info(\"STEP 1: EXECUTING BENCHMARK ANALYSIS PIPELINE\")\n",
        "    logging.info(\"=\"*80)\n",
        "\n",
        "    # The benchmark run is mandatory. If it fails, the entire study fails.\n",
        "    benchmark_run_results = run_transatlantic_spillovers_analysis(\n",
        "        equity_tick_df=equity_tick_df,\n",
        "        rate_tick_df=rate_tick_df,\n",
        "        macro_df=macro_df,\n",
        "        announcement_df=announcement_df,\n",
        "        target_market=target_market,\n",
        "        study_config=study_config\n",
        "    )\n",
        "\n",
        "    # Store the benchmark results.\n",
        "    study_results['benchmark_run'] = benchmark_run_results\n",
        "\n",
        "    # Halt execution if the benchmark pipeline failed.\n",
        "    if benchmark_run_results.get('metadata', {}).get('final_status') == \"FAIL\":\n",
        "        logging.error(\"Benchmark pipeline failed. Halting execution of robustness suites.\")\n",
        "        return study_results\n",
        "\n",
        "    # --- 2. Execute Identification Robustness Suite (Task 16) ---\n",
        "    if run_identification_robustness:\n",
        "        logging.info(\"=\"*80)\n",
        "        logging.info(\"STEP 2: EXECUTING IDENTIFICATION ROBUSTNESS SUITE\")\n",
        "        logging.info(\"=\"*80)\n",
        "\n",
        "        # This suite requires the original raw data for the sub-sample analysis.\n",
        "        raw_data_dict = {\n",
        "            'equity_tick_df': equity_tick_df,\n",
        "            'rate_tick_df': rate_tick_df,\n",
        "            'macro_df': macro_df,\n",
        "            'announcement_df': announcement_df\n",
        "        }\n",
        "\n",
        "        # Call the Task 16 orchestrator.\n",
        "        identification_robustness_results = run_identification_robustness_suite(\n",
        "            benchmark_results=benchmark_run_results,\n",
        "            raw_data=raw_data_dict\n",
        "        )\n",
        "        # Store the results.\n",
        "        study_results['identification_robustness_suite'] = identification_robustness_results\n",
        "    else:\n",
        "        logging.info(\"Skipping Identification Robustness Suite as per configuration.\")\n",
        "\n",
        "    # --- 3. Execute Estimation Method Robustness Suite (Task 17) ---\n",
        "    if run_estimation_robustness:\n",
        "        logging.info(\"=\"*80)\n",
        "        logging.info(\"STEP 3: EXECUTING ESTIMATION METHOD ROBUSTNESS SUITE\")\n",
        "        logging.info(\"=\"*80)\n",
        "\n",
        "        # This suite requires the final analysis-ready DataFrame from the benchmark run.\n",
        "        analysis_ready_df = benchmark_run_results['phase_3_model_prep']['analysis_ready_df']\n",
        "\n",
        "        # Call the Task 17 orchestrator.\n",
        "        estimation_robustness_results = run_estimation_robustness_suite(\n",
        "            analysis_ready_df=analysis_ready_df,\n",
        "            config=study_config\n",
        "        )\n",
        "        # Store the results.\n",
        "        study_results['estimation_robustness_suite'] = estimation_robustness_results\n",
        "    else:\n",
        "        logging.info(\"Skipping Estimation Method Robustness Suite as per configuration.\")\n",
        "\n",
        "    logging.info(\"=\"*80)\n",
        "    logging.info(\"ENTIRE STUDY PIPELINE COMPLETED.\")\n",
        "    logging.info(\"=\"*80)\n",
        "\n",
        "    # Return the final, comprehensive results object.\n",
        "    return study_results\n"
      ],
      "metadata": {
        "id": "Oj1Y4qYAjDCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}